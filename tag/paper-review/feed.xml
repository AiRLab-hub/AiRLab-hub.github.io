<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/tag/paper-review/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-11-27T18:21:42+09:00</updated>
  <id>https://blog.airlab.re.kr/tag/paper-review/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Exploring Randomly Wired Neural Networks for Image Recognition</title>
      <link href="https://blog.airlab.re.kr/2019/11/Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition" rel="alternate" type="text/html" title="Exploring Randomly Wired Neural Networks for Image Recognition" />
      <published>2019-11-26T19:00:00+09:00</published>
      <updated>2019-11-26T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition">&lt;p&gt;안녕하세요 AiRLab 이상우입니다. 이번에 읽어본 논문은 Exploring Randomly Wired Neural Networks for Image Recognition 으로 Network Architecture 를 Random 하게 만들어보면 어떨까? 하는 생각을 가진 논문입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduction&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_1.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 네트워크의 정규한 패턴의 연결이 있는 Network Architecture가 꾸준히 발전해왔으며 이러한 연결들을 랜덤하게 연결하면 어떻게 되는지 실험을 해보았다고 합니다. 결과는 생각보다 놀라웠으며 기존의 Network 보다 성능이 더 좋거나 성능을 견줄만한 결과를 보여주었다고 합니다. 이로써 Network Architecture를 수동적으로 개발하는 것보다는 앞으로 Network generator 를 개발하는 것이 더 좋을 것이다라는 생각을 가지고 있습니다.&lt;br /&gt;
자세하기 알아보기전에 이 논문은 랜덤하게 연결을 해보면 어떨까? 라는 것이 메인 아이디어인만큼 사실 인공지능에서 큰 영감을 줄만한 내용보다는 어떻게 하면 Network를 랜덤하게 연결하는지와 관련된 내용이 논문의 주된 구성입니다.
하지만 제 생각에는 이 논문이 Network Architecture 의 연구 방향을 바꿀만한 논문이라 다들 한번씩 읽어보시면 좋을거 같습니다. 이제 자세한 내용을 소개해드리겠습니다.&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;methodology&quot;&gt;&lt;strong&gt;&lt;u&gt;Methodology&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;이 논문에서 소개하는 Network generator 는  &lt;strong&gt;g(Θ,s)&lt;/strong&gt; 라는 값을 가지고 네트워크를 생성합니다. 이 값이 가지는 의미를 하나하나 설명하겠습니다. &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Θ&lt;/strong&gt; : 네트워크의 다양한 정보를 포함하고 있는 값입니다. 예를 들어 VGG generator 가 있다면 VGG-16 으로 만들건지 VGG-34로 만들지를 결정하고 Network 의 깊이,폭,필터의 크기 등을 지정할 수 있습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;g&lt;/strong&gt; : graph의 연결을 결정합니다. 예를 들면 ResNet generator 가 있다면 F(x)의 값을 연결을 g를 통해서 x+f(x) 를 만들어 주는 값입니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;g(Θ)&lt;/strong&gt; : 집합 N을 반환합니다. 위에 값으로 생성된 연결과 설정을 가지고 만든 네트워크를 반환합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;s&lt;/strong&gt; : 이 과정을 몇번 반복할 것인지 정합니다. g(θ) 를 몇번 호출하여 랜덤 네트워크 패밀리를 구성할 수 있습니다.&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_2.png&quot; width=&quot;200&quot; hight=&quot;70&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 random graph 를 생성하고, 생성된 graph를 가지고 Network에 매핑을 시키는 방식을 가지고 만들었습니다. 그래서 Network generator 는 일반적인 graph를 생성하며 시작되고, 노드를 연결하는 일련의 노드와 edge를 생성합니다. edge는 위에 그림에서 보이는 노드로 들어오거나 나가는 화살표이며, 이는 데이더의 Flow라고 합니다. 파란색 원으로 구성된 부분은 노드라고 부르며 노드는 들어오는 데이터는 weight의 합계를 통해 conv로 들어가며 conv 는 ReLu - convolution - BN triplet 로 구성되어있다고 합니다. 또 노드는 몇개의 Input,Output edge를 가질 수 있다고 합니다. 이를 통해 graph 이론의 일반 graph 생성기를 자유롭게 사용하며 graph를 얻으면 신경망에 매핑이 된다고 합니다.&lt;/p&gt;

&lt;h4 id=&quot;random-graph-models&quot;&gt;&lt;strong&gt;&lt;u&gt;Random Graph Models&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_3.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에 보시다 싶이 3가지 방법으로 짜여진 graph들이 있습니다. 이는 위에서 설명한 일반 graph 생성기로 생성된 graph들이며 이 방법들이 어떻게 사용되었는지 설명을 해드리겠습니다.&lt;br /&gt;
첫번째 방법으로는 Erdos-R ˝ enyi 으로 &lt;strong&gt;ER&lt;/strong&gt;로 표시하고 있습니다. ER 은 N의 노드를 사용하는 경우, 임의의 두개의 노드는 다른 노드들과는 무관하게 edge가 P의 확률로 연결이 된다고 합니다. 이 방법은 모든 노드 쌍에 대해서 반복되며 ER은 P의 확률만을 가지고 있기때문에 ER(P) 로 표시한다고 합니다. &lt;br /&gt;
두번째 방법으로는 Barabasi-Albert 으로 &lt;strong&gt;BA&lt;/strong&gt;로 표시하고 있습니다. BA 은 순차적으로 새 노드를 추가하여 랜덤 graph를 생성하며 초기 상태는 edge가 없는 M노드부터 시작된다고 합니다. 이 방법은 순차적으로 M개의 edge가 있는 노드가 생성될 때까지 반복하며, 중복되는 방향의 edge는 생성하지 않는다고 합니다. 이 과정은 N개의 노드가 생길 때까지 반복하며, BA는 단일 파라미터 M을 가지며, BA(M)으로 표시됩니다. &lt;br /&gt;
세번째 방법으로는 Watts-Strogatz 으로 &lt;strong&gt;WS&lt;/strong&gt;로 표시하고 있습니다. WS 은 처음에 N노드는 정기적으로 링에 배치되고 각 노드는 인접한 K/2에 연결된다고 합니다. 그런 다음 시계방향 루프에서 모든 노드 V에 대해 시계방향 I번째 다음 노드에 연결하는 edge가 P의 확률로 연결이 됩니다. I는 1 ≤ i ≤ K/2 이며 K/2 번 반복된다고 합니다. &lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;&lt;strong&gt;&lt;u&gt;Experiments&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure1. Comparison on random graph generators : ER,BA, and WS&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;일반 graph 생성기 3개로 생성된 네트워크들의 정확도를 비교한 사진입니다. 직관적으로 보이는 결과이니 자세한 설명은 생략하겠습니다. &lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_5.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure2. ImageNet: small computation regime&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 논문 저자가 생성한 랜덤 네트워크로 다른 논문의 네트워크들과 비교했을 때도 정확도 면에서 경쟁력이 있는 결과는 보여줍니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_6.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure3. ImageNet: large computation regime.&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 결과는 FLOPs와 params 수가 현저히 적은데도 불구하고 다른 네트워크들과 비슷한 정확도를 보여주고 있습니다. 가장 정확도가 좋은 PNASNNet-5와 1.3% 가 나지만 FLOPs와 params 수가 확실히 차이가 나는것을 보실 수 있습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_7.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure4. COCO object detection&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;COCO dataset 에서 backbone을 ResNet과 ResNext를 사용했을 때 보다 RandWire를 썻을 때 정확도가 전체적으로 향상되있는 것을 볼수 있습니다. FLOP은 비슷하거나 더 낮다고 하였습니다.&lt;br /&gt;
논문을 마치며 저의 생각은 앞으로 네트워크 구조의 발전이 네트워크 생성기의 설계쪽으로 기울어져 갈것같습니다. 비록 논문의 내용이 인공지능에 영감을 줄만한 내용은 충분히 있었다고는 생각하지 않았으나 네트워크 구조에 대한 방향성이 바뀔 수 있는 논문이라 충분히 읽어볼만 하다고 생각합니다. 부족한 점은 저의 메일이나 댓글로 남겨주세요. 감사합니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sangwoo Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요 AiRLab 이상우입니다. 이번에 읽어본 논문은 Exploring Randomly Wired Neural Networks for Image Recognition 으로 Network Architecture 를 Random 하게 만들어보면 어떨까? 하는 생각을 가진 논문입니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">DeepLabV2</title>
      <link href="https://blog.airlab.re.kr/2019/11/DeepLabV2" rel="alternate" type="text/html" title="DeepLabV2" />
      <published>2019-11-23T20:00:00+09:00</published>
      <updated>2019-11-23T20:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/DeepLabV2</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/DeepLabV2">&lt;p&gt;DeepLabV2 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 노현철 입니다. 
제가 이번에 리뷰할 논문은 &lt;strong&gt;“DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/1.png&quot; alt=&quot;Figure1&quot; /&gt;
요번 논문을 읽기전에 DeepLabV1 논문을 읽지 않아서 인터넷을 간단하게 찾아보았습니다. 검색을 해보니 DeepLabV1에서 주장하는 내용은 CONDITIONAL RANDOM FIELDS(CRF) 라는 내용 한가지 였습니다. semantic segmentation은 픽셀단위의 조밀한 예측이 필요함으로 CRF를 후처리 과정으로 사용하여 픽셀단위 예측의 정확도를 더 높일 수 있게 되었습니다. 특히 fully connected CRF를 사용하면 위 그림과 같이 detail이 살아 있는 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/2.png&quot; alt=&quot;Figure2&quot; /&gt;
또한 이러한 CRF를 한번만 사용하는 것이 아니라  여러번 사용하게 된다면 조금 더 좋은 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 DeepLabv1을 보완하고자 CRF를 포함한 3가지 이슈가 있습니다.&lt;/p&gt;

&lt;p&gt;1) CONDITIONAL RANDOM FIELDS (CRF)&lt;/p&gt;

&lt;p&gt;2) Atrous convolution (dilated convolution)&lt;/p&gt;

&lt;p&gt;3) Atrous Spatial Pyramid Pooling (ASPP)&lt;/p&gt;

&lt;h5 id=&quot;atrous-convolution-dilated-convolution&quot;&gt;Atrous convolution (dilated convolution)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/3.png&quot; alt=&quot;Figure3&quot; /&gt;
기존 DCNN에서 receptive field을 확장 시키려면 pooling 후 convolution 했어야 했습니다. 이것은 feature들의 크기를 줄일 뿐만 아니라 연산량 또한 증가 시켰습니다. 이 논문에서 말하는 Atrous convolution은 이러한 이러한 현상들을 줄여 준다고 합니다. 이 Atrous convolution은 dilated convolution과 이름만 다를 뿐 같은 개념이라고 보시면 됩니다. Atrous convolution는 기존 convolution 과 연산량은 같지만 receptive field 가 확장되는 효과를 가져옵니다. 위에 사진을 보면 rate만큼 간격을 벌리고 그 간격은 0으로 만들어 버려서 receptive field의 크기를 확장 시키는 것 입니다.
&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/4.png&quot; alt=&quot;Figure4&quot; /&gt;
사진으로 비교해 보아도 pooling + conv 보단 Atrous convolution을 사용하는 것이 receptive field가 확장되있는 것을 직관적으로 볼 수 있습니다.&lt;/p&gt;

&lt;h5 id=&quot;atrous-spatial-pyramid-pooling-aspp&quot;&gt;Atrous Spatial Pyramid Pooling (ASPP)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/5.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spatial Pyramid Pooling(SPP)은 sppnet에서 영감을 받고 쓰였다고 하는데 이 논문 spp에 Atrous convolution을 더하여 aspp이라는 방법을 제시 하였습니다.
Atrous convolution을 사용하여 각각의 rate 값들을 각각 {6, 12, 18. 24} 로 하여 Pyramid Pooling 하였습니다. 그리고 이들의 결과들을 합쳐 각각의 receptive field를 수용하여 여러크기의 물체를 인식하는데 좋은 결과를 가져왔습니다.&lt;/p&gt;

&lt;h5 id=&quot;deeplab-v1-v2-비교&quot;&gt;DeepLab v1, v2 비교&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/6.png&quot; alt=&quot;Figure6&quot; /&gt;
DeepLab v1, v2를 구조를 사진으로 비교해보자면 우선 input image가 들어가면 v1은 기본적인 DCNN을 사용하였지만 v2는 Atrous convolution을 사용하여 DCNN을 하였습니다. 그 결과 score map의 크기가 v1보단 v2가 더 크게 나오는것을 볼 수 있습니다. 다음은 bi-linear interpolation방법을 통해 원본의 크기만큼 upsample하였습니다. 그리고 fully connected CRF을 사용하여 정확도를 한층 높였습니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental&quot;&gt;Experimental&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/7.png&quot; alt=&quot;Figure7&quot; /&gt;
실험은 ‘fc6’ layer의 rate값들을 각각 다르게 하여 실험 하였습니다. 그 결과 Kernel: 7x7, rate: 4, CRF사용 하였을때가 Kernel: 3x3, rate: 12, CRF사용 하였을 때와 성능이 같은걸 볼 수 있습니다. 그러나 전자의 실험이 후자의 실험보다 parameters도 많고, speed(img/sec)도 느린것으로 나타났습니다. 그래서 DeepLab-LargeFOV은 kernel size 3×3, r = 12 을 사용하게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/8.png&quot; alt=&quot;Figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두번째 실험은 위 실험에서 구한 DeepLab-LargeFOV와 ASPP안에 들어가는 Atrous convolution rate값들을 다르게 하고 비교하는 실험입니다.
ASPP-S는 rate = {2, 4, 8, 12}, ASPP-L는 rate = {6, 12, 18, 24}입니다. 
실험 결과 rate값이 ASPP-S보다 높게잡은 ASPP-L의 결과가 더 좋은 것으로 나타났습니다.
&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/9.png&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;어쩌다보니 deeplab v1을 읽지않고 v2를 먼저 읽게 되었는데 나름 v1의 내용이 많이 없어서 다행이였습니다. 다음번에는 v3를 읽고 리뷰를 써보도록 하겠습니다..!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyeoncheol Noh</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">DeepLabV2 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PSPNET</title>
      <link href="https://blog.airlab.re.kr/2019/11/pspnet" rel="alternate" type="text/html" title="PSPNET" />
      <published>2019-11-23T19:00:00+09:00</published>
      <updated>2019-11-23T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/pspnet</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/pspnet">&lt;p&gt;PSPNET 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 노현철 입니다. 
제가 이번에 리뷰할 논문은 &lt;strong&gt;“Pyramid Scene Parsing Network”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;pspnet을 읽기전에 Fully Convolutional Network(FCN)에 관한 논문을 읽어보진 않아서 간단하게 인터넷으로 찾아보았습니다. FCN은 Fully Connected layer 가 없는 CNN이 통용됩다고 합니다. 이 FC layer를 없앤 이유는 위치정보의 손실 때문에 없앴다고 합니다.(Segmentation 은 위치정보가 핵심적)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/1.PNG&quot; alt=&quot;Figure1&quot; /&gt;
이 논문에서는 FCN의 한계점을 나타내고 있습니다. 위에 사진을 보듯이 FCN은 보트를 자동차로 인식을하고, 비슷한 카테고리(건물, 초고층 빌딩)는 명확하지않고 둘 다 인식을 하고, 마지막으로 베개와 시트의 외관이 비슷해서 베개를 파싱하지 못하고 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제점들을 보완하고자 이 논문에서 제안하고있는 complex-scene parsing 이슈가 세가지가 있습니다.&lt;/p&gt;

&lt;p&gt;1) Mismatched Relationship (주변 환경의 관계)&lt;/p&gt;

&lt;p&gt;2) Confusion Categories (혼란의 카테고리)&lt;/p&gt;

&lt;p&gt;3) Inconspicuous Classes (눈에 띄지 않는 클래스)&lt;/p&gt;

&lt;h5 id=&quot;mismatched-relationship&quot;&gt;Mismatched Relationship&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/2.PNG&quot; alt=&quot;Figure2&quot; /&gt;
FCN은 ‘보트’를 ‘자동차’로 인식을 하였습니다. 이것은 단순히 외관으로만 판단하였기 때문에 틀렸다 라고 볼 수 있습니다. 하지만 일반적으로 ‘강 위에 자동차’ 보단 ‘강 위에 보트’일 가능성이 더 큽니다. pspnet에서는 외관만 판단하는 것이 아니라 주변 환경까지 고려하여 ‘자동차’가 아닌 ‘보트’로 Prediction 하는 것입니다.&lt;/p&gt;

&lt;h5 id=&quot;confusion-categories&quot;&gt;Confusion Categories&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/3.PNG&quot; alt=&quot;Figure3&quot; /&gt;
Ground Truth를 보듯이 건물과 초고층빌딩사진에서 보듯이 FCN은 건물과 초고층 빌딩 둘 다 인식하고 있습니다. 이러한 비슷한 카테고리{(건물, 초고층 빌딩),(들판, 땅)}들은 혼돈을 줄 수 있습니다. 이러한 문제점을 해결하기 위해 global contextual information 사용하면 카테고리안에 relationship이 명확해 질 수 있다.&lt;/p&gt;

&lt;h5 id=&quot;inconspicuous-classes&quot;&gt;Inconspicuous Classes&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/4.PNG&quot; alt=&quot;Figure4&quot; /&gt;
FCN은 베개와 시트의 유사한 외관으로 인해 구별을 못하고 있습니다. 이는 global scene category를 보면 베개를 파싱 못할 수 있습니다. 이를 개선하기 위해서  눈에 띄지 않는 object, stuff를 포함하는 여러 sub-regions에 global contextual information를 사용하여 해결할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;pspnet구조&quot;&gt;pspnet구조&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/5.PNG&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 사진은 pspnet의 전체적인 구조를 설명하는 사진입니다.&lt;/p&gt;

&lt;p&gt;첫번째로 input image(a)가 주어지면 CNN을 사용하여 Feature map을 얻는데 여기서 사용하는 CNN은 resnet을 사용하였고 dilated Convolution를 사용한 FCN구조라고 합니다. (dilated Convolution은 공간적 특징을 유지하기 때문에 Segmentation 많이사용) 이 Feature map 에서 local contextual information를 얻고 이것을 pool을 하게됩니다. pool의 종류에는 Max pooling, average pooling 둘 다 사용하게 되었지만 average가 우세하여 대부분은 average pooling을 사용하게 되었습니다. 이러한 pooling으로 인해 global contextual information을 얻어냅니다. 또한 pool을 할때 Pyramid Pooling Module을 사용하였고 이는 (1×1, 2×2, 3×3, 6×6)×N 의 크기를 가진 sub-region으로 만들어냅니다.&lt;/p&gt;

&lt;p&gt;global contextual information의 이해를 돕기위해 사진을 보시면
&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/6.PNG&quot; alt=&quot;Figure6&quot; /&gt;
Feature map을 4개의 sub-region으로 나누고 2×2로 average pooling 하면 각 각의 sub-region 특징을 알 수 있습니다. 예를들어 초록 원이 자동차라면 나머지 sub-region의 특징이 물인지 도로인지 구분하여서 상황을 고려할 수 있습니다. ( Segmentaion에 있어서 더 좋은 성능)&lt;/p&gt;

&lt;p&gt;끝으로 (1×1, 2×2, 3×3, 6×6)×N 을 conv하여 1/N 로 줄입니다. 그 이유는 마지막 Feature map들을 Upsampling하고 기존 Feature map과 이어붙이기 때문에 비율을 같게 해주는 것 입니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental&quot;&gt;Experimental&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/7.PNG&quot; alt=&quot;Figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험은 resnet을 기본으로 사용하였고 여기서 B1은 Pyramid Pooling Module의 1x1만 사용했다 라는 뜻이고, B1236 은 1x1, 2x2, 3x3, 6x6 라는 뜻입니다. max보단 average사용했을때 성능이 더 좋았고 DR-dimension reduction (N -&amp;gt; 1/N) 을 사용했을때 성능이 제일 좋았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/8.PNG&quot; alt=&quot;Figure8&quot; /&gt;
표에도 나타나듯이 ResNet이 깊을수록 성능이 좋았고 MS(multi-scale)사용했을때 성능이 제일 좋았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/9.PNG&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;최근들어 Segmentaion에 관심이 생겨 읽게 되었습니다. 조금 아쉬웠던 점이 이 논문이 나오기 전 논문을 먼저 읽고 읽었으면 와닿는 부분이나 이론적인 부분들이 더욱 많았을텐데 라는 생각이 들었습니다. 다음번에는 pspnet 이전에 나온 논문을 읽어보고 싶습니다..!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyeoncheol Noh</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">PSPNET 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mixup: BEYOND EMPIRICAL RISK MINIMIZATION</title>
      <link href="https://blog.airlab.re.kr/2019/11/mixup" rel="alternate" type="text/html" title="Mixup: BEYOND EMPIRICAL RISK MINIMIZATION" />
      <published>2019-11-23T11:00:00+09:00</published>
      <updated>2019-11-23T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/mixup</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/mixup">&lt;h3 id=&quot;mixup-beyond-empirical-risk-minimization-review&quot;&gt;Mixup: BEYOND EMPIRICAL RISK MINIMIZATION Review&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다.&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Mixup&lt;/strong&gt; 입니다. (&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;arXiv:1710.09412&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이번에는 &lt;b&gt;Data Augmentation&lt;/b&gt; 에 관련된 논문입니다. 
&lt;b&gt;저는 Augmentation 관련 논문에서 핵심적인 부분은 결국, 한정된 Data를 어떻게 다뤄야 효과적으로 학습할 수 있을까? 에 관한 대답이라고 생각합니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;논문에서 저자는 일반적으로 Dataset에 의존하여 학습하는 것을 비판하고 있습니다. 여기서 비판이란 Dataset의 원론적인 비판이 아닌, Dataset을 그대로 학습하는 것에 대한 문제점을 지적하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;“Train Data와 조금만 다른 Data를 설명할 수 없다.” 이 부분에서 Train Data에 dependent 한 문제점을 지적하고 있습니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Deeplearning이 활성화된 시기부터 계속적으로 연구되고있는 부분이기도 합니다.&lt;/p&gt;

&lt;p&gt;저자는 한정적인 Dataset에서 어떻게하면 더 General 하게 model 을 만들 수 있을까에 대한 고민을 많이 한 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;BackGround&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;모델의 학습을 위해서 아래와 같은 expected risk를 최소화 하여야 합니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_02.png&quot; width=&quot;600&quot; height=&quot;130&quot; /&gt;[figure_01] (P(x,y) = 결합분포, L = loss-funtion)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
 그러나 대부분의 경우 Joint distribution(결합 확률분포(두 개 이상의 확률변수에 대한 확률분포))을 모릅니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 일반적인 학습의 경우 아래와 같이 학습할 Dataset을 사용하여 &lt;b&gt;empirical distribution&lt;/b&gt;을 아래와 같이 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Empirical distribution(ERM)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_03.png&quot; width=&quot;600&quot; height=&quot;170&quot; /&gt;[figure_02]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  따라서, &lt;b&gt;위에서 구한 Empirical distribution&lt;/b&gt; P를 통하여 &lt;b&gt;Expected risk R&lt;/b&gt;을 아래와 같이 나타낼 수 있습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_04.png&quot; width=&quot;800&quot; height=&quot;100&quot; /&gt;[figure_03]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  간단하게 설명하자면, [figure_01] 을 통해 model을 학습하여야 하는데, dP(x,y) 즉, (joint distribution)을 모르는 경우가 대부분이니까, Train data를 사용하여 [figure_02]와 같이 joint distribution을 empirical distribution으로 대체하여 사용한다. 라는 것입니다.&lt;/p&gt;

&lt;p&gt;결과적으로 [figure_03]과 같은 식이 됩니다.&lt;/p&gt;

&lt;p&gt;그리고 이를 &lt;b&gt;ERM(Empirical Risk Minimization)&lt;/b&gt; 이라고 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Vicinity distribution(VRM)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위에 설명한 joint distribution 을 vicinity distribution을 통해 대체 할 수 도 있습니다. 다음과 같습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_05.png&quot; width=&quot;600&quot; height=&quot;140&quot; /&gt;[figure_04] virtual feature-target pair(x ̃,y ̃),&lt;br /&gt; vicinity of the training feature-target pair (xi,yi).&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  간단하게 trian data에 가상의 어떤 data를 섞었다고 생각하면 된다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_06.png&quot; width=&quot;600&quot; height=&quot;50&quot; /&gt;[figure_05]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
이때 논문에서는 위와 같이 구성한다면, 즉, x ̃와 xi의 차를 평균으로 갖고, 분산값을 Sigma^2으로 갖는 정규분포가 vicinity distribution이 된다고 설명하고 있고, 이 효과는 학습데이터에 Gaussian noise을 더한 것으로 이해하면 된다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;이를 VRM (Vicinal Risk Minimization)이라고 한다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;u&gt;나는 지금까지 ERM/VRM을 이해하는 것이 논문에서 제안하는 바를 이해하는데 충분한 배경이 되었다.
&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;이제 본격적으로 &lt;b&gt;MixUp의 idea&lt;/b&gt;를 살펴보면, 다음과 같이 정리할 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_07.png&quot; width=&quot;100%&quot; height=&quot;100&quot; /&gt;[figure_06] Mixup&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에서 본 VRM과 수식이 많이 유사하다. 
당연한 것이 VRM에서 가상의 어떤 data를 섞는 다고 했던 부분을 Dataset에서 가져와서 쓰겠다는 것이다.&lt;/p&gt;

&lt;p&gt;실제 학습에서 사용되는 코드는 다음과 같다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_08.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_07] Mixup_pytorch_code&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;code를 보면 lambda 값은 beta분포에서 뽑게 되는데, 분포가 [0,1] 사이에서 뽑아지게 됨으로, interpolation의 비율(가상의 data를 train data에 섞는 비율) 을 랜덤하면서도 적절하게 가져갈 수 있게 된다. 여기서 alpha 값은 1로 고정한다.&lt;/p&gt;

&lt;p&gt;그렇게 되면 학습하는 이미지는 다음과 같이 볼 수 있다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_01.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_08] Mixup_pytorch_image&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (CIFAR10 &amp;amp; 100)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_09.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_09] Mixup_pytorch_image&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 CIFAR 10 &amp;amp; 100 에서 기존의 성능보다 1.1% ~ 4.5% 성능을 높이게 되었다.&lt;/p&gt;

&lt;p&gt;즉, 같은 model 같은 Dataset으로 x% 만큼 general한 model을 뽑게 되었다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (corrupted label)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_11.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_10] corrupted label Acc examples&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 표를 보면 손상된 label에 대한 정확도가 기존에 방법보다 굉장히 많은 Acc를 갖는 것을 확인 할 수 있다. 또, 이 실험을 통해서 dropout과 mixup이 긍정적인 결과를 도출해 낸다는 것 또한 확인 할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (Adversarial example)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_10.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_11] Robustness to adversarial examples&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 부분도 요즘 핫한 부분인데, 간단하게 말해서 사람눈에는 똑같이 Panda, car로 구분되지만 computer입장에서는 그렇지 못하게 만드는 즉, 오류를 범하게 만드는 noise를 첨가하는 attack 이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;FGSM,I-FGSM 의 경우 Attack mechanism 이라고 보시면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;기존 학습법 보다 Mixup의 방법이 더 높은 방어능력을 갖고 있다. 즉, 기존보다 더 adversarial attack 에 robust 하다고 볼 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (CIFAR10)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_12.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_12]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 실험 결과를 통하여 Mixup에서 weight decay값은 10^-4이 좋다는 것을 설명하고 있으며, 첨가하는 data를 어떻게 하면 좋을 지에 관한 내용이 포함되어 있습니다.&lt;/p&gt;

&lt;p&gt;가장 높은 Acc를 보인것은 AC + RP 입니다. 또, SC는 효과가 없다고 말하고 있습니다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AC : mix between all classes.
SC : mix within the same class.
RP : mix between random pairs.
KNN : mix between k-nearest neighbors(k=200)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;후기-implementation&quot;&gt;후기 (implementation)&lt;/h3&gt;

&lt;p&gt;우선, DataAugementation에 관한 논문을 처음 접했는데 굉장히 흥미로웠다. 2019_ICCV를 다녀와서 느낀점이 많은데, 그 중 하나는 Data를 어떻게 다룰것인가에 관한 관심이 생겼다는 것이다. 천천히 읽어가면서, CutMix까지 읽어볼 생각이다.&lt;/p&gt;

&lt;p&gt;이번논문을 통해, pytorch를 이용하여. 직접. 구현을 하였는데.&lt;/p&gt;

&lt;p&gt;preActresnet-18의 경우 오차가 0.5% 정도로 거의 paper-performance에 가깝게 구현되었다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_13.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_13] compare performance(even | paper | my)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;[figure_13]을 보면 알 수 있듯이 기존 model을 좀더 general 하게 가져갈 수 있다는 장점이 좋은 것 같다.&lt;/p&gt;

&lt;p&gt;그리고, 혹시나 구현중에 model의 train loss 가 잘 떨어지지 않고, train Acc가 낮다고 잘 못 한거 아닌가 라는 생각을 할 수 있는데, 직접구현한 결과 mixup은 일반적으로 학습하는 cifar100 data를  막 95% 99% 학습할 수가 없다.&lt;/p&gt;

&lt;p&gt;왜냐면 train data 가 그만큼 어렵기 때문이다.&lt;/p&gt;

&lt;p&gt;그러나 test loss 는 더 낮고 test Acc는 더높기 때문에 general 한 결과를 얻어낼 수 있기 때문에 긍정적인것 같다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;[References]&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;  &lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;mixup: Beyond Empirical Risk Minimization
&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Daehan Kim</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Mixup: BEYOND EMPIRICAL RISK MINIMIZATION Review</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
      <link href="https://blog.airlab.re.kr/2019/11/manifold-mixup" rel="alternate" type="text/html" title="Manifold Mixup: Better Representations by Interpolating Hidden States" />
      <published>2019-11-22T11:00:00+09:00</published>
      <updated>2019-11-22T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/manifold-mixup</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/manifold-mixup">&lt;h3 id=&quot;manifold-mixup-better-representations-by-interpolating-hidden-states-리뷰&quot;&gt;Manifold Mixup: Better Representations by Interpolating Hidden States 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Manifold Mixup: Better Representations by Interpolating Hidden States”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Manifold Mixup: Better Representations by Interpolating Hidden States은(이하 Manifold Mixup) 2019ICML에 통과된 논문으로 카테고리는 Classification, Data Augmentation 입니다. 또한 딥러닝의 대가 Yoshua Bengio가 저자로 참여된 논문입니다. Manifold Mixup은 Mixup 논문에서 영감을 얻었으며, 인풋으로 들어오는 이미지 뿐만아니라, hidden states사이에서도 mixup을 하자는게 주된 내용이고, CIFAR100에서는 mixup을 능가하는 성능을 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Manifold Mixup을 이해하기 위해서는 우선 매니폴드 가 무엇인지 먼저 아셔야 합니다. 저는 이 논문을 이해할 정도로만 매니폴드를 설명할 것 이니, 혹시 더 궁금하시다면, &lt;a href=&quot;https://www.youtube.com/watch?v=o_peo6U7IRM&amp;amp;t=4692s&quot;&gt;https://www.youtube.com/watch?v=o_peo6U7IRM&amp;amp;t=4692s&lt;/a&gt; 이활석님의 매니폴드 설명을 참고하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img2.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림이 매니폴드를 나타내는 그림 입니다. 매니폴드란 데이터가 사는 공간입니다. 위의 사진처럼 한 매니폴드가 있으면, 그 위에 모든 데이터를 표현할 수 있고, 그림처럼 똑같은 4 이더라도 다 다른 위치에 있습니다. 또한 사람눈으로는 매우 닮아있는 4 라고 할지라도 매니폴드상 거리가 멀수도, 가까울 수도 있습니다. 직관적으로 딥러닝은 이러한 매니폴드에서 공통된 특성을 가로지르는 하나의 선을 찾는 것 이라고 생각하셔도 될 것 같습니다.
이제 매니폴드를 간략하게 알았으니 다음 설명으로 넘어가겠습니다.&lt;/p&gt;

&lt;p&gt;논문 저자는 친절하게 Manifold Mixup을 한줄로 요약해 줍니다. “Manifold Mixup improves the hidden representations and decision boundaries of neural networks at multiple layers.” 즉 매니폴드 믹스업이란, “다중 레이어가 있을때 그냥 믹스업 처럼 인풋만 이미지를 섞어버리면 불공평하니, 다중 레이어 모든 핏쳐맵에서 섞자!” 입니다. 실제로 이 말이 이 논문의 전부이며 앞으로는 이 말을 증명하고 실험하는 과정입니다. 또한 이 논문은 라벨 스무딩의 효과가 있다. 라고 생각하시면 이해하기 편하실것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img3.png&quot; alt=&quot;Figure1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img4.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림들은 저자가 2D spiral dataset으로 시각화한 그림입니다. 상단의 그림의 왼쪽은 아무것도 사용하지 않은 base이며, 상단의 오른쪽은 Manifold Mixup 입니다. 그림에서 보이는것과 같이 두개의 정확도는 비슷하지만, base는 오버피팅이 생긴 것을 한 눈에 알수 있고, Manifold Mixup은 오버피팅이 일어나지 않은 것을 볼수 있습니다. 또한 아래의 그림은 기존 유명 regularizers 들과의 성능을 정성적으로 비교한 것이며, 정성적으로는 Manifold Mixup이 좋아 보이나, “Manifold Mixup이 다른 타 regularizers들을 능가하는 방법이다!” 라고 생각하지 마시고, 그냥 저런 유명 방법들과 견줄만한 방법이다. 정도로만 생각하시면 좋을것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;manifold-mixup&quot;&gt;Manifold Mixup&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img5.png&quot; alt=&quot;Figure1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img6.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식들은 Manifold Mixup을 이해하기 쉽게 저자가 수식으로 표현한 것이며, 상단의 수식은 매니폴드에 존재하는 hidden states를 섞고, label도 그 수치만큼 섞겠다는 이야기 입니다. 아래의 수식은 전체 학습 프로세스가 어떻게 작동되는지 설명한 수식이며, 학습이 진행되면 input을 포함한 hidden states에 Mixup이 동작한다는 수식입니다. 또한 Beta는 Beta분포를 따르는 것을 의미합니다. Beta분포를 사용한 이유는 랜덤하게 뽑으면 섞이는 두 대상이 일정하게 섞일 확률이 높기 때문에, 한쪽이 더 우세하게 섞기 위하여 Beta분포를 사용한 것 입니다.&lt;/p&gt;

&lt;h3 id=&quot;empirical-investigation-of-flattening&quot;&gt;Empirical Investigation of Flattening&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img7.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 Flattening 하는 것이 왜 좋은가를 실험하기 위하여 MNIST데이터 셋에서 Singular Value Decomposition (SVD)을 통하여 실험합니다.
SVD를 직관적으로 설명하면 선형대수에서 배우는 특이값 분해로 같은 이미지에서 투영변환, 스케일변환, 회전변환을 하였을때 딥러닝 관점에서 augmentation이 된 데이터들 즉 매니폴드를 지나가는 직선의 거리 라고 생각하시면 될 것 같습니다. 이러한 SVD 값이 Maniflod Mixup이 가장 작았습니다. 위의 그림은 이것을 시각화 한 것이고, Manifold Mixup을 사용한 방법에서 MNIST 데이터들이 잘 분류된걸 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img8.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 논문저자는 위의 그래프와 같이 다양한 어규멘테이션에서 실험을 합니다. 논문저자가 주장한대로 Maniflod Mixup을하면 매니폴드를 직관하는 선을 찾기 쉬워져 같은 데이터라면 결과가 좋아야 합니다. 위 그래프도 논문저자가 주장한대로 매니폴드 믹스업은 다양한 데이터에서 강인합니다. 하지만 아쉬운 점은 이 논문 저자는 극한의 튜닝을 했습니다. epoch를 2000까지 돌리는둥 ,,, 최소 1200epoch를 학습합니다.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img9.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 실험 입니다. 논문 저자는 CIFAR10, 100 TINY Imagenet에서 실험을 하였습니다. 위의 표는 그 결과 입니다. 보이시는것과 같이 CIFAR10,100에서는 베타분포를 만들때 사용하는 알파값이 2일때 성능이 가장 좋고 Manifold Mixup에서 성능이 가장 좋습니다. 하지만 TINY Imagenet에서는 mixup보다 성능이 낮은데, 아마 tiny라 그런것이고, full imagenet이면 manifold mixup이 더 좋습니다. 그 결과는 cutmix논문을 참고하시면 확인하실수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문은 저의 직관과 비슷하여 재미있게 읽었던 논문 입니다. 하지만 epoch를 1500까지 맞춰줘야 논문 성능을 구현 할수 있는점(저는 1200에 구현했습니다 ㅎㅎ) 하이퍼파라메터를 공개하지 않은점. 수학적으로 너무 무거운점이 아쉬웠습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Manifold Mixup: Better Representations by Interpolating Hidden States 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Batch Normalization-Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
      <link href="https://blog.airlab.re.kr/2019/11/Batch-Normalization(+group-normalization)" rel="alternate" type="text/html" title="Batch Normalization-Accelerating Deep Network Training by Reducing Internal Covariate Shift" />
      <published>2019-11-18T19:00:00+09:00</published>
      <updated>2019-11-18T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/Batch%20Normalization(+group%20normalization)</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/Batch-Normalization(+group-normalization)">&lt;p&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 강혜윤입니다.&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/strong&gt;(Batch Norm) (&lt;a href=&quot;https://arxiv.org/pdf/1502.03167.pdf&quot;&gt;arXiv:1605.07146&lt;/a&gt;)이며 2015년 발표가 된 연구입니다.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DNN 학습 시키는 것이 어려운 이유 : 학습을 시키는 과정에서 이전 layer의 parameter 변화로 다음 layer의 input의 분포가 변화함 (weight 값의 변화)  &lt;br /&gt;
=&amp;gt; “internal covariate shift”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization에서 normalization을 모델 아키텍처의 일부로 만들고, 각 mini-batch에 대해 normalization을 수행&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization의 장점 : &lt;br /&gt;
① 높은 learning rate 사용 가능 &lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;② 초기화에 크게 신경 쓰지 않아도 됨&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;③ regularizer의 역할로 몇몇 경우에 Dropout을 사용하지 않아도 됨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;[Introduction]&lt;/h3&gt;
&lt;p&gt;▶ SGD(Stochastic Gradient Descent)&lt;/p&gt;

&lt;p&gt;① Gradient Descent : loss function을 계산할 때 전체 data에 대해 계산, 계산 량이 가장 많음&lt;br /&gt;
 (배치는 전체 데이터 셋라고 가정)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;② Mini-batch Gradient Descent : training data의 배치(batches)만 이용해서 그라디언트(gradient)를 구하는 것&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;③ Stochastic Gradient Descent : loss function을 계산할 때 일부 data에 대해 계산 (mini-batch), 계산 량을 줄임&lt;br /&gt;
 (확률적(Stochastic) : 용어는 각 배치를 포함하는 하나의 예가 무작위로 선택된다는 것을 의미)&lt;br /&gt;
 미니배치(mini-batch)가 데이터 한 개로 이루어짐&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DNN을 효율적으로 학습시킬 수 있도록 함&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;SGD의 변형인 momentum, Adagrad 등도 있음&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;loss값을 최소화하기 위해 parameterθ을 최적화시킴&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/1.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SGD 사용 시 특징&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;① Stochastic Gradient는 실제 Gradient의 추정 값이며 이것은 미니배치의 크기 N이 커질수록 더 정확한 추정 값을 가지게 된다.&lt;br /&gt;
 ② 미니배치를 뽑아서 연산을 수행하기 때문에 최신 컴퓨팅 플랫폼에 의하여 병렬적인 연산 수행이 가능하여 더욱 효율적이다.&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존의 문제점&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;① neural network를 학습하기 위한 hyper parameter들의 초기 값 설정을 굉장히 신중하게 해줘야 함 (그렇지 않으면 covariate shift가 발생할 수 있음)&lt;/p&gt;

&lt;p&gt;② Saturation problem&lt;br /&gt;
  기울기가 0이 되어 weight 업데이트량이 0이 되는 현상&lt;br /&gt;
  sigmoid 함수에서 큰 값을 가지면 기울기가 0이 되어 사라짐&lt;br /&gt;
  -&amp;gt; ReLU 함수 사용과 신중하게 initialization하고 작은 learning rate를 사용하면서 해결 (완전한 해결책은 아님)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch Normalization의 등장&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;① internal covariate shift 문제를 줄일 수 있음&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;② DNN 학습을 가속화 시킬 수 있음&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;[batch normalization 공식]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/2.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mini batch 단위에서 정규화 수행&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Mini batch 내의 한 example 내에서의 Activation 들은 각각 독립적&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Γ: scale 조정, β: shift 조정&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiments_mnist&quot;&gt;[Experiments_MNIST]&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/3.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;⦁Batch Normalization makes the distribution more stable and reduces the internal covariate shift.&lt;/p&gt;

&lt;p&gt;[Experiments_ImageNet classification]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/6.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;group-normalization과-batch-normalization-비교&quot;&gt;[Group normalization과 Batch normalization 비교]&lt;/h3&gt;

&lt;p&gt;Normalization formulation
&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/9.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;① Batch normalization
&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/10.png&quot; alt=&quot;Batch Normalization&quot; /&gt;
 -&amp;gt; batch 단위로 정규화 수행 &lt;br /&gt;
 -&amp;gt; batch norm의 문제점 : batch size 단위로 정규화 시키는 과정에서 batch size에 의존적이게 되고, batch size가 작을 경우 학습이 잘 이루어지지 않음 &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;② Group normalization
&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/11.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 문제점을 해결하기 위해 정규화를 batch 단위로 하지 않고, channel을 그룹으로 나누어 그 그룹을 단위로 정규화 수행&lt;br /&gt;
 (더 이상 batch size에 의존적이지 않음)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/12.png&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;-&amp;gt; 위에 실험을 보면 batch size에 의존적인 BN은 batch size 별로 학습에 차이를 보였고, batch size에 비의존적인 GN은 어떤 batch size에도 학습이 잘 이루어지는 것을 확인할 수 있다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyeyun Kang</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Shake-Shake regularization</title>
      <link href="https://blog.airlab.re.kr/2019/11/shake-shake-regularization" rel="alternate" type="text/html" title="Shake-Shake regularization" />
      <published>2019-11-16T11:00:00+09:00</published>
      <updated>2019-11-16T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/shake-shake-regularization</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/shake-shake-regularization">&lt;h3 id=&quot;shake-shake-regularization-리뷰&quot;&gt;Shake-Shake regularization 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Shake-Shake regularization”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 크게보면 Data augmentation에 속하는 논문 입니다. 지금까지는 Data augmentation을 이미지에 했다면, Shake-Shake regularization 논문은 Internal Representations에 augmentation을 합니다. (Internal Representations은 feature map과, weight를 의미합니다.) 이런 Internal Representations augmentation을 통하여 그 당시 CIFAR10, CIFAR100에서 state of the art를 달성합니다.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img1.png&quot; alt=&quot;Figure1&quot; /&gt;
위 그림과 같이 컴퓨터는 사람과는 다르게 물체를 일정 규칙이 있는 스칼라 값으로 인식합니다. 그렇다면 컴퓨터 입장에서는 feature map 그리고 이미지 는 일정 규칙이 있는 데이터이기 때문에 현재처럼 이미지에서만 Data augmentation을 하지말고, Internal Representations에도 Data augmentation을 해주자고 주장합니다. 또한 Internal Representations에 Data augmentation을 해주면 stochastically “blending” 효과가 있다고 주장합니다.&lt;/p&gt;

&lt;h2 id=&quot;model-description-on-3-branch-resnets&quot;&gt;Model description on 3-branch ResNets&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img2.png&quot; alt=&quot;Figure2&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img3.png&quot; alt=&quot;Figure3&quot; /&gt;
논문 저자는 Shake-Shake regularization을 적용하기 간편한 ResNet 구조에서 실험을 하고, 이해를 돕기 위하여 위와 같은 간단한 수식을 말합니다. 위에 보이는 수식 1은 일반적은 resnet입니다. W는 weight이고, x는 텐서, F는 residual function입니다. 논문 저자는 수식 2와 같이, F앞에 일정 α(0과 1 사이의 랜덤한 값)을 곱해줘 Internal Representations에 Data augmentation을 해주는 효과를 냅니다.&lt;/p&gt;

&lt;h2 id=&quot;training-procedure&quot;&gt;Training procedure&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/cover.png&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 shake-shake-regularization의 전체 학습 절차 입니다. 학습의 forward 부분에서도 α[0~1] 사이의 값을 곱해줘서 data augmentation 효과를 주고, backward 부분에도 β[0~1]의 값을 곱해줘서 기존의 연구 되었던 gradient noise를 주는 방식을 gradient augmentaion으로 대체해 줄 수 있다고 논문 저자는 주장합니다. 또한 테스트시에는 0.5로 값을 고정해 줍니다.&lt;/p&gt;

&lt;h2 id=&quot;cifar-10-cifar-100&quot;&gt;CIFAR-10, CIFAR-100&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img4.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 본 논문의 저자는 다양한 실험을 진행하였습니다. Forward pass는 Even / Shake, Backward pass는 Even / Shake / Keep, Mini-batch update rule 은 Image / Batch로 진행하였습니다. Even은 α값을 0.5로 고정하는 방법이고, Shake는 α값은 0~1사이의 값을 랜덤하고 곱해줍니다. Backward의 Kepp은 Forward에 사용한 α값을 그대로 사용하는것 입니다. 또한 Image는 미니배치에서 모든 이미지에 α,β 를 적용하는것이고, Batch 는 미니 배치 단위로 다 같은 α,β를 적용하는것 입니다. 위 그림과 같이 shake shake image가 가장 성능이 좋았고, Even shake batch가 가장 성능이 안 좋았습니다. shake shake image 가 가장 많은 augmentation을 적용한 것이니 당연한 결과라고 생각하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img5.png&quot; alt=&quot;Figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 배치사이즈를 128에서 32로 줄인 다음 CIFAR100에서 실험한 테이블 입니다. 위에 보이는 결과와 같이 배치사이즈가 줄어 들때는 S S I 조합 보다는 S E I 조합이 조금 더 경쟁력 있다는 것을 알 수 있습니다. 논문저자도 이와 관련해서 해석을 하지 못했습니다. 저 또한 그 이유를 해석하는데 어려움이 있어 혹시 아시는분은 댓글 부탁 드립니다. ㅠㅡㅠ&lt;/p&gt;

&lt;h2 id=&quot;comparisons-with-state-of-the-art-results&quot;&gt;Comparisons with state-of-the-art results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img6.png&quot; alt=&quot;Figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 표는 기존의 state-of-the-art 결과와 비교한 표 입니다. 보이시는 것과 같이 저자가 주장한 방법이 state-of-the-art를 달성하였습니다. CIFAR10 에서는 S S I 가 CIFAR100에는 S E I 가 가장 좋았습니다. 이 결과를 분석해보면, gradient에 β를 곱해주는 방법은 좋은 성능을 보장하지 못합니다. 제 생각에는 저자가 주장하는 방법은 gradient noise방법을 완벽하게 대체하지는 못하는 것 같습니다. 또한 논문 저나는 imagenet실험을 하지 않아 많이 아쉽습니다.(제 생각에는 imagenet에서는 잘 안될것 같긴 합니다.)&lt;/p&gt;

&lt;h2 id=&quot;correlation-between-residual-branches&quot;&gt;Correlation between residual branches&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img7.png&quot; alt=&quot;Figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 논문저자는 Shake-Shake regularization을 분석합니다. residual branches사이의 공분산을 구하여 서로의 관계성을 계산합니다. 위 그림은 feature map을 펼친후 나온 스칼라 값들로 관계성을 구한것 입니다. 거의 아무것도 안해준 조합인 Even Even Batch 와 거의 모든 augmentation을 해준 Shake Shake Image 조합을 비교 하였습니다. 그 결과, Shake Shake Image 조합에서 residual branches 값들 사이의 관계성이 작게 나왔으므로, Shake-Shake regularization을 해준다면 overfitting이 일어날 확률을 낮춰줄 수 있음을 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;후기&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img8.png&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 사실 이 논문 저자는 실험을 상당히 많이 했습니다. BatchNorm을 사용하지 않거나, skip connection을 제거하는등 많은 실험을 하였습니다. 하지만 본 리뷰에서 그런 실험을 리뷰하지 않는 이유는 너무나도 당연하기 때문입니다. BatchNorm을 사용하면 성능저하가 있고, skip connection을 제거하면 성능의 약간의 향상은 있지만 유의미 하지는 않습니다. 또한 그 행동이 CIFAR10,100이여서 상승한 것이지 imagenet이었다면 어떻게 될지 모른다는 생각을 했습니다. 이 논문 자체로는 유의미하게 좋은 논문이라고 생각들진 않지만, shake drop, mixup, cutmix등 다른 augmentation 논문을 읽을때 큰 도움이 된다고 생각이 됩니다. (또한 이 논문이 쓸 내용이 없을떄 어떻게 논문을 꽉 꽉 채울수 있나 좋은 교본으로 느껴졌습니다 ㅋㅋㅋ)&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Shake-Shake regularization 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title>
      <link href="https://blog.airlab.re.kr/2019/11/gcnet" rel="alternate" type="text/html" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond" />
      <published>2019-11-16T11:00:00+09:00</published>
      <updated>2019-11-16T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/gcnet</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/gcnet">&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!
오늘 소개할 논문은 GCNet으로 (&lt;a href=&quot;https://arxiv.org/abs/1904.11492&quot;&gt;arXiv:1904.11492&lt;/a&gt;)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.&lt;/p&gt;

&lt;p&gt;이전에 소개시켜드렸던 &lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;Non-local&lt;/a&gt;을 시각화하고, 문제점을 지적하는 논문으로 의미가 크다고 생각합니다.&lt;/p&gt;

&lt;h2 id=&quot;non-local&quot;&gt;Non-local&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/non-local.png&quot; width=&quot;60%&quot; alt=&quot;non-local&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Non-local Block은 Global Conext를 query-specific하게 접근하는 방식으로 많이 알려져 왔습니다.&lt;/p&gt;

&lt;p&gt;위 그림의 NL 블록을 보더라도 Shape이  HW x HW 로 각 포인트에 대하여 전체 포인트에 대한 Score Map을 만드는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 GCNet에서는 이러한 필터를 시각화해보았는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure2.png&quot; width=&quot;60%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 결과를 보면 각 Point(query)에 대한 NL의 시각화 맵을 볼 수 있는데, 여기서 재밌는 것은 서로 다른 Point에 대하여 모든 결과들이 거의 유사한 heatmap을 보인다는 것입니다. 이는 정량적으로도 확인을 할 수 있는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure3.png&quot; width=&quot;60%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Method는 여러 Non-local의 종류이고, E-Gaussian(Embedding Gaussian)이 Original NL 입니다.&lt;/p&gt;

&lt;p&gt;여기서의 Cosine distance를 보더라고, 서로 다른 InPut에 대하여 큰 차이가 없는 Output이 나오는 것을 정량적으로도 확인을 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;simplifying-the-non-local-block-snl&quot;&gt;Simplifying the Non-local Block (SNL)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure7.png&quot; width=&quot;50%&quot; alt=&quot;figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과처럼 모든 point에서 유사한 heatmap이 나온다고 하면, 충분히 NL의 크기를 줄일 수 있을 것이라고 생각이 드실 겁니다. 논문에서도 이러한 부분을 지적하며 경량화된 Simplifying Non-local Block을 제시하는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure4.png&quot; width=&quot;50%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식은 기존의 NL로 각 point i에 대하여, 모든 point j가 연산되는 i, j가 서로 연관적인 것으로 표현이 되고 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 위의 결과처럼 모든 포인트에 대하여, 비슷한 결과가 나오고 있으니 i와 j는 독립적(query independent)으로 아래와 같이 변경해 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure5.png&quot; width=&quot;50%&quot; alt=&quot;figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 조금 더 연산량을 줄이기 위해 W(v)를 밖으로 뺌으로서 아래의 SNL이 완셩이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure6.png&quot; width=&quot;50%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 Mask RCNN에서의 결과를 보게 되면 NL과 SNL의 성능이 크게 차이가 나지 않음을(아니 조금더 좋음을?)볼 수 있습니다. 또한 연산량과, Prams를 보더라도 SNL이 훨씬 효율적이여 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure8.png&quot; width=&quot;50%&quot; alt=&quot;figure7&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;global-context-block&quot;&gt;Global Context Block&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure9.png&quot; width=&quot;80%&quot; alt=&quot;figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진처럼 Global Context 모듈들은 Context modeling과 Transform 모델로 분리되어 표현이 될 수 있습니다. 여기서 GCNet은 SNL의 context modeling block 과 SE block의 Transform block이 합쳐진 형태로 구성이 되었습니다. 때문에 SE의 Transform에서 Conv를 2번 나누어 계산 하는 효과인 연산량을 더 낮추는 효과도 가지게 되었습니다.(SNL Transform 연산량: C&lt;em&gt;C, GCNet 연산량: C&lt;/em&gt;C/r + C/r*C)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure10.png&quot; width=&quot;50%&quot; alt=&quot;figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 위해 하이퍼 파라미터(r) 하나가 추가 되었는데요, 논문에서는 r을 4로 설정한 것이 가장 높은 성능을 보인다고 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure11.png&quot; width=&quot;50%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추가적으로 Layer Norm을 추가하여, Normalize 및 Regularize의 효과를 보았다고 합니다.&lt;/p&gt;

&lt;p&gt;따라서, 최종적으로 GCNet은 아래 수식 하나로 표현이 되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure11&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문의 실험은 Mask RCNN과 Classification, Action Recognition에서 실험되었고, 모든 보틀넥에서 GCBlock을 사용한 것이 가장 좋은 성능을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;마치며&quot;&gt;마치며..&lt;/h2&gt;

&lt;p&gt;이번 논문은 NL의 문제점을 지적하고, NL을 시각화 하는 것만으로도 충분하게 의미기 큰 논문이었던 것 같습니다. 또한 NL의 파라미터와 연산량을 줄여 기존에 한개 혹은 두개만 사용가능 했던 것을 모든 보틀넥에 삽입하여 성능을 크게 키운 것도 인상적입니다.&lt;/p&gt;

&lt;p&gt;하지만, all GCNet처럼, all SNL도 실험 결과가 있었으면 하는 아쉬움도 남습니다.&lt;/p&gt;

&lt;p&gt;이번에 ICCV2019를 다녀와서 다음번에 ICCV에 소개되었던 재밌는 논문들을 더 소개시켜드리도록 하겠습니다. 감사합니다 :)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cao, Yue, et al. “GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond.” arXiv preprint arXiv:1904.11492 (2019).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;https://j911.me/2019/11/nonlocal.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요. AiRLab(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다! 오늘 소개할 논문은 GCNet으로 (arXiv:1904.11492)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">SlowFast Network for Video Recognition</title>
      <link href="https://blog.airlab.re.kr/2019/11/SlowFast" rel="alternate" type="text/html" title="SlowFast Network for Video Recognition" />
      <published>2019-11-13T06:00:00+09:00</published>
      <updated>2019-11-13T06:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/SlowFast</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/SlowFast">&lt;p&gt;안녕하세요? 이번에 &lt;strong&gt;SlowFast&lt;/strong&gt; 논문 Review를 하게된 &lt;strong&gt;AirLab&lt;/strong&gt; 이상우입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduction&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;SlowFast는 Video Recognition 을 위한 네트워크 구조입니다. 이 네트워크는 FAIR (FaceBook Artificial Intelligence Research) 에서 발표한 논문으로
이전의 다른 네트워크들과의 다른점은 Opticalflow 를 사용하지 않은 영상 인식 네트워크 였다는 것입니다. 이로써 End-To-End 학습이 영상인식에서도 가능해졌다고 합니다.
이 논문은 영장류의 물체의 행동을 인식하는 세포에서 영감을 받았다고 하는데 자세한 내용은 밑에서 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;slowfast-networks&quot;&gt;&lt;strong&gt;&lt;u&gt;SlowFast Networks&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure1.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;

&lt;center&gt;(figure1. SlowFast Networks)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;SlowFast는 2가지의 Pathway가 있습니다. 위에 보이는 Pathway는 Slow pathway라고 부르며, 이 Pathway는 낮은 프레임으로 주어질 수 있는 의미적 정보를 포착하도록
설계가 되었다고 합니다. 간단한 예를 들면 박수라는 행동에서 손이라는 객체를 파악하는데 힘을 싣는 경로입니다. 반대로 Fast Pathway의 경우는 높은 프레임속도로 빠르게 변화하는
움직임을 포착하는 역할을 합니다. 이 부분이 기존 영상인식에서 Opticalflow 로 수행되었던 부분입니다.
Slow Pathway는 τ 값을 가지는데 대표적으로 16을 사용하였다고 합니다. 이 값은 30프레임의 영상의 경우 30프레임중에 약 2프레임 정도의 이미지만 뽑아준다고 합니다.
Fast Pathway는 t/α 값을 가지고 α의 값을 1 이상의 값을 가진다고 합니다. 논문 저자는 대표적으로 8의 값을 사용하였다고 합니다. (t = 전체 프레임) 30프레임 인 경우
약 4프레임 정도의 이미지를 뽑게 됩니다.
이 논문에서 중요한 부분은 Fast Pathway인데 이 부분은 전체 연산량의 20% 밖에 수행하지 않는다고 합니다. 저는 처음에 보고 많은 프레임을 다루는데 더 많은 연산량이
사용될거라 생각했는데 이 부분에서 중요한 아이디어가 있었습니다. Fast Pathway의 경우 위에서 Opticalflow 를 대체해서 사용된 부분이고, 움직임을 포착하는 것을 위해
설계되었다고 하였습니다. 이 부분에서 느낌이 오신분이 있을수도 있는데요. 결론적으로 &lt;strong&gt;많은 채널의 정보를 사용하지 않습니다.&lt;/strong&gt; 단순히 예를 들어 말씀을 드리면, 박수라는 행동을
인식하기 위해서 손을 인식해야되고 손이 무엇을 할수 있는지 파악을 해야합니다. 그래서 의미적 정보를 파악할 때 손이라는 객체의 색상이 살색이라면 조금 더 쉽게 손을 파악할 수 있습니다.
하지만 손이라는 정보를 알고선 행동을 파악할 떄는 손이 회색,노란색이라도 손이라는 정보를 안다면 박수라는 행동을 쉽게 인식할 수 있습니다.
논문에서는 공간과 차원에 대한 특별한 처리가 없기때문에 채널의 수가 적어도 된다고 설명이 되어있으며, 즉 이는 &lt;strong&gt;계산이 가벼워지고 처리속도가 빨라지게&lt;/strong&gt; 되었다고 합니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure2.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure2. An example instantiation of the SlowFast network)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에 이미지를 참조하시면 조금 더 이해에 도움이 되실겁니다. 초록색의 경우 Fast Pathway에서 나오는 프레임입니다. Slow Pathway보다 높은 프레임률을 사용하고 있다는
것이 확인이 됩니다. 노란색의 경우 Fast Pathway에서 사용하는 채널의 수를 보여줍니다. Slow Pathway에서 사용하는 값의 1/8 정도의 채널의 수를 사용하고 있는것도 확인이 됩니다.&lt;/p&gt;

&lt;p&gt;그런데 이쯤에서 궁금한 점이 생기셨을 겁니다. 각각의 Pathway에서 나온 출력값은 다른 형태를 띄고 있는데 어떻게 두 개를 합쳐서 영상 인식을 하게 되는걸까요?
이 부분에서는 Lateral Connections 라는 방식을 사용하여서 간단히 두개의 값의 형태를 맞춰줍니다.
Slow pathway  = {T, S^2, C}
Fast pathway  = {αT, S^2, βC}
각각 pathway에서 나온 feature의 형태는 이러한 형태를 띄는데 3가지 방식을 중점으로 형태를 바꿔주게 됩니다.
[1]. Time-to-channel : {αT, S^2, βC} 이러한 feature의 형태를 {T, S^2, αβC} 형태로 바꿔줍니다. 즉 α값을 하나 프레임의 채널로 변환을 시킵니다.
[2]. Time-strided sampling : {αT, S^2, βC} α프레임 중 하나만 샘플링하여 {T, S^2, βC}의 형태로 바줍니다.
[3]. Time-strided convolution : 2βC 의 출력 채널과 stride = α 를 가진 5×1^2 커널의 3D convolution을 사용한다고 합니다.
즉, 전체적으로 컨볼루션과 샘플링 프레임을 채널단위로 바꿔주며 Slow pathway와 Fast pathway를 맞춰줍니다.&lt;/p&gt;

&lt;h4 id=&quot;main-results&quot;&gt;&lt;strong&gt;&lt;u&gt;Main Results&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure3.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure3. Comparison with the state-of-the-art on Kinetics-400.)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure3. Accuracy/complexity tradeof.)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure5.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure3. Per-category AP on AVA.)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;제가 논문을 읽으면서 느낀점이 있는 몇가지 결과를 가지고 왔습니다. 
figure3 의 경우 Kinetics 데이터에서 Resnet 101을 백본으로 사용한 SlowFast 네트워크가 Opticalflow 를 사용한 다른 네트워크보다 좋은 성능을 보이면서
영상 인식에서 이제는 Opticalflow 보다는 RGB 채널을 통한 접근이 더 발전할 것으로 보입니다.
figure4 의 경우 SlowFast Network 에서 Slow만 단일적으로 사용했을 때보다 Slow와 Fast를 같이 사용했을 때 더 효율적인 모습이 보여졌으며 네트워크가 깊어 질수록
더 좋은 성능을 보여주었습니다.
figure5 의 경우 Fast Pathway 의 문제점이라고 할 수 있는 점이 보였습니다. 위에서 말했듯이, Slow와 Fast를 같이 썼을 경우에 전체적으로 성능향상이 있었으나 몇가지 행동은
그런게 많은 행동의 변화가 없는 경우가 있었습니다. Fast Pathway의 역할이 중요하지 않은 행동들 즉 잠을 자거나, 핸드폰으로 통화를 하는 등 움직임이 별로 없는 행동에서는 Slow Pathway 만 단독적으로 썼을 때가 더 성능이 좋았습니다. 이런 점을 보면 Fast Pathway가 정적인 행동에서는 긍정적인 영향을 미치지 않으며, 정적인 행동에서는 다른 접근법이 필요하다고 저는 느껴졌습니다.&lt;/p&gt;

&lt;p&gt;이상 SlowFast 논문 리뷰를 마치며 읽어주신 분들께 감사드리며, 틀린 점이나 고쳐야 될 부분은 댓글이나 mwlee0860@gmail.com 으로 메일을 보내주시면 감사하겠습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sangwoo Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요? 이번에 SlowFast 논문 Review를 하게된 AirLab 이상우입니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Transfer learning</title>
      <link href="https://blog.airlab.re.kr/2019/11/Transfer-learning" rel="alternate" type="text/html" title="Transfer learning" />
      <published>2019-11-13T05:00:00+09:00</published>
      <updated>2019-11-13T05:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/Transfer-learning</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/Transfer-learning">&lt;p&gt;Transfer learning 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 노현철 입니다. 
제가 이번에 리뷰할 논문은 &lt;strong&gt;“How transferable are features in deep neuralnetworks?”&lt;/strong&gt; 이른바 &lt;strong&gt;“transfer learning”&lt;/strong&gt; 에 관한 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;transfer learning이란 
적은 dataset에서 학습을 시키면 over-fitting이 일어날 가능성이 많아짐으로 많은 dataset에서 학습을 시킨 일부의 layer들을 가져와서 이 적은 dataset에 적용 시켜 적은 dataset에서도 강인함을 보여줄 수 있는 것이 transfer learning입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/01.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 데이터셋으로 학습을 시키면 초반 레이어에서는 Generality한 파라미터들이 나오고 후반 레이어에서는 Specificity한 파라미터들이 나온다. transfer learning은 초반 레이어에 나오는 Generality한 파라미터를 이용하는 것이다. 그 이유는 각각의 데이터 셋을 학습시켜 얻고자하는  목적이나 원하는 것들이 다르기 때문에 Generality한 파라미터들을 사용하는 것이다. 이 논문에서는 어디까지가 Generality하고 어디서부터 Specificity한지 실험도 하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/02.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 보듯이  Generality 레이어에서는 Gabor filters, color blobs 와 같은 것들이 학습된다.
&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/03.png&quot; alt=&quot;Figure3&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;gabor-filters&quot;&gt;Gabor filters&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/04.png&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;color-blobs&quot;&gt;color blobs&lt;/h5&gt;

&lt;h3 id=&quot;experimental&quot;&gt;Experimental&lt;/h3&gt;

&lt;p&gt;● 1000개의 이미지넷에서 500(A), 500(B) 로 나누어 실험을 한다.&lt;/p&gt;

&lt;p&gt;● A, B 모두 총 8개의 conv레이어를 사용할 것이다.&lt;/p&gt;

&lt;p&gt;● transferred layers는 frozen시키거나 fine-tuned시키는 실험이다. fine-tuned을 사용한 것은 (+)가 쓰여져 있는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/05.png&quot; alt=&quot;Figure5&quot; /&gt;
그림에서 보듯이 baseA 와 baseB는 transfer없이 학습을 시킨 것이고 BnB, AnB는 각각 B에서 학습시킨 파라미터를 B에 적용, A에서 학습시킨 파라미터를 B에 적용한다는 의미이다. (+)가 붙은 경우들은 fine-tuned을 시킨경우들이다. (기본적으로 transferred layers들은 frozen 시킴) 
transfer시키는 레이어의 범위는 1번째 레이어 에서부터 마지막 8번째 레이어 까지 각각 실험하여서 어디까지 General 한지보고, 또한 fine-tuning 하였을 때 성능의 변화가 있는지 보는 실험이다.&lt;/p&gt;
&lt;h4 id=&quot;results&quot;&gt;results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/06.png&quot; alt=&quot;Figure6&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;bnb&quot;&gt;BnB&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(n=1, 2) base B와 동일&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;(n=3,4,5) 성능저하
    &lt;ul&gt;
      &lt;li&gt;레이어간 co-adapted(동화기능)이 있어 상위 레이어만 이 기능을 배울 수 없다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(n=6,7) 성능 다소상향
    &lt;ul&gt;
      &lt;li&gt;학습의 필요성이 점차 줄어듦.&lt;/li&gt;
      &lt;li&gt;(6,7or7,8)사이 features가 co-adapted이 덜하다.&lt;/li&gt;
      &lt;li&gt;중간보다 하단,상단이 optimization 하기 좋다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;bnb-1&quot;&gt;BnB+&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;전체적으로 base B와 비슷
    &lt;ul&gt;
      &lt;li&gt;co-adapted(동화기능)의 성능저하를 방지시켜줌.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;성능향상은 없었음.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;anb&quot;&gt;AnB&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;일부 layer는 Transfer Learning 하는것이 좋다.&lt;/li&gt;
  &lt;li&gt;(n=1,2) layer는 general 하다.&lt;/li&gt;
  &lt;li&gt;(n=3) 약간감소, (n=4,5,6,7) 성능대폭하락
    &lt;ul&gt;
      &lt;li&gt;이것으로 인해 두 가지 감소 이유를 알 수 있음.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;1) (n=3,4,5) co-adaptation으로 인한 감소&lt;/p&gt;

    &lt;p&gt;2) (n=6,7) generality 보단 specificity 하기 때문&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;anb-1&quot;&gt;AnB+&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Fine-tuning은 좋은 성능을 보여줌.
    &lt;ul&gt;
      &lt;li&gt;Transfer Learning의 목적과 반대로 dataset이 많은 경우에도 사용하면 성능을 향상 시킬 수 있다!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(n=1~7) 성능을 유지, 성능 향상
    &lt;ul&gt;
      &lt;li&gt;이 효과는 첫번째 네트워크(A)의 양에 크게 의존하지 않음.&lt;/li&gt;
      &lt;li&gt;이 효과는 너무 많은 retraining을 한다는 것은 놀라운 일.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;co-adapted 때문에 중간layers에서 분할하는 것은 어렵다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;상위layers의 specialization로 인해 Transfer Learning의 부정적인 영향을 끼친다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;random weights보단 Transfer Learning이 좋다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;new task에 Transfer Learning, fine-tuning을 더하면 성능을 향상 시키는데 유용할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;랩실에 들어오고 첫 세미나 발표를 한 논문이고, 또한 끝까지 읽어본 몇 안되는 논문이라 더욱 뜻깊고 기억에 남는 논문이였습니다.
지금은 많이 부족하지만 세미나와 논문리뷰를 하면서 실력을 한층 한층 쌓아 나아가겠습니다. 감사합니다!!!!!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Hyeoncheol Noh</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Transfer learning 리뷰</summary>
      

      
      
    </entry>
  
</feed>
