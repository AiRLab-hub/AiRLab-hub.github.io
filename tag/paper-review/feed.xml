<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/tag/paper-review/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-07-24T15:11:08+09:00</updated>
  <id>https://blog.airlab.re.kr/tag/paper-review/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">DenseNet</title>
      <link href="https://blog.airlab.re.kr/2017/07/densenet" rel="alternate" type="text/html" title="DenseNet" />
      <published>2017-07-24T22:00:00+09:00</published>
      <updated>2017-07-24T22:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2017/07/densenet</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2017/07/densenet">&lt;p&gt;DenseNet: Densely Connected Convolutional Networks&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다. :)&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Densely Connected Convolutional Networks&lt;/strong&gt;(DenseNet) (&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;arXiv:1608.06993&lt;/a&gt;)이며 2016년 처음 발표가 된 연구입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/resnet-figure1.png&quot; alt=&quot;resnet-figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet에서의 shortcut Connection과 같은 Shorter Connection의 연구를 통해 과거보다 더 깊고, 더 정확하며, 더 효율적인 네트워크가 만들어져왔습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 ResNet과 비슷하지만 다른 네트워크(DenseNet)를 제안하고있습니다.&lt;/p&gt;

&lt;p&gt;DenseNet은 아래와 같이 각각의 레이어의 모든 레어어가 모두 feed-forward로 연결되어있는 구조로 되어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/figure1.png&quot; width=&quot;50%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 DenseBlock의 Convolution Layer는 이전의 모든 Convolution Layer의 Output들과 Concatenate되어 Convolution되게 됩니다. 여기서 ResNet과의 차이점이 들어나게 되는데 ResNet은 과거의 Feature Map이 서로 더해지는 반면에 DenseNet에서는 각 Feature Map들이 Concat 되는 것입니다. 이를 통해 DenseNet은 과거의 Feature Map과 현재의 Feature Map이 서로 섞이지 않고 학습이 될 수 있도록 합니다.&lt;/p&gt;

&lt;p&gt;이러한 구조는 아래와 같은 장점을 발생시킵니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Vanishing-gradient 문제 완화&lt;/li&gt;
  &lt;li&gt;더 강력한 피쳐 프로파게이션이 가능&lt;/li&gt;
  &lt;li&gt;피쳐 재사용을 촉진&lt;/li&gt;
  &lt;li&gt;파라미터의 수를 감소&lt;/li&gt;
  &lt;li&gt;Regularlizing 효과와 Overfitting 감소&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그리고 이 논문은 여러 벤치마크 DB인 CIFAR10, CIFAR100, SVHN, ImageNet에서 검증되었고, 논문발표 당시 state-of-the-art를 달성했습니다.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/table1.png&quot; width=&quot;80%&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DenseNet은 표와 같은 구조로 설계가 되어있습니다. 여기에서 차별점이 되는 Dense Block과, Transition Block을 한번 알아봅시다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dense Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dense Block은 DenseNet의 가장 중요한 컨셉이 포함된 블록입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/figure1.png&quot; width=&quot;50%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진과 같이 각 레이어는 이전의 레이어의 Output을 계속 Concat하여 Feed-forward 합니다. 그러다 보면 어느 지점에서는 채널이 아주 커지는 경우가 생길 수 있는데, 이 때문에 이 논문에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Growth rate&lt;/code&gt;이라는 개념을 도입합니다. 각 블록의 Convolution들은 Growth rate만큼의 채널만 Output하여 너무 큰 Output이 발생하지 않도록 해줍니다. 따라서, 위 그림은 Growth rate가 4인 경우의 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transition Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DenseNet은 Feature를 Down-sampling하기 위해 Transition Block을 사용합니다. Transition Block은  1x1 Convolution과 average pooling으로 구성이 돠어있으며, DenseNet의 하이퍼파라미터인 &lt;code class=&quot;highlighter-rouge&quot;&gt;theta&lt;/code&gt;값을 통해 다운샘플링하게 됩니다. 기본으로는 theta에 0.5를 주어 average pooling시 stride를 2 (1/theta)로 주어 다운샘플링하게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/table2.png&quot; width=&quot;80%&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문에서는 여러 환경에서의 실험을 진행하였는데, DesneNet이 가장 뛰여난 성능을 보이는 것으로 나타났습니다.(수치는 test error(%))&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-densenet-for-cifar10&quot;&gt;Implementation of DenseNet for CIFAR10&lt;/h3&gt;

&lt;p&gt;실제 논문을 &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;를 통해 모델을 구현해보았고, 코드는 &lt;a href=&quot;https://github.com/J911/DenseNet-pytorch&quot;&gt;https://github.com/J911/DenseNet-pytorch&lt;/a&gt;에 배포해 두었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/result.png&quot; width=&quot;80%&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;color&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;growth_rate&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;theta&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;dropout&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;scheduler&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;test error(%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Orange&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;lr_decay [60, 120, 160]&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Blue&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;lr_decay [60, 120, 160]&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pink&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CosineAnnealingLR&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.86&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Huang, Gao, et al. “Densely connected convolutional networks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">DenseNet: Densely Connected Convolutional Networks</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">MobileNetV2</title>
      <link href="https://blog.airlab.re.kr/2017/07/mobilenetv2" rel="alternate" type="text/html" title="MobileNetV2" />
      <published>2017-07-22T19:00:00+09:00</published>
      <updated>2017-07-22T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2017/07/mobilenetv2</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2017/07/mobilenetv2">&lt;p&gt;MobileNetV2 : Inverted Residuals and Linear Bottlenecks 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 MobileNetV2 : Inverted Residuals and Linear Bottlenecks 입니다. 간단하게 한 줄로 이 논문을 소개하자면 모바일이나, 임베디드에서도 실시간을 작동할 수 있게 모델이 경량화 되면서도, 정확도 또한 많이 떨어지지 않게하여, 속도와 정확도 사이의 트레이드 오프 문제를 어느정도 해결한 네트워크 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure4.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 이 논문을 읽기전에 알아두면 좋은 Related Works는 아래 두 논문 입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Xception: Deep Learning with Depthwise Separable Convolutions(https://arxiv.org/abs/1610.02357)&lt;/li&gt;
  &lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(https://arxiv.org/abs/1704.04861)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;개인적으로 이 논문을 읽으면서 기존에 존재했던 “Xception”, “MobileNets” 과의 크게 다른점을 저는 느끼지 못했습니다(그렇기에 이 논문을 제대로 이해 하시려면 앞에서 언급한 두 논문을 읽어보시는걸 추천 드립니다). 이 논문은 저자들이 “Xception”에서 제안했던  Depthwise Separable Convolutions을 그대로 사용합니다. 또한 Depthwise Separable Convolutions이 사용하는 철학, 가설을 그대로 채택합니다. 의식의 흐름대로 읽다보면 Depthwise Separable Convolutions이 뭐지? 하는 질문이 당연히 드실꺼라고 생각합니다.&lt;/p&gt;

&lt;h3 id=&quot;depthwise-separable-convolutions이란&quot;&gt;Depthwise Separable Convolutions이란?&lt;/h3&gt;

&lt;p&gt;Depthwise Separable Convolutions을 아주 간단하게 요약하면 &lt;strong&gt;Depthwise Convolutions + Pointwise Convolutions&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Depthwise Separable Convolutions을 설명하기 전에 기존의 Convolutions을 생각해 봅시다.
&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure5.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에 보이시는 것 처럼 기존의 Convolutions은 채널과 과 필터가 동시에 고려되서 최종 아웃풋을 만듭니다. 하지만 이 논문의 저자는 cross-channels correlation(입력 채널들 사이의 유사도)과 spatial correlation(필터와 하나의 특정 채널 사이의 관계)이 완전하게 독립적이기 때문에 &lt;strong&gt;채널과 필터를 따로 분리해서 학습&lt;/strong&gt;을 진행해도 문제가 없다고 주장합니다. 실제로 연상량을 계산해보면 Traditional convolutions은 &lt;strong&gt;입력 이미지의 크기x입력 이미지의 채널x 커널사이즈 제곱x아웃풋채널&lt;/strong&gt; 이지만 Depthwise Separable Convolutions의 연산량은 &lt;strong&gt;입력 이미지의 크기x입력 이미지의 채널x (커널사이즈 제곱+아웃풋채널)&lt;/strong&gt; 이기 때문에 &lt;strong&gt;8~9배&lt;/strong&gt; 정도 연산량이 줄어듭니다.(커널사이즈는 3 이라고 가정합니다)&lt;/p&gt;

&lt;p&gt;다음으로 이 논문에서 주장하는 Linear Bottlenecks 입니다.&lt;/p&gt;

&lt;h3 id=&quot;linear-bottlenecks-이란&quot;&gt;Linear Bottlenecks 이란?&lt;/h3&gt;

&lt;p&gt;지금부터 조금 어려운 이야기를 직관적으로 쉽게 풀이하겠습니다(논문에서도 직관이라는 단어를 많이 사용합니다). 우선 manifold라는 말을 알고 있으셔야 합니다.
manifold란 어떤 이미지의 차원들이 존재하는 공간이라고 생각하시면 됩니다. 이 논문에서는 Manifold의 가설을 언급합니다(It has been longassumed  that  manifolds  of  interest  in  neural  networkscould be embedded in low-dimensional subspaces.). manifold 가설은 고차원의 정보는 사실 저차원으로 표현 가능하다는 것입니다. 예를 들어서 설명하면, 실제 세상에 존재하는 모든 사물들은 3차원 이라고 이야기를 하지만 사람들은 실제로 사물을 구분할 때는 2차원 정보를 받아들여 사물을 구분할 수 있다는 것 입니다. 즉 고차원 정보는 사실 저차원 정보로도 충분히 구분 할 수 있다는 것 입니다.&lt;/p&gt;

&lt;p&gt;지금까지 Manifold에 대하여 설명한 이유는 이 논문에서 Linear Bottlenecks을 만들때 1x1의 pointwise Convolutions을 하여 차원수를 줄이기 때문입니다. &lt;strong&gt;Manifold의 가설 그대로 고차원의 채널은 사실 저차원의 채널로 표현할 수 있다&lt;/strong&gt; 라는 논리 전개 입니다.(채널을 과도하게 줄이면 안됩니다. 예를들어서 사람은 3차원의 정보를 2차원으로 구분하지만 1차원으로는 구분 못하는 것과 같습니다.)&lt;/p&gt;

&lt;p&gt;Linear Bottlenecks에서 주장하는 또 다른 하나는 ReLU는 필연적으로 정보 손실을 야기하기 때문에 어떤 특별한 작용을 해줘서 그 정보손실을 방어해야 한다는 것 입니다. 이제 그 특별한 작용에 대하여 말씀드리겠습니다.&lt;/p&gt;

&lt;p&gt;시작하기 전에 가장 간단히 한줄로 요약하면 &lt;strong&gt;“채널수가 충분히 많으면 ReLU를 사용해도 중요 정보는 보존된다!”&lt;/strong&gt; 입니다. 이 문장을 계속 상기시키면서 글을 읽으시면 이해하시는데 도움이 되실 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure8.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 보이시는 그림처럼 채널이 1인 데이터가 ReLU를 지나면 중요 정보가 삭제 될 수 도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure9.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 위에 보이시는 그림처럼 채널이 2 인 데이터가 ReLU를 지나면 중요 정보가 삭제 되더라도 다른 채널에서는 아직까지 존재할 가능성이 채널이 많으면 많을수록 높기 때문에 &lt;strong&gt;채널이 많을때 ReLU를 사용하면 괜찮다는 것&lt;/strong&gt; 입니다.(어차피 나중에 전부 합쳐져서 예측하기 때문에)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure6.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 보이시는 그림은 이 주장을 실험적으로 증명한 것 입니다. 차원을 2, 3, 5, 15, 30 을 각각쓰고 ReLU를 쓰고 원래대로 복원하였습니다. 그림에서 보이는 것 과 같이 차원이 적을때는 ReLU를 쓰면 정보가 손실되어 원본 영상을 복원할 수 없지만 차원을 충분히 늘리고 ReLU를 쓰면 15, 30 과 같이 잘 복원 할 수 있다는 것 입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 Linear Bottlenecks은 ReLU를 적용하지 않습니다. 위에서 말씀드린것과 같이 차원이 매우많이 축소된 상태이기 때문에 ReLU를 사용하면 정보손실이 있을 수도 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure3.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험적으로 증명을 했는데, Linear Bottlenecks에 ReLU6을 썻을때와 안썻을때의 정확도의 차이 입니다.(ReLU6를 사용하는 이유는 연산량에 있어서 이득을 볼 수 있다고 알아보았는데 정확하진 않습니다. 혹시 정확한 이유를 아시는분은 댓글 부탁드립니다.) 또 shortcut 위치에 대한 실험도 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;inverted-residuals이란&quot;&gt;Inverted Residuals이란?&lt;/h3&gt;

&lt;p&gt;앞에서 설명드린 Depthwise Separable Convolutions과 Linear Bottlenecks을 결합하면 Inverted Residuals 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure1.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존의 Residuals을 거꾸로 뒤집은 모양이라 Inverted Residuals이라고 부르는것 같습니다. 앞에서 언급한 논리되로 ReLU를 사용해야 하기 때문에 채널을 확장(pointwise Convolutions)하고 Depthwise Convolutions을 진행합니다. 또 Linear Bottlenecks에서 대로 다시 채널수를 줄입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure2.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;stride가 1일때는 shortcut이 있지만 strdie가 2 일때는 shortcut 이 없습니다. 이유는 논문에서 설명하지 않고 있지만 이미지의 크기가 줄어들때 정보의 선형성이 보장되 않기 때문이라고 추측하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;memory-efficient-inference&quot;&gt;Memory Efficient Inference&lt;/h3&gt;

&lt;p&gt;논문에서는 gpu에서 내부 메모리와 외부 메모리가 있기 때문에 내부로 올릴때의 크기과 나갈때의 크기만 중요하기때문에 메모리 스왑적인 부분에서 봤을때도 이 논문에서 제안한 Inverted Residuals구조가 효율적이라고 주장하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;논문리뷰 끝입니다. 논문의 Conclusions은 개인적인 견해가 필요하고, 이 부분은 이 글을 읽고 있는 독자 여러분이 편견없이 논문을 읽으면 좋겠다고 생각하여 리뷰하지 않겠습니다.&lt;/p&gt;

&lt;p&gt;코드는 cifar10에 적용한 것이고 &lt;a href=&quot;https://github.com/seominseok0429/cifar10-mobilenetv2-pytorch&quot;&gt;https://github.com/seominseok0429/cifar10-mobilenetv2-pytorch&lt;/a&gt; 에 배포해 두었습니다.&lt;/p&gt;

&lt;p&gt;끝까지 읽어 주셔서 감사합니다!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">MobileNetV2 : Inverted Residuals and Linear Bottlenecks 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Wide Residual Networks</title>
      <link href="https://blog.airlab.re.kr/2017/07/WRN" rel="alternate" type="text/html" title="Wide Residual Networks" />
      <published>2017-07-17T19:00:00+09:00</published>
      <updated>2017-07-17T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2017/07/WRN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2017/07/WRN">&lt;p&gt;Wide Residual Networks&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다. 앞으로 연구실 세미나 준비를 통해 알게 된 내용을 연구실 블로그에 기록하게 되었습니다 :)&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Wide Residual Networks&lt;/strong&gt;(Wide ResNet) (&lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;arXiv:1605.07146&lt;/a&gt;)이며 2016년 발표가 된 연구입니다.&lt;/p&gt;

&lt;p&gt;같은 과거의 연구들은 점진적으로 CNN(Convolution Neural Networks)이 이미지 인식을 위해 더 깊어질 수 있도록 연구되어왔습니다.&lt;/p&gt;

&lt;p&gt;이를 위해 Optimizer, Initializer, Skip-connection과 같은 많은 방법들이 연구되어왔고, 그중, ResNet(Residual Networks)는 shortcut connection을 통해 네트워크가 깊어지면 깊어질수록 생기는 &lt;code class=&quot;highlighter-rouge&quot;&gt;vanishing gradient&lt;/code&gt;를 해결하면서도 더 깊게 네트워크를 쌓을 수 있도록 설계되었습니다.&lt;/p&gt;

&lt;h3 id=&quot;width-vs-depth-in-residual-networks&quot;&gt;Width vs Depth in residual Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/figure1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet은 shorcut connection을 통해 많은 layer을 학습 할 수 있도록 하였습니다. 하지만 망이 깊어지면 깊어질 수록 의미있는 정보(context)를 갖는 필터의 수의 비가 적어지는 문제가 발생하게 되었습니다.&lt;/p&gt;

&lt;p&gt;때문에 저자는 Block의 수를 증가시키지 않고, Residual Block의 Channel을 증가시키는 방향으로 연구를 시도하였습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 Residual Block을 (a), (c)의 구조와 같이 3x3 컨볼루션이 두 개로 이루어진 경우를 B(3,3)으로 표기하였습니다. 이와 마찬가지로 (b)의 경우는 B(1,3,1) (d)는 B(3,1,3)으로 표기가 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table1.png&quot; alt=&quot;Table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;B(3,3)의 경우 구조가 위와 같이 정의됩니다. K 는 widen factor, N은 블록의 수(각 Residual Block의 수)입니다. 이해를 돕기 위해 K가 2인 경우 아래와 같이 구현이 될 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WideResNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# pseudo code...&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;/h3&gt;
&lt;p&gt;Dropout은 Coadaptive하고 overfitting을 막기 위해 많은 네트워크에 적용되어 왔습니다. 추가적으로 Internal Covariate Shift 이슈를 막기위한 Batch Norm 과 같은 방법들도 연구가되었는데, 이 방법들은 Regularizer의 효과도 볼 수 있습니다.
이 논문에서는 Residual Block의 컨볼루션 사이에 Dropout(Dorp rate 0.3)을 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental result&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table5.png&quot; alt=&quot;Table5&quot; /&gt;
본 논문에서는 CIFAR10과 CIFAR100을 여러 모델을 통해 실험하였고, 위의 결과와 같이 WRN(depth 28, k 10)이 test error 4로 가장 높은 성능을 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/figure2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;실선은 test error, 점선은 train error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table6.png&quot; alt=&quot;Table6&quot; /&gt;
추가적으로 이 논문에서는 Dropout을 사용한 것과 사용하지 않은 것을 비교하였는데, 이 결과 역시 드롭아웃을 사용한 것이 더 좋은 결과를 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-wrn&quot;&gt;Implementation of WRN&lt;/h3&gt;

&lt;p&gt;실제 논문을 &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;를 통해 모델을 구현해보았고, 코드는 &lt;a href=&quot;https://github.com/J911/WRN-pytorch&quot;&gt;https://github.com/J911/WRN-pytorch&lt;/a&gt;에 배포해 두었습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Zagoruyko, Sergey, and Nikos Komodakis. “Wide residual networks.” arXiv preprint arXiv:1605.07146 (2016).&lt;/li&gt;
  &lt;li&gt;He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Wide Residual Networks</summary>
      

      
      
    </entry>
  
</feed>
