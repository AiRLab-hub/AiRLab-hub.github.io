<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="https://blog.airlab.re.kr/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" /><updated>2019-11-16T22:27:16+09:00</updated><id>https://blog.airlab.re.kr/</id><title type="html">AiRLab. Research Blog</title><subtitle>Artificial intelligence and Robotics Laboratory</subtitle><entry><title type="html">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title><link href="https://blog.airlab.re.kr/2019/11/gcnet" rel="alternate" type="text/html" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond" /><published>2019-11-16T11:00:00+09:00</published><updated>2019-11-16T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/gcnet</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/gcnet">&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!
오늘 소개할 논문은 GCNet으로 (&lt;a href=&quot;https://arxiv.org/abs/1904.11492&quot;&gt;arXiv:1904.11492&lt;/a&gt;)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.&lt;/p&gt;

&lt;p&gt;이전에 소개시켜드렸던 &lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;Non-local&lt;/a&gt;을 시각화하고, 문제점을 지적하는 논문으로 의미가 크다고 생각합니다.&lt;/p&gt;

&lt;h2 id=&quot;non-local&quot;&gt;Non-local&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/non-local.png&quot; width=&quot;60%&quot; alt=&quot;non-local&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Non-local Block은 Global Conext를 query-specific하게 접근하는 방식으로 많이 알려져 왔습니다.&lt;/p&gt;

&lt;p&gt;위 그림의 NL 블록을 보더라도 Shape이  HW x HW 로 각 포인트에 대하여 전체 포인트에 대한 Score Map을 만드는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 GCNet에서는 이러한 필터를 시각화해보았는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure2.png&quot; width=&quot;60%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 결과를 보면 각 Point(query)에 대한 NL의 시각화 맵을 볼 수 있는데, 여기서 재밌는 것은 서로 다른 Point에 대하여 모든 결과들이 거의 유사한 heatmap을 보인다는 것입니다. 이는 정량적으로도 확인을 할 수 있는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure3.png&quot; width=&quot;60%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Method는 여러 Non-local의 종류이고, E-Gaussian(Embedding Gaussian)이 Original NL 입니다.&lt;/p&gt;

&lt;p&gt;여기서의 Cosine distance를 보더라고, 서로 다른 InPut에 대하여 큰 차이가 없는 Output이 나오는 것을 정량적으로도 확인을 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;simplifying-the-non-local-block-snl&quot;&gt;Simplifying the Non-local Block (SNL)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure7.png&quot; width=&quot;50%&quot; alt=&quot;figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과처럼 모든 point에서 유사한 heatmap이 나온다고 하면, 충분히 NL의 크기를 줄일 수 있을 것이라고 생각이 드실 겁니다. 논문에서도 이러한 부분을 지적하며 경량화된 Simplifying Non-local Block을 제시하는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure4.png&quot; width=&quot;50%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식은 기존의 NL로 각 point i에 대하여, 모든 point j가 연산되는 i, j가 서로 연관적인 것으로 표현이 되고 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 위의 결과처럼 모든 포인트에 대하여, 비슷한 결과가 나오고 있으니 i와 j는 독립적(query independent)으로 아래와 같이 변경해 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure5.png&quot; width=&quot;50%&quot; alt=&quot;figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 조금 더 연산량을 줄이기 위해 W(v)를 밖으로 뺌으로서 아래의 SNL이 완셩이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure6.png&quot; width=&quot;50%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 Mask RCNN에서의 결과를 보게 되면 NL과 SNL의 성능이 크게 차이가 나지 않음을(아니 조금더 좋음을?)볼 수 있습니다. 또한 연산량과, Prams를 보더라도 SNL이 훨씬 효율적이여 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure8.png&quot; width=&quot;50%&quot; alt=&quot;figure7&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;global-context-block&quot;&gt;Global Context Block&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure9.png&quot; width=&quot;80%&quot; alt=&quot;figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진처럼 Global Context 모듈들은 Context modeling과 Transform 모델로 분리되어 표현이 될 수 있습니다. 여기서 GCNet은 SNL의 context modeling block 과 SE block의 Transform block이 합쳐진 형태로 구성이 되었습니다. 때문에 SE의 Transform에서 Conv를 2번 나누어 계산 하는 효과인 연산량을 더 낮추는 효과도 가지게 되었습니다.(SNL Transform 연산량: C&lt;em&gt;C, GCNet 연산량: C&lt;/em&gt;C/r + C/r*C)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure10.png&quot; width=&quot;50%&quot; alt=&quot;figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 위해 하이퍼 파라미터(r) 하나가 추가 되었는데요, 논문에서는 r을 4로 설정한 것이 가장 높은 성능을 보인다고 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure11.png&quot; width=&quot;50%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추가적으로 Layer Norm을 추가하여, Normalize 및 Regularize의 효과를 보았다고 합니다.&lt;/p&gt;

&lt;p&gt;따라서, 최종적으로 GCNet은 아래 수식 하나로 표현이 되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure11&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문의 실험은 Mask RCNN과 Classification, Action Recognition에서 실험되었고, 모든 보틀넥에서 GCBlock을 사용한 것이 가장 좋은 성능을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;마치며&quot;&gt;마치며..&lt;/h2&gt;

&lt;p&gt;이번 논문은 NL의 문제점을 지적하고, NL을 시각화 하는 것만으로도 충분하게 의미기 큰 논문이었던 것 같습니다. 또한 NL의 파라미터와 연산량을 줄여 기존에 한개 혹은 두개만 사용가능 했던 것을 모든 보틀넥에 삽입하여 성능을 크게 키운 것도 인상적입니다.&lt;/p&gt;

&lt;p&gt;하지만, all GCNet처럼, all SNL도 실험 결과가 있었으면 하는 아쉬움도 남습니다.&lt;/p&gt;

&lt;p&gt;이번에 ICCV2019를 다녀와서 다음번에 ICCV에 소개되었던 재밌는 논문들을 더 소개시켜드리도록 하겠습니다. 감사합니다 :)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cao, Yue, et al. “GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond.” arXiv preprint arXiv:1904.11492 (2019).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;https://j911.me/2019/11/nonlocal.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaemin Lee</name></author><category term="paper-review" /><summary type="html">안녕하세요. AiRLab(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다! 오늘 소개할 논문은 GCNet으로 (arXiv:1904.11492)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.</summary></entry><entry><title type="html">SlowFast Network for Video Recognition</title><link href="https://blog.airlab.re.kr/2019/11/SlowFast" rel="alternate" type="text/html" title="SlowFast Network for Video Recognition" /><published>2019-11-13T06:00:00+09:00</published><updated>2019-11-13T06:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/SlowFast</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/SlowFast">&lt;p&gt;안녕하세요? 이번에 &lt;strong&gt;SlowFast&lt;/strong&gt; 논문 Review를 하게된 &lt;strong&gt;AirLab&lt;/strong&gt; 이상우입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduction&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;SlowFast는 Video Recognition 을 위한 네트워크 구조입니다. 이 네트워크는 FAIR (FaceBook Artificial Intelligence Research) 에서 발표한 논문으로
이전의 다른 네트워크들과의 다른점은 Opticalflow 를 사용하지 않은 영상 인식 네트워크 였다는 것입니다. 이로써 End-To-End 학습이 영상인식에서도 가능해졌다고 합니다.
이 논문은 영장류의 물체의 행동을 인식하는 세포에서 영감을 받았다고 하는데 자세한 내용은 밑에서 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;slowfast-networks&quot;&gt;&lt;strong&gt;&lt;u&gt;SlowFast Networks&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure1.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;

&lt;center&gt;(figure1. SlowFast Networks)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;SlowFast는 2가지의 Pathway가 있습니다. 위에 보이는 Pathway는 Slow pathway라고 부르며, 이 Pathway는 낮은 프레임으로 주어질 수 있는 의미적 정보를 포착하도록
설계가 되었다고 합니다. 간단한 예를 들면 박수라는 행동에서 손이라는 객체를 파악하는데 힘을 싣는 경로입니다. 반대로 Fast Pathway의 경우는 높은 프레임속도로 빠르게 변화하는
움직임을 포착하는 역할을 합니다. 이 부분이 기존 영상인식에서 Opticalflow 로 수행되었던 부분입니다.
Slow Pathway는 τ 값을 가지는데 대표적으로 16을 사용하였다고 합니다. 이 값은 30프레임의 영상의 경우 30프레임중에 약 2프레임 정도의 이미지만 뽑아준다고 합니다.
Fast Pathway는 t/α 값을 가지고 α의 값을 1 이상의 값을 가진다고 합니다. 논문 저자는 대표적으로 8의 값을 사용하였다고 합니다. (t = 전체 프레임) 30프레임 인 경우
약 4프레임 정도의 이미지를 뽑게 됩니다.
이 논문에서 중요한 부분은 Fast Pathway인데 이 부분은 전체 연산량의 20% 밖에 수행하지 않는다고 합니다. 저는 처음에 보고 많은 프레임을 다루는데 더 많은 연산량이
사용될거라 생각했는데 이 부분에서 중요한 아이디어가 있었습니다. Fast Pathway의 경우 위에서 Opticalflow 를 대체해서 사용된 부분이고, 움직임을 포착하는 것을 위해
설계되었다고 하였습니다. 이 부분에서 느낌이 오신분이 있을수도 있는데요. 결론적으로 &lt;strong&gt;많은 채널의 정보를 사용하지 않습니다.&lt;/strong&gt; 단순히 예를 들어 말씀을 드리면, 박수라는 행동을
인식하기 위해서 손을 인식해야되고 손이 무엇을 할수 있는지 파악을 해야합니다. 그래서 의미적 정보를 파악할 때 손이라는 객체의 색상이 살색이라면 조금 더 쉽게 손을 파악할 수 있습니다.
하지만 손이라는 정보를 알고선 행동을 파악할 떄는 손이 회색,노란색이라도 손이라는 정보를 안다면 박수라는 행동을 쉽게 인식할 수 있습니다.
논문에서는 공간과 차원에 대한 특별한 처리가 없기때문에 채널의 수가 적어도 된다고 설명이 되어있으며, 즉 이는 &lt;strong&gt;계산이 가벼워지고 처리속도가 빨라지게&lt;/strong&gt; 되었다고 합니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure2.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure2. An example instantiation of the SlowFast network)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에 이미지를 참조하시면 조금 더 이해에 도움이 되실겁니다. 초록색의 경우 Fast Pathway에서 나오는 프레임입니다. Slow Pathway보다 높은 프레임률을 사용하고 있다는
것이 확인이 됩니다. 노란색의 경우 Fast Pathway에서 사용하는 채널의 수를 보여줍니다. Slow Pathway에서 사용하는 값의 1/8 정도의 채널의 수를 사용하고 있는것도 확인이 됩니다.&lt;/p&gt;

&lt;p&gt;그런데 이쯤에서 궁금한 점이 생기셨을 겁니다. 각각의 Pathway에서 나온 출력값은 다른 형태를 띄고 있는데 어떻게 두 개를 합쳐서 영상 인식을 하게 되는걸까요?
이 부분에서는 Lateral Connections 라는 방식을 사용하여서 간단히 두개의 값의 형태를 맞춰줍니다.
Slow pathway  = {T, S^2, C}
Fast pathway  = {αT, S^2, βC}
각각 pathway에서 나온 feature의 형태는 이러한 형태를 띄는데 3가지 방식을 중점으로 형태를 바꿔주게 됩니다.
[1]. Time-to-channel : {αT, S^2, βC} 이러한 feature의 형태를 {T, S^2, αβC} 형태로 바꿔줍니다. 즉 α값을 하나 프레임의 채널로 변환을 시킵니다.
[2]. Time-strided sampling : {αT, S^2, βC} α프레임 중 하나만 샘플링하여 {T, S^2, βC}의 형태로 바줍니다.
[3]. Time-strided convolution : 2βC 의 출력 채널과 stride = α 를 가진 5×1^2 커널의 3D convolution을 사용한다고 합니다.
즉, 전체적으로 컨볼루션과 샘플링 프레임을 채널단위로 바꿔주며 Slow pathway와 Fast pathway를 맞춰줍니다.&lt;/p&gt;

&lt;h4 id=&quot;main-results&quot;&gt;&lt;strong&gt;&lt;u&gt;Main Results&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure3.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure3. Comparison with the state-of-the-art on Kinetics-400.)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure3. Accuracy/complexity tradeof.)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-08-SlowFast/figure5.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(figure3. Per-category AP on AVA.)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;제가 논문을 읽으면서 느낀점이 있는 몇가지 결과를 가지고 왔습니다. 
figure3 의 경우 Kinetics 데이터에서 Resnet 101을 백본으로 사용한 SlowFast 네트워크가 Opticalflow 를 사용한 다른 네트워크보다 좋은 성능을 보이면서
영상 인식에서 이제는 Opticalflow 보다는 RGB 채널을 통한 접근이 더 발전할 것으로 보입니다.
figure4 의 경우 SlowFast Network 에서 Slow만 단일적으로 사용했을 때보다 Slow와 Fast를 같이 사용했을 때 더 효율적인 모습이 보여졌으며 네트워크가 깊어 질수록
더 좋은 성능을 보여주었습니다.
figure5 의 경우 Fast Pathway 의 문제점이라고 할 수 있는 점이 보였습니다. 위에서 말했듯이, Slow와 Fast를 같이 썼을 경우에 전체적으로 성능향상이 있었으나 몇가지 행동은
그런게 많은 행동의 변화가 없는 경우가 있었습니다. Fast Pathway의 역할이 중요하지 않은 행동들 즉 잠을 자거나, 핸드폰으로 통화를 하는 등 움직임이 별로 없는 행동에서는 Slow Pathway 만 단독적으로 썼을 때가 더 성능이 좋았습니다. 이런 점을 보면 Fast Pathway가 정적인 행동에서는 긍정적인 영향을 미치지 않으며, 정적인 행동에서는 다른 접근법이 필요하다고 저는 느껴졌습니다.&lt;/p&gt;

&lt;p&gt;이상 SlowFast 논문 리뷰를 마치며 읽어주신 분들께 감사드리며, 틀린 점이나 고쳐야 될 부분은 댓글이나 mwlee0860@gmail.com 으로 메일을 보내주시면 감사하겠습니다.&lt;/p&gt;</content><author><name>Sangwoo Lee</name></author><category term="paper-review" /><summary type="html">안녕하세요? 이번에 SlowFast 논문 Review를 하게된 AirLab 이상우입니다.</summary></entry><entry><title type="html">Transfer learning</title><link href="https://blog.airlab.re.kr/2019/11/Transfer-learning" rel="alternate" type="text/html" title="Transfer learning" /><published>2019-11-13T05:00:00+09:00</published><updated>2019-11-13T05:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/Transfer-learning</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/Transfer-learning">&lt;p&gt;Transfer learning 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 노현철 입니다. 
제가 이번에 리뷰할 논문은 &lt;strong&gt;“How transferable are features in deep neuralnetworks?”&lt;/strong&gt; 이른바 &lt;strong&gt;“transfer learning”&lt;/strong&gt; 에 관한 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;transfer learning이란 
적은 dataset에서 학습을 시키면 over-fitting이 일어날 가능성이 많아짐으로 많은 dataset에서 학습을 시킨 일부의 layer들을 가져와서 이 적은 dataset에 적용 시켜 적은 dataset에서도 강인함을 보여줄 수 있는 것이 transfer learning입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/01.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 데이터셋으로 학습을 시키면 초반 레이어에서는 Generality한 파라미터들이 나오고 후반 레이어에서는 Specificity한 파라미터들이 나온다. transfer learning은 초반 레이어에 나오는 Generality한 파라미터를 이용하는 것이다. 그 이유는 각각의 데이터 셋을 학습시켜 얻고자하는  목적이나 원하는 것들이 다르기 때문에 Generality한 파라미터들을 사용하는 것이다. 이 논문에서는 어디까지가 Generality하고 어디서부터 Specificity한지 실험도 하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/02.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 보듯이  Generality 레이어에서는 Gabor filters, color blobs 와 같은 것들이 학습된다.
&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/03.png&quot; alt=&quot;Figure3&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;gabor-filters&quot;&gt;Gabor filters&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/04.png&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;color-blobs&quot;&gt;color blobs&lt;/h5&gt;

&lt;h3 id=&quot;experimental&quot;&gt;Experimental&lt;/h3&gt;

&lt;p&gt;● 1000개의 이미지넷에서 500(A), 500(B) 로 나누어 실험을 한다.&lt;/p&gt;

&lt;p&gt;● A, B 모두 총 8개의 conv레이어를 사용할 것이다.&lt;/p&gt;

&lt;p&gt;● transferred layers는 frozen시키거나 fine-tuned시키는 실험이다. fine-tuned을 사용한 것은 (+)가 쓰여져 있는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/05.png&quot; alt=&quot;Figure5&quot; /&gt;
그림에서 보듯이 baseA 와 baseB는 transfer없이 학습을 시킨 것이고 BnB, AnB는 각각 B에서 학습시킨 파라미터를 B에 적용, A에서 학습시킨 파라미터를 B에 적용한다는 의미이다. (+)가 붙은 경우들은 fine-tuned을 시킨경우들이다. (기본적으로 transferred layers들은 frozen 시킴) 
transfer시키는 레이어의 범위는 1번째 레이어 에서부터 마지막 8번째 레이어 까지 각각 실험하여서 어디까지 General 한지보고, 또한 fine-tuning 하였을 때 성능의 변화가 있는지 보는 실험이다.&lt;/p&gt;
&lt;h4 id=&quot;results&quot;&gt;results&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-13-Transfer-learning/06.png&quot; alt=&quot;Figure6&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;bnb&quot;&gt;BnB&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;(n=1, 2) base B와 동일&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;(n=3,4,5) 성능저하
    &lt;ul&gt;
      &lt;li&gt;레이어간 co-adapted(동화기능)이 있어 상위 레이어만 이 기능을 배울 수 없다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(n=6,7) 성능 다소상향
    &lt;ul&gt;
      &lt;li&gt;학습의 필요성이 점차 줄어듦.&lt;/li&gt;
      &lt;li&gt;(6,7or7,8)사이 features가 co-adapted이 덜하다.&lt;/li&gt;
      &lt;li&gt;중간보다 하단,상단이 optimization 하기 좋다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;bnb-1&quot;&gt;BnB+&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;전체적으로 base B와 비슷
    &lt;ul&gt;
      &lt;li&gt;co-adapted(동화기능)의 성능저하를 방지시켜줌.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;성능향상은 없었음.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;anb&quot;&gt;AnB&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;일부 layer는 Transfer Learning 하는것이 좋다.&lt;/li&gt;
  &lt;li&gt;(n=1,2) layer는 general 하다.&lt;/li&gt;
  &lt;li&gt;(n=3) 약간감소, (n=4,5,6,7) 성능대폭하락
    &lt;ul&gt;
      &lt;li&gt;이것으로 인해 두 가지 감소 이유를 알 수 있음.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;1) (n=3,4,5) co-adaptation으로 인한 감소&lt;/p&gt;

    &lt;p&gt;2) (n=6,7) generality 보단 specificity 하기 때문&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;anb-1&quot;&gt;AnB+&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Fine-tuning은 좋은 성능을 보여줌.
    &lt;ul&gt;
      &lt;li&gt;Transfer Learning의 목적과 반대로 dataset이 많은 경우에도 사용하면 성능을 향상 시킬 수 있다!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(n=1~7) 성능을 유지, 성능 향상
    &lt;ul&gt;
      &lt;li&gt;이 효과는 첫번째 네트워크(A)의 양에 크게 의존하지 않음.&lt;/li&gt;
      &lt;li&gt;이 효과는 너무 많은 retraining을 한다는 것은 놀라운 일.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;co-adapted 때문에 중간layers에서 분할하는 것은 어렵다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;상위layers의 specialization로 인해 Transfer Learning의 부정적인 영향을 끼친다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;random weights보단 Transfer Learning이 좋다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;new task에 Transfer Learning, fine-tuning을 더하면 성능을 향상 시키는데 유용할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;랩실에 들어오고 첫 세미나 발표를 한 논문이고, 또한 끝까지 읽어본 몇 안되는 논문이라 더욱 뜻깊고 기억에 남는 논문이였습니다.
지금은 많이 부족하지만 세미나와 논문리뷰를 하면서 실력을 한층 한층 쌓아 나아가겠습니다. 감사합니다!!!!!&lt;/p&gt;</content><author><name>Hyeoncheol Noh</name></author><category term="paper-review" /><summary type="html">Transfer learning 리뷰</summary></entry><entry><title type="html">Non-local Neural Networks</title><link href="https://blog.airlab.re.kr/2019/11/nonlocal" rel="alternate" type="text/html" title="Non-local Neural Networks" /><published>2019-11-03T11:00:00+09:00</published><updated>2019-11-03T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/nonlocal</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/nonlocal">&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!
오늘 소개할 논문은 Non-local Neural Networks (&lt;a href=&quot;https://arxiv.org/abs/1711.07971&quot;&gt;arXiv:1711.07971&lt;/a&gt;)이며, CVPR2018에서 소개된 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;기존의 Convolution Neural Networks들은 3x3, 5x5과 같은 작은 Filter를 거치며 국소적인 피쳐의 특성을 추출해 낼 수 있기 때문에, Pooling과 함께 사용하며 많은 SOTA Architecture들을 만들어냈습니다.&lt;/p&gt;

&lt;p&gt;하지만, 이러한 CNN 구조는 피쳐의 특수한 특성을 추출 하는 것에 대하여는 훌륭한 퍼포먼스를 보이지만, 영상의 전체의 Context를 보는 것에는 한계를 보이고 있습니다. 이번에 소개시켜드리는 Non-local Neural Networks는 기존의 Non-local Mean Filter에서 착안을 하여 네트워크가 입력의 전체적인 Context역시 학습할 수 있도록 도와줍니다.&lt;/p&gt;

&lt;h3 id=&quot;non-local-mean-filter&quot;&gt;Non-local Mean Filter&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/non-local-filter.png&quot; width=&quot;300px&quot; alt=&quot;figure1&quot; /&gt;
Non-local Mean Filter는 기존의 영상처리에서 사용되던 denoising 알고리즘으로, 위 사진과 같이 특정 구역에 대하여, 모든 영역에 대하여 유사한 픽셀들을 찾아 평균을 처리하는 방식으로 denoising합니다.&lt;/p&gt;

&lt;h3 id=&quot;non-local-networks&quot;&gt;Non-local Networks&lt;/h3&gt;
&lt;p&gt;Non-local Networks는 Non-local Block으로 모듈화 되어 CNN 구조 사이에 삽입되어 사용될 수 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/non-local.png&quot; width=&quot;60%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 다이어그램이 Non-local Block의 다이어그램입니다. 입력된 Input에 대하여 1x1 Conv를 통하여 theta, phi, g를 만들어 낸 뒤 phi의 aixs를 교차해주어 theta와 연산 후 Softmax를 거쳐 모든 맵에 대한 Score Map를 만들어줍니다. 이 맵은 어텐션맵과 유사한 느낌으로 이해가 됩니다. 이 맵과 g를 연산하여 입력에 대하여 어텐션이 적용된 맵을 만들어준뒤 1x1 Conv를 거쳐 기존의 입력과 더하여 출력을하는 구조입니다. 따라서 Non-local 블록은 local한 정보가 아닌 Non-local한 영역까지 학습이 가능한 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;application&quot;&gt;Application&lt;/h3&gt;
&lt;p&gt;Non-local은 General하게 여러 도메인에서 사용될 수 있습니다. 유명한 네트워크들은 Action Recognition에서 I3D + ResNext101 구조에 Non-local Block를 붙인 구조와 Segmentation에 Non-local Block를 붙인 구조들이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I3D + Non-local&lt;/strong&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/nonlocal-i3d.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과를 보시면, Optical Flow를 fusion한 다른 Architecture보다 NL을 붙이고 RGB영상만 사용한 구조가 더 높은 성능을 보이고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CCNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/ccnet.png&quot; width=&quot;50%&quot; alt=&quot;figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Segmentation Task에서도 NL은 좋은 성능을 보입니다. CCNet은 이번 ICCV2019에 소개된 네트워크인데, NL이 연산량이 많은 이슈를 Criss Cross 방식을 통해 연산량은 줄이면서 NL의 효과를 그대로 가져가는 구조를 소개하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/CCNet-result.png&quot; width=&quot;60%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과를 보면 NL 혹은 RCCA(Recurrent Criss-Cross Attention)를 붙인 네트워크가 CityScapes와 ADE20K 에서 SOTA를 달성한 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;CCNet은 다음번에 자세히 리뷰해 보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;simple-implementation-of-nl&quot;&gt;Simple implementation of NL&lt;/h3&gt;
&lt;p&gt;간단한 NL(2D)를 구현해본 코드를 공유합니다.
&lt;a href=&quot;https://github.com/J911/DeepLabV3-NonLocal/blob/master/models/NonLocalBlock.py&quot;&gt;https://github.com/J911/DeepLabV3-NonLocal/blob/master/models/NonLocalBlock.py&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Wang, Xiaolong, et al. “Non-local neural networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.&lt;/li&gt;
  &lt;li&gt;Huang, Zilong, et al. “Ccnet: Criss-cross attention for semantic segmentation.” arXiv preprint arXiv:1811.11721 (2018).&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaemin Lee</name></author><category term="paper-review" /><summary type="html">안녕하세요. AiRLab(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다! 오늘 소개할 논문은 Non-local Neural Networks (arXiv:1711.07971)이며, CVPR2018에서 소개된 논문입니다.</summary></entry><entry><title type="html">Fast-R-CNN</title><link href="https://blog.airlab.re.kr/2019/10/Fast-R-CNN" rel="alternate" type="text/html" title="Fast-R-CNN" /><published>2019-10-04T00:00:00+09:00</published><updated>2019-10-04T00:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/10/Fast%20R-CNN</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/10/Fast-R-CNN">&lt;p&gt;Fast-R-CNN Review&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. 지난번 R-CNN 에 이어서 Fast-R-CNN 을 Review 해보려고 합니다.. ^_^&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Fast-R-CNN&lt;/strong&gt;(&lt;a href=&quot;https://https://arxiv.org/abs/1504.08083&quot;&gt;arXiv:1504.08083&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduciton&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduciton&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;당시 deep ConvNet[14,16]이 발전하면서 image classification과 object detection의 Acc가 상당히 향상 되었다고 합니다. &lt;br /&gt;
당연히 classification보다 object detection이 복잡합니다. 때문에, [9,11,19,25]는 느리고 비효율적인 multi-stage pipline model을 train합니다.&lt;br /&gt;
[14,16]은 Alex_net과 Backpropagation입니다.&lt;br /&gt;
[9,11,19,25]는 R-CNN,SPP,Overfeat,segDeepM입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Multi-stage_pipline&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;
R-CNN에서 3개의 module을 따로 학습 해야하는 것을 생각하면 될 것 같습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;논문에서는&lt;/strong&gt; detection task가 object의 정밀한 localization을 필요로 하기 때문에 &lt;u&gt;두가지 issue&lt;/u&gt;가 생긴다고 합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Issue1]&lt;/strong&gt; : 수많은 객체 위치 후보들을 처리해야한다.&lt;br /&gt;
이는 Speed,accuracy,simplicification을 손상시킵니다.&lt;br /&gt;
&lt;strong&gt;[Issue2]&lt;/strong&gt; : 정확한 localization을 위해 rough_localization을 다듬어야한다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;때문에 논문에서는&lt;/strong&gt; [9,11]Network를 base로 하여 train과정을 단순화 합니다. (Sigle-stage traing algorthm을 제안합니다.) 결과적으로, R-CNN,SPP_net보다 VGGnet을 각각 9배,3배 빠르게 학습합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;R-CNN의 단점&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Multi-stage pipline(train)(Region proposal,SVM,bounding-box)&lt;/li&gt;
  &lt;li&gt;SVM과 bounding-box regressor의 경우(train) 각 image의 proposal의 feature들이 disk에 기록됩니다. (VGG16의 경우 2.5일이 소요됩니다._VOC07)&lt;/li&gt;
  &lt;li&gt;object detection이 느리다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;논문에서는&lt;/strong&gt; Fast-R-CNN을 설명하기전, R-CNN과 SPPnet을 비교합니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPPnet은 sharing compute를 이용하여 R-CNN의 속도를 높이기 위해 제안되었다.
    &lt;ul&gt;
      &lt;li&gt;여기서 말하는 sharing compute는 R-CNN과 달리 SPPnet은 한번의 convnet을 통과한 feature를 이용해 detection을 함으로써, end-to-end 학습이 된다는 이야기를 하는 것 입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결과적으로, test:10~100배, train:3배 빠릅니다. &lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;그러나&lt;/strong&gt;, SPPnet도 주목할만한 &lt;strong&gt;단점&lt;/strong&gt;이 있습니다. (drawback)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPPnet도 R-CNN과 마찬가지로, train은 multi-stage pipline(feature extraction/fine-tune(log_loss)/SVM train, bounding-box regressors 를 가집니다.) 또, feature가 disk에 기록됩니다.&lt;/li&gt;
  &lt;li&gt;SPPnet에서 제안된 algorithm은 convolutional layer를 update할수 없고 convolution layer 전에 SPP를 할 수 없다.(network perpomance를 제한한다.)&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Contributions&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Fast-R-CNN)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;image의 region_proposal을 crop&amp;amp;warp한 R-CNN과 달리 Fast-R-CNN은 입력이미지와 RoI_projection(object proposal)을 입력으로 받는다.&lt;/li&gt;
  &lt;li&gt;network를 통해 convfeature map을 생성하기위해 전체 이미지를 처리한다.&lt;/li&gt;
  &lt;li&gt;각 object proposal에 대해 RoIPooling layer가 고정길이 vector를 추출한다. (각 feature vector는 fc_layer에 연속적으로 전달된다.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Multi-task loss&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure10.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Multi-task loss)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;RoI pooling layer&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;
입력이미지에 한번만 CNN을 적용하고 RoI pooling을 이용하여 object 판별을 위한 feature를 추출하자는 것이 핵심이다.&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RoI로 추출할 feature size = HxW(eg.7x7)&lt;/li&gt;
  &lt;li&gt;Feature map 위에 RoI의 좌표 (r,c,h,w)&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure7.gif&quot; width=&quot;678&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(RoI pooling layer)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Feature map위에서 h/H x w/W 만큼 Grid를 만들어 pooling 하면 결과적으로 원하는 HxW (첫 fc-layer와 호환이 되도록 하는 사이즈)feature size가 됩니다.&lt;br /&gt;
(서로 다른 size의 region proposal이 입력되더라도 같은 size의 stride로 7x7을 만들어준다.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Initializing from pre-trained networks&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 pre-train된 ImageNet networks를 각각 실험합니다. pre-train network가 Fast-R-CNN을 initialize할때 3가지의 transform을 거칩니다.&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;마지막 maxpooling layer를 RoI pooling layer로 대체 한다.&lt;/li&gt;
  &lt;li&gt;network_last fully connected layer와 softmax layer는  two sibling layer로 대체됩니다.&lt;/li&gt;
  &lt;li&gt;network는 two input 을 받도록 합니다. (image list와 해당 image의 RoI 목록)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Mini-batch sampling&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;fine-tuning에서, N=2(image),R=128 로 하여 R/N으로 image당 64RoI를 뽑습니다. CNN연산량을 줄이는 효과가 있습니다.       다음 그림과 같은 방식을 hierarchical sampling이라고 합니다.(계층적 샘플링)
Ground-truth와 IoU &amp;gt; 0.5 (Positive), [0.1,0.5] 일 경우 Negative로 구분합니다. (즉, IoU가 너무 낮은것은 sample로 추가하지 않는다.) 
//train 할때, image horizontally flipped(p=0.5)의 augmentation만 사용하고 다른 augmentation 은 사용하지 않았다고 합니다. //&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure6.png&quot; width=&quot;678&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Mini-batch sampling)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;scale-invariance&quot;&gt;&lt;u&gt;Scale invariance&lt;/u&gt;&lt;br /&gt;&lt;/h4&gt;
&lt;p&gt;논문에서 Scal invariance 를 해결하기 위해 2가지 방법을 사용하였습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[brute force learning]&lt;/strong&gt;&lt;br /&gt;
train/test 에서 미리 정해진 픽셀 크기로 처리된다. 때문에, network는 train_data 로 부터 정해진 scale의 detection을 해야합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[using image pyramids]&lt;/strong&gt;&lt;br /&gt;
image pyramid를 통해 network는 scale invariance를 제공한다. Test 에서는 image pyramid는 각region_proposal을 nomalize하는데 사용됩니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 GPU memory issue로 인하여 소규모 네트워크에 대해서만 multi_scale train을 하였다고 합니다. 즉, multi_scale에 관한 실험은 별로 진행하지 않았다는 것이며 논문의 내용은 scale invariance에 더 초점을 맞추고 있습니다.
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;&lt;u&gt;Conclusion&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;저자가 말한대로 Fast-R-CNN은 R-CNN과 SPPnet과 다르게 깔끔함을 강조하고 있다.
Fast-R-CNN의 특징은 다음과 같다. &lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이미지당 CNN을 한번만 수행한다.&lt;/li&gt;
  &lt;li&gt;RoI pooling layer의 도입.(마지막 max-pooling 대신 사용되었다.)&lt;/li&gt;
  &lt;li&gt;Classification 에서 SVM을 사용하던 것 과 달리 Softmax로 변경되었다. 즉, 하나의 network가 된것이다.&lt;/li&gt;
  &lt;li&gt;Multitask loss 에서 보면 bounding Box regressor 또한 network의 부분이 되었다. &lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;이러한 특징을 바탕으로&lt;/strong&gt; , R-CNN/SPPnet과 달리 mAP가 높고, multi-task loss 를 이용하여 single-satge train이 가능하고, end-to-end학습이 가능하고 feature caching을 하지 않으므로 disk 공간을 필요로 하지 않는다는 장점이 있다.&lt;/p&gt;

&lt;p&gt;Fast-R-CNN의 다음 버전인, Faster-R-CNN은 region proposal 생성 방식의 개선을 주 논점으로 다룹니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Daehan Kim</name></author><category term="paper-review" /><summary type="html">Fast-R-CNN Review</summary></entry><entry><title type="html">R-CNN</title><link href="https://blog.airlab.re.kr/2019/10/R-CNN" rel="alternate" type="text/html" title="R-CNN" /><published>2019-10-01T00:00:00+09:00</published><updated>2019-10-01T00:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/10/R-CNN</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/10/R-CNN">&lt;p&gt;R-CNN Review&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. R-CNN을 시작으로 paper review post 를 작성하려고 합니다. ^_^&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;R-CNN&lt;/strong&gt;(Rich feature hierarchies for accurate object detection and semantic segmentation) (&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot;&gt;arXiv:1311.2524&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduciton&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduciton&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;논문 발표 이전&lt;strong&gt;SIFT&lt;/strong&gt;와 &lt;strong&gt;HOG&lt;/strong&gt; 에 상당한 기반을 두고 visual recognition task를 수행했다고 합니다. 그러나 낮은 perpomance 로인해 R-CNN을 고안 하였습니다. 
여기서 SIFT와 HOG를 간단하게 설명하면, 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;SIFT&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure1.jpg&quot; alt=&quot;figure1&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt; SIFT feature vector는 feature 주변의 영상패치를 4x4 블럭으로 나누고, (1) 각 블럭에 속한 pixel들의 gradient방향과 크기에 대한 histogram을 구하고 bin값으로 연결한 128차원 벡터 입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;크기, 형태, 방향에 강인하면서도 구분력이 뛰어나다.&lt;/li&gt;
  &lt;li&gt;기하학적 정보는 무시하고 feature 단위로 매칭을 수행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;HOG&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure2.jpg&quot; alt=&quot;figure2&quot; /&gt;&lt;br /&gt;
 HOG 는 대상 영역을 일정 셀로 분할한뒤, 각 셀마다 edge pixel들의 방향에 대한 histogram을 구한 뒤 이를 bin값으로 연결한 벡터 이다. (edge의 방향 histogram으로 본다.)&lt;br /&gt;
블록 단위로는 기하학적 정보를 유지하고, 그 내부는 histogram을 사용하여 local변화에 어느정도 강인하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;template matching과 histogram maching에 중간 단계의 매칭방법이다.&lt;/li&gt;
  &lt;li&gt;기하학적 정보와 local 한 정보를 모두(일정 양) 가진다.&lt;/li&gt;
  &lt;li&gt;figure2를 보면 알겠지만 edge의 방향정보를 사용하기 때문에 특이한(독특한) edge를 갖는 object를 식별하는데 적합한 영상 feature이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;SIFT/HOG&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SIFT&lt;/th&gt;
      &lt;th&gt;HOG&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;이용하는 정보&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;local information&lt;/td&gt;
      &lt;td&gt;edge information&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;형태변화가 심한 경우 detection&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;esay&lt;/td&gt;
      &lt;td&gt;difficult&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;example&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;액자속 그림(패턴)&lt;/td&gt;
      &lt;td&gt;자동차, 배드민턴 라켓&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;**&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;복잡한 image&lt;/td&gt;
      &lt;td&gt;단순하고 독특한 image&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;이 논문에서는 &lt;strong&gt;HOG 기반의 시스템 보다 CNN을 사용하여 높은 performance를 보여주기 위해 2가지 문제&lt;/strong&gt;에 초점을 맞춥니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;u&gt;[One]&lt;/u&gt; sliding-window 의 문제점을 파악하고 region proposal Algorithm을 사용했다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure4.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(sliding-window method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure5.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(region-proposal method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sliding window는 탐색해야하는 영역의 수가 이미지 전체이기 때문에 비효율적이고 입력 영상에 ‘물체가 있을거 같은’ 영역을 빠른 속도로 찾아내는 region proposal algorithm을 사용 하였다.&lt;br /&gt;
&lt;strong&gt;결과적으로 search space가 훨씬 줄어들기 때문에 빠르고 효율적 이다.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;[Two]&lt;/u&gt; large CNN을 training하기 위한 충분한 dataset이 없었고, ILSVRC DATA로 pre-training 된 model을 가져와서 fine-tune 하여 PASCAL DATA에 적용하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;object-detection-with-r-cnn&quot;&gt;&lt;strong&gt;&lt;u&gt;object detection with R-CNN&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;                   R-CNN은 3가지의 module 로 구성되어 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure3.png&quot; alt=&quot;figure3&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;region proposal을 얻어내는 module&lt;/li&gt;
  &lt;li&gt;CNN에 넣어 feture map을 얻어내는 module&lt;/li&gt;
  &lt;li&gt;CNN을 통해 추출된 feture map을 이용하여 Linear SVM을 사용하여 분류하는 module&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이때, CNN model은 Alexnet을 거의 그대로 가져와 사용하였다.&lt;br /&gt;
당연히 CNN은 고정 크기의 input을 입력으로 받기 때문에. region proposal을 CNN에 넣기 위해 crop &amp;amp; warp 를 한다.
논문에 나와있지만, 읽다보면 왜 R-CNN은 Classifier로 Softmax를 쓰지 않고 SVM을 사용했는지에 대한 의문이 들 수 있다. 근데 이는 수식적에 근간을 두고 사용한 것이 아닌 실험적인 결과로 Softmax를 사용했을때 mAP가 낮아졌기 때문에 SVM을 사용한 것 이다.(54.2% -&amp;gt; 50.9%)&lt;/p&gt;

&lt;p&gt;SVM을 간단하게 설명하면 CNN으로 부터 추출된 featuer vector들을 class별로 점수를 주고 object인지 아닌지 object라면 어떤 object 인지 판별하는 classifier이다.&lt;/p&gt;

&lt;p&gt;figure를 보면 Bbox reg 라는 부분이 있는데 bounding Box Regression인데 이는 필수로 사용해야 하는 부분은 아니라고 한다. 그러나 사용한 mAP가 더 좋기 때문에 사용하는 것을 권장한다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure6.png&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Bounding-box method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
ground-truth(G) 와 Predicted-Box(P) 가 있을때, P를 G로 mapping 해주는 것이다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure7.png&quot; width=&quot;600&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Result)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
결과적으로, 다음과 같은 output을 얻을 수 있다.&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;논문을-읽으면서-여러자료들을-참고하여-r-cnn의-단점을-이해해-보았다&quot;&gt;&lt;u&gt;논문을 읽으면서 여러자료들을 참고하여 R-CNN의 단점을 이해해 보았다.&lt;/u&gt;&lt;/h4&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure8.png&quot; width=&quot;900&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Table)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
위와 같이 VOC2010 test에서 좋은 성능을 보였지만 단점은 존재한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;region proposal된 약 2K의 image를 모두 crop하고 warp하는 과정에서 cpu를 통해 이뤄지기 때문에, 매우 비효율 적이라는 점을 이해하였다.&lt;br /&gt;
&lt;strong&gt;직접 경험해본 일화&lt;/strong&gt;중 하나는, 1980x1080 를 224x224 image preprocessing 하는 과정이 오래걸려서 GPU가 놀고 있는 시간이 많았던 적이 있다. (224x224 image를 따로 저장하여 문제 해결.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3가지의 module을 따로 학습해야하기 때문에 비효율적이다.
&lt;strong&gt;때문에 Fast_R-CNN에서 Multi-task loss&lt;/strong&gt;를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;end-to-end 학습이 불가능 하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Daehan Kim</name></author><category term="paper-review" /><summary type="html">R-CNN Review</summary></entry><entry><title type="html">Mean teachers are better role models</title><link href="https://blog.airlab.re.kr/2019/09/mean-teacher" rel="alternate" type="text/html" title="Mean teachers are better role models" /><published>2019-09-19T11:00:00+09:00</published><updated>2019-09-19T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/09/mean-teacher</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/09/mean-teacher">&lt;p&gt;Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!&lt;/p&gt;

&lt;p&gt;오늘 소개할 논문은 Mean teachers are better role models (&lt;a href=&quot;https://arxiv.org/abs/1703.01780&quot;&gt;arXiv:1703.01780&lt;/a&gt;)이며, NIPS 2017에서 소개된 논문입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Semi-Supervised Leaning에 관련된 논문이고, Π Model과 Temporal ensemble 이후에 소개되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure1.png&quot; width=&quot;80%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;π-model&quot;&gt;Π Model&lt;/h4&gt;
&lt;p&gt;Π Model은 위 그림의 위 쪽과 같은 구조와 같이, 라벨이 주어진 경우 Closs Entropy를 사용하여 학습을 하고, 라벨이 주어지지 않은 경우는 augmented input의 결과와 이전과 다른 dropout 과 augmention을 사용한 결과의 squared difference를 이용해 학습을 합니다.&lt;/p&gt;

&lt;h4 id=&quot;temporal-ensemble&quot;&gt;Temporal ensemble&lt;/h4&gt;
&lt;p&gt;Temporal ensemble과 Π Model의 차이점은 라벨이 주어지지 않았을 때 Temporal ensemble은 이전의 결과에 대한 값들을 앙상블한 결과를 가지고 학습을 하는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;mean-teacher&quot;&gt;Mean Teacher&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/cover.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;
위에서 소개한 두개의 모델은 모델을 공유하여 학습이 되지만, Mean Teacher의 경우 Student와 Teacher 모델로 나뉘어 학습이 진행됩니다. Teacher 모델은 Student 모델의 Weight를 공유하고, 대신에 Student모델의 아래의 방식과 같이 EMS Weight를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure2.png&quot; width=&quot;30%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 후 라벨이 주어지지 않은 경우 기존 방식과 유사하게 Teacher 모델과 Student 모델의 consistency cost (J)를 계산하여 학습이 진행되게 됩니다.
&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure3.png&quot; width=&quot;40%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 방법은 Temporal ensemble에 비하여 2가지 이점을 가지는데, 첫 번째는 Student와 Teacher 사이의 빠른 Feedback이 가능하여 더 높은 ACC를 가지게 되는 것이고, 두 번째는 large scale datasets을 online으로 학습할 수 있는 것 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/table1.png&quot; width=&quot;80%&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과와 같이 SVHM과 CIFAR10 에서의 Mean Teacher를 사용한 네트워크가 Π Model과 Temporal ensemble보다 더 높은 퍼포먼스를 보임을 확인할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tarvainen, Antti, and Harri Valpola. “Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.” Advances in neural information processing systems. 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rasmus, Antti, et al. “Semi-supervised learning with ladder networks.” Advances in neural information processing systems. 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Laine, Samuli, and Timo Aila. “Temporal ensembling for semi-supervised learning.” arXiv preprint arXiv:1610.02242 (2016).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaemin Lee</name></author><category term="paper-review" /><summary type="html">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</summary></entry><entry><title type="html">R(2+1)D</title><link href="https://blog.airlab.re.kr/2019/09/R(2+1)D" rel="alternate" type="text/html" title="R(2+1)D" /><published>2019-09-18T19:00:00+09:00</published><updated>2019-09-18T19:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/09/R(2+1)D</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/09/R(2+1)D">&lt;h3 id=&quot;a-closer-look-at-spatiotemporal-convolutions-for-action-recognition-리뷰&quot;&gt;A Closer Look at Spatiotemporal Convolutions for Action Recognition 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“A Closer Look at Spatiotemporal Convolutions for Action Recognition”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Facebook에서 “Du Tran”씨가 2018년도에 cvpr에서 발표한 논문입니다. 이 논문은 3D conv를 공간정보와 시간정보로 나눠 conv하는 구조를 처음 제안하였고, 다양한 spatiotemporal conv 방법들을 비교합니다. 또한 Sports-1m, kenetics, ucf101, hmdb51에서 state-of-the-art를 달성합니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;AlexNet이 처음공개된 이후로 이미지 인식 분야는 multi-scale convolutions, residual learning, dense connections 과 같은 많은 진보가 있었습니다. 반면에 비디오 분야에서는 “새롭다” 하는 큰 발전이 없습니다. 현재의 state-of-the-art인 I3D또한 bset hand-crafted 방법인 iDT랑 비교하였을때 그렇게 놀랄만한 진보는 아닙니다.(저의 의견이 아닙니다. 논문저자의 의견입니다 ㅎㅎ.) 또한 image-based 2D CNN의 Sports-1M에서의 성능은 state-of-the-art랑 비슷합니다.(저자는 3D conv의 진보는 없고, image-based 방법이랑 별 차이가 없다고 강조하는것 같습니다.) 이러한 결과에 기초하여, 시퀀스의 정적 프레임에 이미 포함 된 강력한 행동 클래스 정보 때문에, 시간적 추론이 정확한 행동 인식에 필수적이지 않다고 생각 할 수도 있습니다. 논문저자는 시간적 정보가 행동인식에 필수적이지 않다는 생각을 제거하기 위하여, 다양한 실험을 진행합니다. 논문저자들의 실험에 의하면, 3D ResNets이 Sports-1M 및 Kinetics과 같은 action recognition benchmarks에서 훈련되고 평가 될 때 동일한 깊이에 대해 2D ResNets보다 월등히 뛰어남을 보여줍니다. 이러한 실험결과에 기초하여, 논문 저자들은 2D cnn과 3d cnn 사이의 R(2+1)D를 제안합니다. (2+1)D의 의미는 3D conv를 두개의 개별적인 2D 공간 conv와 1D 시간 conv로 분해하는것 입니다. 첫번째 장점은 파라메터가 증가하지 않고, relu를 두번 사용할 수 있기때문에 비선형성을 증가시킬수 있습니다. 두번째 장점은 optimization하는 것을 도와주는 것입니다. 아래의 그림은 똑같은 구조여도 R(2+1)D가 더 최적화가 잘된다는 그래프 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;convolutional-residual-blocks-for-video&quot;&gt;Convolutional residual blocks for video&lt;/h2&gt;

&lt;p&gt;이 논문에서 사용하는 residual blocks은 전부 “vanilla” residual blocks을 사용합니다. 아래의 표현에서 3 x L x H x W,라는 표현들이 나오는데, 3은 rgb 3채널, L을 프레임 길이, H는 높이, W 넓이 입니다.&lt;/p&gt;

&lt;h4 id=&quot;r2d-2d-convolutions-over-the-entire-clip&quot;&gt;R2D: 2D convolutions over the entire clip&lt;/h4&gt;

&lt;p&gt;R2D는 2D CNN방법이고, 3L x H x W 입니다. 즉 프레임의 길이를 하나의 차원으로 보는것이 아니라, 채널로 보는것 입니다. 1프레임당 3채널씩 할당되는것 입니다.&lt;/p&gt;

&lt;h4 id=&quot;f-r2d-2d-convolutions-over-frames&quot;&gt;f-R2D: 2D convolutions over frames&lt;/h4&gt;

&lt;p&gt;f-R2D는 2D CNN방법이고, 프레임을 채널로 쌓아서 사용하는 방법이 아니라, 전체의 프레임을 개별적으로 2D CNN 하는 방법 입니다.&lt;/p&gt;

&lt;h4 id=&quot;r3d-3d-convolutions&quot;&gt;R3D: 3D convolutions&lt;/h4&gt;

&lt;p&gt;R3D는 가장 일반적은 3D conv 방법이고, 3 x L x H X W 입니다. 프레임을 시간축을 하나 추가하여 stack하고 3D conv를 하는것 입니다.&lt;/p&gt;

&lt;h4 id=&quot;mcxand-rmcx-mixed-3d-2d-convolutions&quot;&gt;MCxand rMCx: mixed 3D-2D convolutions&lt;/h4&gt;

&lt;p&gt;한 가지 가설은 모션 모델링(즉, 3D convolution)이 초기 계층에서 특히 유용할 수 있는 반면, 후반 계층에서는 필수적이지 않는다는 가설 입니다. 학습초기에는 3D ResNets (R3D)를 초기에 사용하고, 후반에는 2D conv를 사용합니다. 또 그 반대는 학습초기에는 2D conv를 사용하다가 후반에는 3D conv 사용하는 방법입니다.&lt;/p&gt;

&lt;h4 id=&quot;r21d-21d-convolutions&quot;&gt;R(2+1)D: (2+1)D convolutions&lt;/h4&gt;

&lt;p&gt;또 이 방법은 논문저자가 주장하는 방법인데, full 3D conv를 2D의 공간 conv와 1D 시간 conv로 분해합니다. 논문 저자는 3D convolutional filters(N×t×d×d)를  N x 1 x d x d 와 M x t x 1 x 1로 분해 합니다. 그렇게하면 기존의 3D conv와 동일하면서, 비선형성을 증가시키고 최적화에 도움을 줍니다. 아래의 그림은 앞에서 언급한 다양한 구조의 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;이 논문의 저자는 Resnet을 사용하기 위하여, 오버피팅이 나타나지 않기 위하여 충분히 큰 데이터셋인 Sport 1M, Kinetics을 사용합니다. 또한 다른 작은 데이터셋에서 잘 작동하나 궁금하기 때문에, UCF101,HMDB51에 transfer learning을 적용합니다.&lt;/p&gt;

&lt;h4 id=&quot;experimental-setup&quot;&gt;Experimental setup&lt;/h4&gt;

&lt;p&gt;모든 프레임의 사이즈는 112 x 112로 사용하였고,자세한 구조는 아래의 그림을 봐주시면 됩니다. 다양한 디테일이 논문에 언급되는데 더 자세하게 알고 싶으시면 논문을 읽어보시는걸 추천드립니다.&lt;/p&gt;

&lt;h4 id=&quot;comparison-of-spatiotemporal-convolutions&quot;&gt;Comparison of spatiotemporal convolutions&lt;/h4&gt;

&lt;p&gt;R2D부터 R(2+1)D까지 모든 net을 clip단위와 video단위로 평가합니다. 아래의 첨부한 그림을 보면, 이 논문저자가 주장한 R(2+1)D 방법은 R3D방법과 파라메터 차이는 나지 않지만 정확도는 모든 부분에서 가장 높은걸 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img3.png&quot; alt=&quot;Figure3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;action-recognition-with-a-34-layer-r21d-net&quot;&gt;Action recognition with a 34-layer R(2+1)D net&lt;/h4&gt;

&lt;p&gt;이 부분에서는 다양한 데이터셋과 기존의 방법들에서 실험한 결과들을 정리합니다. 아래의 표를 확인해 보시면 i3d-two-stream방법을 제외하고는 모두 1등을 달성하였습니다. i3d-two-stream방법에 비하여 정확도가 낮은 이유는 옵티컬 플로우를 뽑는 방법이 i3d는 tvl1 방법이지만, 이논문의 저자는 Farneback’s 방법을 사용하기 때문이라고 논문 저자는 추측하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img4.png&quot; alt=&quot;Figure4&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img5.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문을 읽으면서 I3D의 논문과 매우 비슷하다고 생각했습니다. 하지만 이 논문은 hand-craft논문들도 잘 정리해 줘서 좋았고,Farneback 방법과 tvl1방법의 차이라던지, 다양한 기술들에 대하여 잘 기술한것 같습니다. 하지만 제가 개인적으로 생각할때 이 논문 또한 뭐 대단한 하게 새로운 아이디어라고 생각하지 않습니다.&lt;/p&gt;</content><author><name>Minseok Seo</name></author><category term="paper-review" /><summary type="html">A Closer Look at Spatiotemporal Convolutions for Action Recognition 리뷰</summary></entry><entry><title type="html">2019년 2학기 정기 상담 안내</title><link href="https://blog.airlab.re.kr/2019/09/190903" rel="alternate" type="text/html" title="2019년 2학기 정기 상담 안내" /><published>2019-09-03T19:00:00+09:00</published><updated>2019-09-03T19:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/09/190903</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/09/190903">&lt;p&gt;한밭대학교 정기상담 안내&lt;/p&gt;

&lt;p&gt;최동걸 교수님 상담학생은 아래 한글파일을 참조하여 상담하시기 바랍니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/post-files/2019-09-03-notice/19년2학기1차상담_학번_이름.hwp&quot; target=&quot;_blank&quot;&gt;19년2학기1차상담_학번_이름.hwp&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;상담을 원하는 학생은 &lt;strong&gt;이번주 금요일 (9월 6일)까지&lt;/strong&gt; 교수님 방문 앞 종이에 상담 요청 시간을 적을 것.&lt;/li&gt;
  &lt;li&gt;상담 받기 하루 전 위 파일을 내려받아 빨간색으로 표시되어 있는 부분을 본인에 맞게 작성하고 &lt;strong&gt;파일 제목을 19년2학기1차상담_학번_이름.hwp로 작성하여 dgchoi@hanbat.ac.kr로 보낼 것&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;상담 내용은 공백 제외 &lt;strong&gt;최소 500자 이상&lt;/strong&gt; 작성할 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;대면 상담을 추천하지만 상담일정상 바쁜 학생은 서면상담으로 대체 가능&lt;/li&gt;
&lt;/ol&gt;</content><author><name>AiRLab.</name></author><category term="notice" /><summary type="html">한밭대학교 정기상담 안내</summary></entry><entry><title type="html">Unified Perceptual Parsing for Scene Understanding</title><link href="https://blog.airlab.re.kr/2019/08/upernet" rel="alternate" type="text/html" title="Unified Perceptual Parsing for Scene Understanding" /><published>2019-08-21T19:03:00+09:00</published><updated>2019-08-21T19:03:00+09:00</updated><id>https://blog.airlab.re.kr/2019/08/upernet</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/08/upernet">&lt;h1 id=&quot;unified-perceptual-parsing-for-scene-understanding&quot;&gt;Unified Perceptual Parsing for Scene Understanding&lt;/h1&gt;
&lt;p&gt;Understanding&lt;/p&gt;

&lt;p&gt;안녕하세요! &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이소열입니다!&lt;/p&gt;

&lt;p&gt;이번에 소개할 논문은 &lt;strong&gt;Unified Perceptual Parsing for Scene Understanding&lt;/strong&gt;입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;이 논문의 실제 구현은 https://github.com/CSAILVision/unifiedparsing 에 있습니다!&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;인간의 눈은 한번 장면을 본 것 뿐으로, 매우 다양한 정보를 추출해낼 수 있습니다. 어떤 장면을 보고있는것인지, 이 물체가 무엇인지, 이 물체가 무엇으로 이루어져 있는지(part), 어떤 재질, 재료로 이루어져 있는지 등 다양한 정보가 담겨있습니다. 
딥러닝과 이미지처리의 발전으로 인간 수준의 recognition이 가능하게 되었지만, 다양한 visual recognition task들은 독립적으로 진행되었습니다. 독립적으로 진행됨에 따라, 이 다양한 작업들을 동시에 진행/학습할 수 있을까? 라는 의문이 생기게 되었고, 이는 논문의 motive가 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-upernet/01.png&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; UperNet의 다양한 task &amp;gt;&lt;/p&gt;

&lt;p&gt;UperNet에는 여러가지 challenge가 있습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;어떠한 데이터셋에도, 모든 task(level)의 annotation이 있는 데이터셋이 없다. 예를들어, ADE20K는 scene parsing, DTD에는 texture recognition 등 데이터셋에 하나의 task의 annotation만 존재한다.&lt;/li&gt;
  &lt;li&gt;여러 데이터셋에서 나온 annotation들은 heterogeneous이다(서로 종류가 다름). 예를들어, ADE20k는 pixel-wise annotation들을 가지고 있는 반면, DTD는 image-level annotation들을 가지고 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이러한 challenges를 다루기 위해, 이 논문에서는 다른 데이터셋을 다루고, 여러 level의 task를 학습하는 새로운 framework를 제안합니다.&lt;/p&gt;

&lt;p&gt;이 framework는 단일 네트워크에서 feature의 계층적 특성을 이용합니다. 이게 어떤 장면인가를 판별할 때는, high-level feature를 사용하고, object segmentation, part segmentation등은 모든 level의 feature를 사용합니다.&lt;/p&gt;

&lt;p&gt;이 논문에서 주장하는 contributions은 다음과 같습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;한번에 multiple visual concepts를 분석하는 framework 제안&lt;/li&gt;
  &lt;li&gt;여러 종류의 dataset에서 학습되는 계층적 구조의 Network 제안&lt;/li&gt;
  &lt;li&gt;이 모델이 하나의 이미지에서 다양한 정보를 찾아낼 수 있음을 발견&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;2-defining-unified-perceptual-parsing&quot;&gt;2. Defining Unified Perceptual Parsing&lt;/h2&gt;

&lt;h3 id=&quot;21-datasets&quot;&gt;2.1 Datasets&lt;/h3&gt;

&lt;p&gt;다양하고 넓은 visual concept의 segmentation을 수행하기 위해, 이 논문에서는 &lt;strong&gt;Broadly and Densely Labeled Dataset (Broden)&lt;/strong&gt; 을 사용합니다. Broden dataset이라고 표현이 계속되는데, 여러가지 데이터셋을 합치고, 이 네트워크에서 학습을 할 수 있게 간단한 조정을 한 데이터셋 입니다. 이 논문에서 Broden dataset을 위해 ADE20K, Pascal-context, Pascal-Part, OpenSurfaces, DTD데이터셋을 합쳤습니다. 결과적으로, Broden dataset에는 scenes, objects, objects parts, material and textures의 정보가 담겨있습니다.&lt;/p&gt;

&lt;p&gt;하지만, 이 데이터셋에는 다른 클래스의 sample들로부터 unbalance함을 발견했습니다. 따라서, Broden dataset을 구성할 때, 다음과 같은 정책들을 기본으로 하였습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;다른 데이터셋에서부터 다온 서로다른 비슷한 concepts들을 합침. 예를 들어 ADE20K, Pascal-Context 및 Pascal-Part의 object 및 part 주석이 병합 및 통합됨.&lt;/li&gt;
  &lt;li&gt;최소 50개 이상의 데이터가 있는 class만 포함시키며, 최소 20개 이상의 part image만 포함시킴.&lt;/li&gt;
  &lt;li&gt;OpenSurface 데이터셋에서 under-sample된 label을 수동으로 합침. 예를 들어, stone과 concrete는 stone으로 합치고, 투명 plastic과 불투명 plastic은 plastic으로 합침. 또한, 50게 이하의 이미지를 갖는 label도 제거함.&lt;/li&gt;
  &lt;li&gt;400 scene label이 넘는 AKE20K를 365개의 label을 가진 Places 데이터셋에 맞춤.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-upernet/02.png&quot; alt=&quot;table1&quot; /&gt;
&amp;lt; Broden 데이터셋의 구성 &amp;gt;&lt;/p&gt;

&lt;p&gt;위의 표가 Broden dataset의 구성입니다. 여러가지 task를 위한 데이터셋을 사용하였고, 또한 성능 측정 기준을 task마다 달리 하였으며, class의 수를 맞춰주었습니다.&lt;/p&gt;

&lt;h3 id=&quot;22-metrics&quot;&gt;2.2 Metrics&lt;/h3&gt;

&lt;p&gt;모델의 성능을 측정하기 위해, 각 데이터셋의 annotations마다 다른 metrics을 사용하였습니다. semantic segmentation을 측정하기 위해 Pixel Accuracyt(P.A.)와 mean IoU(mIoU)를 사용하였습니다. image에 unlabeld area가 존재하는데, 그 부분은 계산을 할 때 제외하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;3-designing-networks-for-unified-perceptual-parsing&quot;&gt;3. Designing Networks for Unified Perceptual Parsing&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-upernet/03.png&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; UperNet의 구조 &amp;gt;&lt;/p&gt;

&lt;p&gt;UperNet은 위와 같은 구조로 구성되어있습니다. 기본적으로 Feature Pyramid Network(FPN)의 형태로 구성되어있습니다. FPN이란, 피라미드 구조로, 다중 레벨 feature를 추출합니다. 또한, 피라미드의 head부분에 PPM Head를 덧붙였습니다. feature map의 encoder부분은 ResNet으로 구성되어있습니다.&lt;/p&gt;

&lt;p&gt;각 stage를 C2, C3, C4, C5라 하고, FPN에서의 feature map output을 P2, P3, P4, P5라고 해봅시다. scene의 정보를 분석하는 작업에서는 high level feature만을 사용합니다. 장면을 분석할 때는, 전체 이미지를 보고 어떤것인지 판단해야 하기 때문입니다. object의 정보를 판단할 때는, 모든 level의 feature를 사용합니다. part segmentation은 하나의 object 내에서 이루어지므로, object와 같은 Fused Feature Map을 사용합니다. marerial에 대한 정보를 판단할 때는, low level feature만을 사용합니다. 마지막으로, texture를 판단할 때는, low level feature만을 사용합니다. 하나의 object 내에서도 여러개의 texture정보가 담겨있을 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;또한, texture를 학습할 때는, 전체 네트워크를 학습시킨 이후 texture만을 따로 조금 더(논문에서는 few epoch라고 표현) 학습시킵니다. 그리고 texture는 encoder에 아무런 영향(grad에 관한)을 주지 않습니다.&lt;/p&gt;

&lt;h3 id=&quot;31-implementation-details&quot;&gt;3.1 Implementation details&lt;/h3&gt;

&lt;p&gt;우선, 모든 classifier 앞에 convolutional head가 존재합니다. decoder부분에서, 모든 layer의 크기를 맞춰주기 위해, bilinear interpolation방법으로 P2의 scale로 맞춰주고 concat을 진행합니다. 모든 나머지 non-classifier인 convolutional layer(FPN을 포함)에서는 512channel의 output을 가지며 이후 batch normalization을 진행합니다. 그 이후, ReLU를 적용합니다. learning rate는 “poly”방식을 따라 변화시킵니다. weight decay = 0.0001, momentum = 0.9를 적용합니다. 공정한 비교를 위해, size는 450으로 resize합니다. backbone network는 ImageNet에서 pretrain된 모델을 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;32-design-discussion&quot;&gt;3.2 Design discussion&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-upernet/04.png&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; 다른 방식과의 비교 &amp;gt;&lt;/p&gt;

&lt;p&gt;사실, 이 논문에서 주장하고있는 성능은 PSPNet보다 조금 낮습니다. 하지만, 큰 차이가 없고 계산 시간이 2배정도 빨라서 더 좋은 성능이다라는것을 주장하는 것 같습니다.&lt;/p&gt;

&lt;p&gt;결과적으로, 실제 구현을 통해 pyramid 구조를 사용하여 high/low level의 feature를 모두 잘 찾아내며, decoder에서는 bilinear interporation을 사용했기 때문에, 비교적 적은 연산량을 볼 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;4-main-result&quot;&gt;4. Main result&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-upernet/05.png&quot; alt=&quot;table3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; 여러 task에 대한 result &amp;gt;&lt;/p&gt;

&lt;p&gt;표를 보시면, 각 task에 맞는 결과들을 볼 수 있습니다. O는 object, P는 part, S는 Scene, M은 Material, T는 Texture에 대한 task를 나타내고 있습니다. 표의 결과를 보았을 때, 하나의 task만을 하는것이 가장 높은 효율이 나왔고, 여러 task를 동시에 진행했을 때 성능이 높아지는것은 확인할 수 없었습니다. 결과적으로, 이 네트워크는 여러가지 작업을 한번에 수행할 수 있다 정도로 볼 수 있을 것 같습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 소개하는 main result를 다음과 같이 요약해보았습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;고화질의 input image에서 더욱 성능이 향상되었다.&lt;/li&gt;
  &lt;li&gt;PPM을 head에 추가함으로 더욱 성능을 높였다.&lt;/li&gt;
  &lt;li&gt;deconvolution대신 interpolation을 사용했으며, 1x1 convolution을 사용해 연산량을 줄였다.&lt;/li&gt;
  &lt;li&gt;서로다른 annotations에서 Multi-task learning을 진행하였다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;번외&quot;&gt;번외.&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-upernet/06.png&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 논문에서 하나의 part로 발표된 부분인데, 제 생각에는 덜 중요한 부분이어서 번외로 넣었습니다. 논문의 저자가 object와 part간의 관계를 노드와 간선으로 표현한 표 입니다. 참고만 하시면 될 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Xiao, Tete, et al. “Unified perceptual parsing for scene understanding.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.&lt;/li&gt;
  &lt;li&gt;Github https://github.com/CSAILVision/unifiedparsing&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Soyeol Lee</name></author><category term="paper-review" /><summary type="html">Unified Perceptual Parsing for Scene Understanding Understanding</summary></entry></feed>