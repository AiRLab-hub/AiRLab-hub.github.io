<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="https://blog.airlab.re.kr/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" /><updated>2019-11-27T18:56:34+09:00</updated><id>https://blog.airlab.re.kr/</id><title type="html">AiRLab. Research Blog</title><subtitle>Artificial intelligence and Robotics Laboratory</subtitle><entry><title type="html">CAM: Learning Deep Features For Discriminative Localization</title><link href="https://blog.airlab.re.kr/2019/11/CAM" rel="alternate" type="text/html" title="CAM: Learning Deep Features For Discriminative Localization" /><published>2019-11-27T11:30:00+09:00</published><updated>2019-11-27T11:30:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/CAM</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/CAM">&lt;p&gt;안녕하세요 AiRLab 박주희입니다.
오늘 소개할 논문은 Learning Deep Features For Discriminative Localization 으로 CAM이라고도 불리며,CVPR2016 에서 소개된 논문입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduction&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;CNN은 이미지 레벨의 라벨 교육만 받았음에도 object를 localization하는 능력이 뛰어납니다.
이 논문에서는 단순히 object를 localization 하는것을 넘어 이미지의 어떤 영역이 차별화 되어 사용되고 있는지를 정확히 파악하는 능력을 Weakly-Supervised Object Localization과 Visualizing CNNs을 통해 generalize 할 수있다는 것을보여줍니다. 또한 GAP를 사용한 CAM (Class Activation Map)이라는 방법을 제시하여 이미지를 차별화 하였습니다. 방금 언급한 내용들을 밑에서 더 자세히 설명을 해 보도록 하겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;weakly-supervised-object-localization&quot;&gt;Weakly-Supervised Object Localization&lt;/h4&gt;
&lt;p&gt;앞선 연구들에서는 실제로 Localization 능력을 평가하지 않았고, end-to-end로 train하지 않았으며 Object Localization을 위해 네트워크의 multiple forward pass가 필요 했습니다. 이 때문에 실제 데이터 셋으로 확장되기 어려웠고 이 논문에서는 &lt;b&gt;end-to-end로 train&lt;/b&gt;하고 &lt;b&gt;single forward pass&lt;/b&gt;로 Object Localization 할 수있음을 보였습니다. 이 접근 방식과 가장 유사한 방식은 Global Max Pooling 인데 object 한 지점을 localize 하는 방법입니다. 이 방법은 object의 전체범위를 결정하기보다 경계선에 놓여있는 한 점에 한정 됩니다. 이와 비슷한 방법인 Global Average Pooling이 이 논문에서 처음 제시한 방법은 아니지만 더욱 정확한 discriminative localization에 적용 할수있다는 것이 이 논문의 핵심입니다. 여기서 이 기술의 단순성으로 빠르고 정확한 Localization을 위해 다양한 Computer Vision에 적용 될 것이라고 믿고 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;visualizing-cnns&quot;&gt;Visualizing CNNs&lt;/h4&gt;
&lt;p&gt;앞선 연구들에서는 fully-connected layer를 무시하고 전체적으로 불완전하며 Conv층만 분석을 했습니다. 이 논문에서는 fc층을 없애고 대부분의 성능을 유지하였는데 이로써 네트워크의 처음부터 끝까지 이해를 할 수있게 되었고 이 방식은 차별화되는 이미지의 영역을 정확히 강조 할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;cam-class-activation-map&quot;&gt;CAM (Class Activation Map)&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/fig3.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
CNN에서 Global Average Pooling을 사용하여 CAM을 생성 합니다.&lt;br /&gt;
특정 카테고리에 대한 CAM은 해당 카테고리를 식별하기위해 CNN이 사용하는 차별화된 이미지 영역을 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/fig2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
위 그림은 map 생성 절차입니다.
Network In Network와 GoogleNet과 유사한 network의 아키텍쳐를 사용하여 주로 conv층으로 구성되며 최종 출력 직전의 conv feature map에 fc대신 GAP를 사용하였습니다.
GAP는 마지막 conv layer에서 각 단위의 feature map의 spatial 평균을 출력합니다.
이 값의 가중합은 최종 출력 생성에 사용되는데 이와 유사하게 이 논문에서는 CAM을 얻기 위해 conv layer의 feature map의 가중치 합을 계산하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/function1.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/function2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
F&lt;sub&gt;k&lt;/sub&gt;는 feature map을 의미하며, softmax의 식은 weight들과 F&lt;sub&gt;k&lt;/sub&gt;들과의 sum을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/fig4.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
이 그림은 MAP을 생성하기 위해 다른 클래스들을 사용할 때 단일 영상에 대한 CAM의 차이를 강조합니다. 
다른 카테고리에 대한 차별적인 영역이 특정 이미지에 대해서도 다른 것을 관찰합니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 GAP loss가 물체의 범위를 식별하도록 촉진한다고 믿는데 map의 평균을 구할 때 모든 낮은 activation이 특정 map의 출력을 감소시키기 때문에 물체의 모든 차별적인 부분을 찾아냄으로써 그 값을 최대화할 수 있기 때문입니다. (GMP는 차별적인 부분을 제외하고 모든 영상에 대한 점수는 최대값만 수행하기 때문에 score에 영향을 미치지 않습니다.)&lt;/p&gt;

&lt;h4 id=&quot;weakly-supervised-object-localization-실험&quot;&gt;Weakly-Supervised Object Localization 실험&lt;/h4&gt;
&lt;p&gt;논문에서는 classification과 localization에 대해 실험을 했습니다. 실험 설정은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;기본적인 설정으로 fc layer를 GAP로 변경하였고 여러 conv layer를 제거함으로써 mapping resolution을 향상시켰습니다.
또한 AlexNet, VGGnet, GoogleNet을 각각 수정하여 AlexNet-GAP, VGGnet-GAP, GoogleNet-GAP네트워크를 만들었습니다.&lt;/p&gt;

&lt;h3 id=&quot;classification-results&quot;&gt;Classification Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table1.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
전체적으로 conv layer를 제거하고 GAP를 사용한 네트워크들이 성능이 1-2%떨어진 모습을 볼 수있습니다.
AlexNet*-GAP는 GAP전에 conv layer를 2개 더 추가한 네트워크입니다. 
 localization에 대한 높은 성능을 얻기 위해서는 classification 성능이 중요한 것을 알 수있습니다.&lt;/p&gt;

&lt;p&gt;### Localization Results
 &lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table1.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Simple thresholding technique를 사용하여 heat 된 map의 부분을 찾는 방식을 사용하였습니다. 
(Simple thresholding technique 이란 Activation map에서 max 값을 찾아 그 20% 이상이 되는 영역을 찾고 labeling을 통해 가장 큰 덩어리를 찾고 그것을 둘러싼 bounding box를 찾는 방법입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
이 테이블은 이전 연구인 backpropagation기법을 쓴 네트워크와 CAM을 적용한 네트워크를 비교한 것입니다.&lt;br /&gt;
GoogleNet-GAP가 가장 성능이 좋은 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table3.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
Weakly-Supervised방식과 fully-supervised방식을 쓴 네트워크를 비교한 테이블입니다. 
GoogleNet-GAP(heuristics)가 37.1%로 CAM방법중 가장 성능이 좋지만 fully-supervised 방식을 쓴 AlexNet과 성능이 비슷하며 GoogleNet끼리 비교를 했을때는 약간의 성능 저하가 보입니다. 
따라서 이 논문에서는 fully-supervised 네트워크와 비교를 하기에는 아직 무리가 있다고 판단을 하였습니다. 
여기서 약간 다른 bounding box 선택 기법을 사용함으로써 GoogleNet-GAP방식에 비해  GoogleNet-GAP(heuristics) 가 5.8% 향상 되었는데 이 방식은 1등 예측 class와 2등 예측 class의 activation map으로부터 하나는 타이트하고 하나는 루즈한 bounding box를 총 두개 선택합니다. 
그리고 3등 예측 class의 bounding box로부터 루즈한 박스를 고르는 방법으로 이는 classification의 accuracy와 localization의 accuracy사이에서 trade-off 관계입니다. 
따라서 이 localization accuracy가 향상됨을 볼 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;결론&quot;&gt;결론&lt;/h4&gt;
&lt;p&gt;결론적으로 Learning Deep Features For Discriminative Localization은 CNN에서 GAP를 이용하여 bounding box annotation없이 object localization을 하는 &lt;b&gt;CAM&lt;/b&gt; 이라는 기법을 제안한 논문입니다.&lt;/p&gt;</content><author><name>Juhui Park</name></author><category term="paper-review" /><summary type="html">안녕하세요 AiRLab 박주희입니다. 오늘 소개할 논문은 Learning Deep Features For Discriminative Localization 으로 CAM이라고도 불리며,CVPR2016 에서 소개된 논문입니다.</summary></entry><entry><title type="html">Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification</title><link href="https://blog.airlab.re.kr/2019/11/He-initialization" rel="alternate" type="text/html" title="Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification" /><published>2019-11-27T10:30:00+09:00</published><updated>2019-11-27T10:30:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/He-initialization</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/He-initialization">&lt;p&gt;안녕하세요 AiRLab 박주희입니다.
오늘 소개할 논문은 Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification (https://arxiv.org/pdf/1502.01852.pdf)이며, ICCV2015에서 소개된 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;이 논문에서는 두가지 측면에 대한 image classification을 위한 rectifier neural networks를 연구했습니다.
먼저 ReLU에서 파생된 Parametric Rectified LinearUnit (PReLU)을 제안합니다. PReLU는 추가 계산 cost가 거의 들지 않고, overfitting의 위험도 적습니다.
두번째로 rectifier의 비 선형성을 고려한 강력한 초기화 방법을 도출했습니다. 이 방법은 깊은 모델에서 직접적으로 사용 할 수있고, 더 깊고 넓은 network architecture를 살펴볼수있습니다.&lt;/p&gt;

&lt;p&gt;이런 학습가능한 활성함수와 초기화 방법을 통해 ImageNet 2012 classification dataset에서 4.94% top-5 error를 달성하였습니다.
이 결과는 보고된 인간 수준 성능(5.1%)를 능가하는 최초의 결과 입니다.&lt;/p&gt;

&lt;h3 id=&quot;parametric-rectifiers&quot;&gt;Parametric Rectifiers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/ReLU_vs_PReLU.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
ReLU와 PReLU그래프 입니다. 여기서 PReLU 그래프의 경우, 음의 부분은 일정한 값을 가지지 않고 적응적으로 학습 합니다.
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/PReLU_Definition.PNG&quot; width=&quot;30%&quot; alt=&quot;error&quot; /&gt;
a&lt;sub&gt;i&lt;/sub&gt;=0 일경우 ReLU가 되고, a&lt;sub&gt;i&lt;/sub&gt;가 학습 가능한 파라미터일 경우 PReLU가 되며 a&lt;sub&gt;i&lt;/sub&gt;=0.01 일 경우 Leaky ReLU가 됩니다.
이때 PReLU는 매우 적은 수의 추가 매개변수를 도입하였습니다. (추가 파라미터의 수는 총 채널수와 동일하고, 이는 총 가중치 수를 고려할때 무시가 가능합니다.)
그렇기 때문에 Overfitting에 대해 걱정을 하지 않아도 되는 이점이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;initialization-of-filter-weights-for-rectifiers&quot;&gt;Initialization of Filter Weights for Rectifiers&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;“Xavier” initialization VS “He” initialization&lt;/b&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/Xavier_vs_He.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
“Xavier”초기화 방법은 무작위 초기화가 아닌 입력과 출력의 특성을 고려한 방법으로, 선형인 경우에서만 사용 가능하지만
“He”초기화 방법은 비선형일 경우에도 사용이 가능한 “Xavier”방법의 변형입니다. 이 방법은 비 선형적인 ReLU와 PReLU함수에서도 사용이 가능합니다.
위 그래프의 빨간색이 “He”초기화 방법을 파란색은 “Xavier”초기화 방법을 나타냅니다.
He 초기화 방법은 가중치 분포를 2로 나누어 비 선형함수에서 쓰기 더욱 적합합니다. 
두 초기화 방법 모두 수렴 가능하지만 “He”초기화 방법이 더 빨리 수렴하는 것을 볼 수있습니다.
또한 오른쪽 그래프(30-layer모델)를 보면 “He”초기화 방법은 수렴을 하지만 “Xavier”초기화 방법은 학습을 완전히 지연시키고 gradient가 감소되는것을 관찰 할 수있습니다.&lt;/p&gt;

&lt;p&gt;따라서 “He”초기화 방법이 더 깊은 모델에서 적용이 가능하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/table2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
하지만 ImageNet에서는 아직 큰 이점을 찾지 못했습니다.
30-layer 모델의 경우 38.56/16.59의 top-1/top-5 error를 가지는 반면에 위 표(14-layer)의 33.82/13.34 보다 훨씬 좋지 않음을 볼 수있는데
이는 layer가 깊을 수록 training error가 증가하기 때문이며 이 문제는 여전히 open problem 입니다.&lt;/p&gt;

&lt;p&gt;결론적으로 “He”초기화 방법은 깊은 모델에서의 정확성에 대한 이점은 보여주지 못했지만 깊이 증가에 대한 더 많은 연구를 위한 토대를 마련했습니다.&lt;/p&gt;

&lt;h3 id=&quot;experiments-on-imagenet&quot;&gt;Experiments on ImageNet&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;ReLU VS PReLU&lt;/b&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/Fig4.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
여기서 PReLU는 channel-wise 버전을 사용했고, ReLU와 PReLU 모두 같은 epoch으로 train하였습니다.
위 그래프는 training 동안 train/val error를 나타냈습니다.&lt;/p&gt;

&lt;p&gt;PReLU는 ReLU에 비해 더 빨리 수렴되는 것을 볼 수있으며 PReLU의 train error와 val error 모두 ReLU보다 낮습니다.
따라서 PReLU가 ReLU에 비해 더 좋은 성능을 가지고 있음을 다시 한번 입증 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Single-model Results and Multi-model Results&lt;/b&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/realsinglemodel.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt; 
A+ReLU가 VGG-19에 보고된 7.1% single-model의 결과 보다 상당히 좋습니다.
이는 얕은 모델을 미리 train 하지 않고 end-to-end train을 했기 때문이라고 보고 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/multimodel.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
또한 C+PReLU는 5.7%로 multi-model 보다 좋은 결과를 가졌고, model B와 modle C를 비교했을때 C가 더 나음을 볼수있습니다.
(model B는 model A에 비해 deep하고, model C는 wide한 model입니다.)&lt;/p&gt;

&lt;p&gt;따라서 모델이 충분히 깊을때 폭이 정확도에 필수적인 요소인것을 알 수있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Surpassing Human-Level Performance?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;ImageNet 데이터셋에서 인간 성능이 약 5.1% top-5 error 인데 비해 이 연구는 4.94%의 error 결과를 도출 했습니다. 이는 인간 수준의 성과를 초과했음을 의미합니다.
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/Fig5.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이들의 방법으로 위 그림을 coucal”, “komondor”, “yellow lady’s slipper” 라고 성공적으로 인식을 하는 반면에 인간은 개,새,꽃 이라고 단순하게 인식을 합니다.
이렇게 특정 데이터셋에서는 우수한 결과를 도출하지만  일반적인 객체 인식에서 문맥의 이해나 고도의 지식이 필요한 경우에는 실수를 저지르기때문에 machine vision이  human vision을 능가하는것은 아닙니다.
그럼에도 이 결과는 시각적 인식에서 인간 수준의 성능과 일치하는 machine algorithm의 잠재적 가능성을 보여줍니다.&lt;/p&gt;</content><author><name>Juhui Park</name></author><category term="paper-review" /><summary type="html">안녕하세요 AiRLab 박주희입니다. 오늘 소개할 논문은 Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification (https://arxiv.org/pdf/1502.01852.pdf)이며, ICCV2015에서 소개된 논문입니다.</summary></entry><entry><title type="html">Exploring Randomly Wired Neural Networks for Image Recognition</title><link href="https://blog.airlab.re.kr/2019/11/Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition" rel="alternate" type="text/html" title="Exploring Randomly Wired Neural Networks for Image Recognition" /><published>2019-11-26T19:00:00+09:00</published><updated>2019-11-26T19:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition">&lt;p&gt;안녕하세요 AiRLab 이상우입니다. 이번에 읽어본 논문은 Exploring Randomly Wired Neural Networks for Image Recognition 으로 Network Architecture 를 Random 하게 만들어보면 어떨까? 하는 생각을 가진 논문입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduction&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_1.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 네트워크의 정규한 패턴의 연결이 있는 Network Architecture가 꾸준히 발전해왔으며 이러한 연결들을 랜덤하게 연결하면 어떻게 되는지 실험을 해보았다고 합니다. 결과는 생각보다 놀라웠으며 기존의 Network 보다 성능이 더 좋거나 성능을 견줄만한 결과를 보여주었다고 합니다. 이로써 Network Architecture를 수동적으로 개발하는 것보다는 앞으로 Network generator 를 개발하는 것이 더 좋을 것이다라는 생각을 가지고 있습니다.&lt;br /&gt;
자세하기 알아보기전에 이 논문은 랜덤하게 연결을 해보면 어떨까? 라는 것이 메인 아이디어인만큼 사실 인공지능에서 큰 영감을 줄만한 내용보다는 어떻게 하면 Network를 랜덤하게 연결하는지와 관련된 내용이 논문의 주된 구성입니다.
하지만 제 생각에는 이 논문이 Network Architecture 의 연구 방향을 바꿀만한 논문이라 다들 한번씩 읽어보시면 좋을거 같습니다. 이제 자세한 내용을 소개해드리겠습니다.&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;methodology&quot;&gt;&lt;strong&gt;&lt;u&gt;Methodology&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;이 논문에서 소개하는 Network generator 는  &lt;strong&gt;g(Θ,s)&lt;/strong&gt; 라는 값을 가지고 네트워크를 생성합니다. 이 값이 가지는 의미를 하나하나 설명하겠습니다. &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Θ&lt;/strong&gt; : 네트워크의 다양한 정보를 포함하고 있는 값입니다. 예를 들어 VGG generator 가 있다면 VGG-16 으로 만들건지 VGG-34로 만들지를 결정하고 Network 의 깊이,폭,필터의 크기 등을 지정할 수 있습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;g&lt;/strong&gt; : graph의 연결을 결정합니다. 예를 들면 ResNet generator 가 있다면 F(x)의 값을 연결을 g를 통해서 x+f(x) 를 만들어 주는 값입니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;g(Θ)&lt;/strong&gt; : 집합 N을 반환합니다. 위에 값으로 생성된 연결과 설정을 가지고 만든 네트워크를 반환합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;s&lt;/strong&gt; : 이 과정을 몇번 반복할 것인지 정합니다. g(θ) 를 몇번 호출하여 랜덤 네트워크 패밀리를 구성할 수 있습니다.&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_2.png&quot; width=&quot;200&quot; hight=&quot;70&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 random graph 를 생성하고, 생성된 graph를 가지고 Network에 매핑을 시키는 방식을 가지고 만들었습니다. 그래서 Network generator 는 일반적인 graph를 생성하며 시작되고, 노드를 연결하는 일련의 노드와 edge를 생성합니다. edge는 위에 그림에서 보이는 노드로 들어오거나 나가는 화살표이며, 이는 데이더의 Flow라고 합니다. 파란색 원으로 구성된 부분은 노드라고 부르며 노드는 들어오는 데이터는 weight의 합계를 통해 conv로 들어가며 conv 는 ReLu - convolution - BN triplet 로 구성되어있다고 합니다. 또 노드는 몇개의 Input,Output edge를 가질 수 있다고 합니다. 이를 통해 graph 이론의 일반 graph 생성기를 자유롭게 사용하며 graph를 얻으면 신경망에 매핑이 된다고 합니다.&lt;/p&gt;

&lt;h4 id=&quot;random-graph-models&quot;&gt;&lt;strong&gt;&lt;u&gt;Random Graph Models&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_3.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에 보시다 싶이 3가지 방법으로 짜여진 graph들이 있습니다. 이는 위에서 설명한 일반 graph 생성기로 생성된 graph들이며 이 방법들이 어떻게 사용되었는지 설명을 해드리겠습니다.&lt;br /&gt;
첫번째 방법으로는 Erdos-R ˝ enyi 으로 &lt;strong&gt;ER&lt;/strong&gt;로 표시하고 있습니다. ER 은 N의 노드를 사용하는 경우, 임의의 두개의 노드는 다른 노드들과는 무관하게 edge가 P의 확률로 연결이 된다고 합니다. 이 방법은 모든 노드 쌍에 대해서 반복되며 ER은 P의 확률만을 가지고 있기때문에 ER(P) 로 표시한다고 합니다. &lt;br /&gt;
두번째 방법으로는 Barabasi-Albert 으로 &lt;strong&gt;BA&lt;/strong&gt;로 표시하고 있습니다. BA 은 순차적으로 새 노드를 추가하여 랜덤 graph를 생성하며 초기 상태는 edge가 없는 M노드부터 시작된다고 합니다. 이 방법은 순차적으로 M개의 edge가 있는 노드가 생성될 때까지 반복하며, 중복되는 방향의 edge는 생성하지 않는다고 합니다. 이 과정은 N개의 노드가 생길 때까지 반복하며, BA는 단일 파라미터 M을 가지며, BA(M)으로 표시됩니다. &lt;br /&gt;
세번째 방법으로는 Watts-Strogatz 으로 &lt;strong&gt;WS&lt;/strong&gt;로 표시하고 있습니다. WS 은 처음에 N노드는 정기적으로 링에 배치되고 각 노드는 인접한 K/2에 연결된다고 합니다. 그런 다음 시계방향 루프에서 모든 노드 V에 대해 시계방향 I번째 다음 노드에 연결하는 edge가 P의 확률로 연결이 됩니다. I는 1 ≤ i ≤ K/2 이며 K/2 번 반복된다고 합니다. &lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;&lt;strong&gt;&lt;u&gt;Experiments&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure1. Comparison on random graph generators : ER,BA, and WS&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;일반 graph 생성기 3개로 생성된 네트워크들의 정확도를 비교한 사진입니다. 직관적으로 보이는 결과이니 자세한 설명은 생략하겠습니다. &lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_5.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure2. ImageNet: small computation regime&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 논문 저자가 생성한 랜덤 네트워크로 다른 논문의 네트워크들과 비교했을 때도 정확도 면에서 경쟁력이 있는 결과는 보여줍니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_6.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure3. ImageNet: large computation regime.&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 결과는 FLOPs와 params 수가 현저히 적은데도 불구하고 다른 네트워크들과 비슷한 정확도를 보여주고 있습니다. 가장 정확도가 좋은 PNASNNet-5와 1.3% 가 나지만 FLOPs와 params 수가 확실히 차이가 나는것을 보실 수 있습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-Exploring-Randomly-Wired-Neural-Networks-for-Image-Recognition/figure_r_7.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;figure4. COCO object detection&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;COCO dataset 에서 backbone을 ResNet과 ResNext를 사용했을 때 보다 RandWire를 썻을 때 정확도가 전체적으로 향상되있는 것을 볼수 있습니다. FLOP은 비슷하거나 더 낮다고 하였습니다.&lt;br /&gt;
논문을 마치며 저의 생각은 앞으로 네트워크 구조의 발전이 네트워크 생성기의 설계쪽으로 기울어져 갈것같습니다. 비록 논문의 내용이 인공지능에 영감을 줄만한 내용은 충분히 있었다고는 생각하지 않았으나 네트워크 구조에 대한 방향성이 바뀔 수 있는 논문이라 충분히 읽어볼만 하다고 생각합니다. 부족한 점은 저의 메일이나 댓글로 남겨주세요. 감사합니다.&lt;/p&gt;</content><author><name>Sangwoo Lee</name></author><category term="paper-review" /><summary type="html">안녕하세요 AiRLab 이상우입니다. 이번에 읽어본 논문은 Exploring Randomly Wired Neural Networks for Image Recognition 으로 Network Architecture 를 Random 하게 만들어보면 어떨까? 하는 생각을 가진 논문입니다.</summary></entry><entry><title type="html">DeepLabV2</title><link href="https://blog.airlab.re.kr/2019/11/DeepLabV2" rel="alternate" type="text/html" title="DeepLabV2" /><published>2019-11-23T20:00:00+09:00</published><updated>2019-11-23T20:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/DeepLabV2</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/DeepLabV2">&lt;p&gt;DeepLabV2 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 노현철 입니다. 
제가 이번에 리뷰할 논문은 &lt;strong&gt;“DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/1.png&quot; alt=&quot;Figure1&quot; /&gt;
요번 논문을 읽기전에 DeepLabV1 논문을 읽지 않아서 인터넷을 간단하게 찾아보았습니다. 검색을 해보니 DeepLabV1에서 주장하는 내용은 CONDITIONAL RANDOM FIELDS(CRF) 라는 내용 한가지 였습니다. semantic segmentation은 픽셀단위의 조밀한 예측이 필요함으로 CRF를 후처리 과정으로 사용하여 픽셀단위 예측의 정확도를 더 높일 수 있게 되었습니다. 특히 fully connected CRF를 사용하면 위 그림과 같이 detail이 살아 있는 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/2.png&quot; alt=&quot;Figure2&quot; /&gt;
또한 이러한 CRF를 한번만 사용하는 것이 아니라  여러번 사용하게 된다면 조금 더 좋은 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 DeepLabv1을 보완하고자 CRF를 포함한 3가지 이슈가 있습니다.&lt;/p&gt;

&lt;p&gt;1) CONDITIONAL RANDOM FIELDS (CRF)&lt;/p&gt;

&lt;p&gt;2) Atrous convolution (dilated convolution)&lt;/p&gt;

&lt;p&gt;3) Atrous Spatial Pyramid Pooling (ASPP)&lt;/p&gt;

&lt;h5 id=&quot;atrous-convolution-dilated-convolution&quot;&gt;Atrous convolution (dilated convolution)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/3.png&quot; alt=&quot;Figure3&quot; /&gt;
기존 DCNN에서 receptive field을 확장 시키려면 pooling 후 convolution 했어야 했습니다. 이것은 feature들의 크기를 줄일 뿐만 아니라 연산량 또한 증가 시켰습니다. 이 논문에서 말하는 Atrous convolution은 이러한 이러한 현상들을 줄여 준다고 합니다. 이 Atrous convolution은 dilated convolution과 이름만 다를 뿐 같은 개념이라고 보시면 됩니다. Atrous convolution는 기존 convolution 과 연산량은 같지만 receptive field 가 확장되는 효과를 가져옵니다. 위에 사진을 보면 rate만큼 간격을 벌리고 그 간격은 0으로 만들어 버려서 receptive field의 크기를 확장 시키는 것 입니다.
&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/4.png&quot; alt=&quot;Figure4&quot; /&gt;
사진으로 비교해 보아도 pooling + conv 보단 Atrous convolution을 사용하는 것이 receptive field가 확장되있는 것을 직관적으로 볼 수 있습니다.&lt;/p&gt;

&lt;h5 id=&quot;atrous-spatial-pyramid-pooling-aspp&quot;&gt;Atrous Spatial Pyramid Pooling (ASPP)&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/5.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spatial Pyramid Pooling(SPP)은 sppnet에서 영감을 받고 쓰였다고 하는데 이 논문 spp에 Atrous convolution을 더하여 aspp이라는 방법을 제시 하였습니다.
Atrous convolution을 사용하여 각각의 rate 값들을 각각 {6, 12, 18. 24} 로 하여 Pyramid Pooling 하였습니다. 그리고 이들의 결과들을 합쳐 각각의 receptive field를 수용하여 여러크기의 물체를 인식하는데 좋은 결과를 가져왔습니다.&lt;/p&gt;

&lt;h5 id=&quot;deeplab-v1-v2-비교&quot;&gt;DeepLab v1, v2 비교&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/6.png&quot; alt=&quot;Figure6&quot; /&gt;
DeepLab v1, v2를 구조를 사진으로 비교해보자면 우선 input image가 들어가면 v1은 기본적인 DCNN을 사용하였지만 v2는 Atrous convolution을 사용하여 DCNN을 하였습니다. 그 결과 score map의 크기가 v1보단 v2가 더 크게 나오는것을 볼 수 있습니다. 다음은 bi-linear interpolation방법을 통해 원본의 크기만큼 upsample하였습니다. 그리고 fully connected CRF을 사용하여 정확도를 한층 높였습니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental&quot;&gt;Experimental&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/7.png&quot; alt=&quot;Figure7&quot; /&gt;
실험은 ‘fc6’ layer의 rate값들을 각각 다르게 하여 실험 하였습니다. 그 결과 Kernel: 7x7, rate: 4, CRF사용 하였을때가 Kernel: 3x3, rate: 12, CRF사용 하였을 때와 성능이 같은걸 볼 수 있습니다. 그러나 전자의 실험이 후자의 실험보다 parameters도 많고, speed(img/sec)도 느린것으로 나타났습니다. 그래서 DeepLab-LargeFOV은 kernel size 3×3, r = 12 을 사용하게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/8.png&quot; alt=&quot;Figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두번째 실험은 위 실험에서 구한 DeepLab-LargeFOV와 ASPP안에 들어가는 Atrous convolution rate값들을 다르게 하고 비교하는 실험입니다.
ASPP-S는 rate = {2, 4, 8, 12}, ASPP-L는 rate = {6, 12, 18, 24}입니다. 
실험 결과 rate값이 ASPP-S보다 높게잡은 ASPP-L의 결과가 더 좋은 것으로 나타났습니다.
&lt;img src=&quot;/assets/images/posts/2019-11-24-DeepLabV2/9.png&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;어쩌다보니 deeplab v1을 읽지않고 v2를 먼저 읽게 되었는데 나름 v1의 내용이 많이 없어서 다행이였습니다. 다음번에는 v3를 읽고 리뷰를 써보도록 하겠습니다..!&lt;/p&gt;</content><author><name>Hyeoncheol Noh</name></author><category term="paper-review" /><summary type="html">DeepLabV2 리뷰</summary></entry><entry><title type="html">PSPNET</title><link href="https://blog.airlab.re.kr/2019/11/pspnet" rel="alternate" type="text/html" title="PSPNET" /><published>2019-11-23T19:00:00+09:00</published><updated>2019-11-23T19:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/pspnet</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/pspnet">&lt;p&gt;PSPNET 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 노현철 입니다. 
제가 이번에 리뷰할 논문은 &lt;strong&gt;“Pyramid Scene Parsing Network”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;pspnet을 읽기전에 Fully Convolutional Network(FCN)에 관한 논문을 읽어보진 않아서 간단하게 인터넷으로 찾아보았습니다. FCN은 Fully Connected layer 가 없는 CNN이 통용됩다고 합니다. 이 FC layer를 없앤 이유는 위치정보의 손실 때문에 없앴다고 합니다.(Segmentation 은 위치정보가 핵심적)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/1.PNG&quot; alt=&quot;Figure1&quot; /&gt;
이 논문에서는 FCN의 한계점을 나타내고 있습니다. 위에 사진을 보듯이 FCN은 보트를 자동차로 인식을하고, 비슷한 카테고리(건물, 초고층 빌딩)는 명확하지않고 둘 다 인식을 하고, 마지막으로 베개와 시트의 외관이 비슷해서 베개를 파싱하지 못하고 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제점들을 보완하고자 이 논문에서 제안하고있는 complex-scene parsing 이슈가 세가지가 있습니다.&lt;/p&gt;

&lt;p&gt;1) Mismatched Relationship (주변 환경의 관계)&lt;/p&gt;

&lt;p&gt;2) Confusion Categories (혼란의 카테고리)&lt;/p&gt;

&lt;p&gt;3) Inconspicuous Classes (눈에 띄지 않는 클래스)&lt;/p&gt;

&lt;h5 id=&quot;mismatched-relationship&quot;&gt;Mismatched Relationship&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/2.PNG&quot; alt=&quot;Figure2&quot; /&gt;
FCN은 ‘보트’를 ‘자동차’로 인식을 하였습니다. 이것은 단순히 외관으로만 판단하였기 때문에 틀렸다 라고 볼 수 있습니다. 하지만 일반적으로 ‘강 위에 자동차’ 보단 ‘강 위에 보트’일 가능성이 더 큽니다. pspnet에서는 외관만 판단하는 것이 아니라 주변 환경까지 고려하여 ‘자동차’가 아닌 ‘보트’로 Prediction 하는 것입니다.&lt;/p&gt;

&lt;h5 id=&quot;confusion-categories&quot;&gt;Confusion Categories&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/3.PNG&quot; alt=&quot;Figure3&quot; /&gt;
Ground Truth를 보듯이 건물과 초고층빌딩사진에서 보듯이 FCN은 건물과 초고층 빌딩 둘 다 인식하고 있습니다. 이러한 비슷한 카테고리{(건물, 초고층 빌딩),(들판, 땅)}들은 혼돈을 줄 수 있습니다. 이러한 문제점을 해결하기 위해 global contextual information 사용하면 카테고리안에 relationship이 명확해 질 수 있다.&lt;/p&gt;

&lt;h5 id=&quot;inconspicuous-classes&quot;&gt;Inconspicuous Classes&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/4.PNG&quot; alt=&quot;Figure4&quot; /&gt;
FCN은 베개와 시트의 유사한 외관으로 인해 구별을 못하고 있습니다. 이는 global scene category를 보면 베개를 파싱 못할 수 있습니다. 이를 개선하기 위해서  눈에 띄지 않는 object, stuff를 포함하는 여러 sub-regions에 global contextual information를 사용하여 해결할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;pspnet구조&quot;&gt;pspnet구조&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/5.PNG&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 사진은 pspnet의 전체적인 구조를 설명하는 사진입니다.&lt;/p&gt;

&lt;p&gt;첫번째로 input image(a)가 주어지면 CNN을 사용하여 Feature map을 얻는데 여기서 사용하는 CNN은 resnet을 사용하였고 dilated Convolution를 사용한 FCN구조라고 합니다. (dilated Convolution은 공간적 특징을 유지하기 때문에 Segmentation 많이사용) 이 Feature map 에서 local contextual information를 얻고 이것을 pool을 하게됩니다. pool의 종류에는 Max pooling, average pooling 둘 다 사용하게 되었지만 average가 우세하여 대부분은 average pooling을 사용하게 되었습니다. 이러한 pooling으로 인해 global contextual information을 얻어냅니다. 또한 pool을 할때 Pyramid Pooling Module을 사용하였고 이는 (1×1, 2×2, 3×3, 6×6)×N 의 크기를 가진 sub-region으로 만들어냅니다.&lt;/p&gt;

&lt;p&gt;global contextual information의 이해를 돕기위해 사진을 보시면
&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/6.PNG&quot; alt=&quot;Figure6&quot; /&gt;
Feature map을 4개의 sub-region으로 나누고 2×2로 average pooling 하면 각 각의 sub-region 특징을 알 수 있습니다. 예를들어 초록 원이 자동차라면 나머지 sub-region의 특징이 물인지 도로인지 구분하여서 상황을 고려할 수 있습니다. ( Segmentaion에 있어서 더 좋은 성능)&lt;/p&gt;

&lt;p&gt;끝으로 (1×1, 2×2, 3×3, 6×6)×N 을 conv하여 1/N 로 줄입니다. 그 이유는 마지막 Feature map들을 Upsampling하고 기존 Feature map과 이어붙이기 때문에 비율을 같게 해주는 것 입니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental&quot;&gt;Experimental&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/7.PNG&quot; alt=&quot;Figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험은 resnet을 기본으로 사용하였고 여기서 B1은 Pyramid Pooling Module의 1x1만 사용했다 라는 뜻이고, B1236 은 1x1, 2x2, 3x3, 6x6 라는 뜻입니다. max보단 average사용했을때 성능이 더 좋았고 DR-dimension reduction (N -&amp;gt; 1/N) 을 사용했을때 성능이 제일 좋았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/8.PNG&quot; alt=&quot;Figure8&quot; /&gt;
표에도 나타나듯이 ResNet이 깊을수록 성능이 좋았고 MS(multi-scale)사용했을때 성능이 제일 좋았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-pspnet/9.PNG&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;최근들어 Segmentaion에 관심이 생겨 읽게 되었습니다. 조금 아쉬웠던 점이 이 논문이 나오기 전 논문을 먼저 읽고 읽었으면 와닿는 부분이나 이론적인 부분들이 더욱 많았을텐데 라는 생각이 들었습니다. 다음번에는 pspnet 이전에 나온 논문을 읽어보고 싶습니다..!&lt;/p&gt;</content><author><name>Hyeoncheol Noh</name></author><category term="paper-review" /><summary type="html">PSPNET 리뷰</summary></entry><entry><title type="html">Mixup: BEYOND EMPIRICAL RISK MINIMIZATION</title><link href="https://blog.airlab.re.kr/2019/11/mixup" rel="alternate" type="text/html" title="Mixup: BEYOND EMPIRICAL RISK MINIMIZATION" /><published>2019-11-23T11:00:00+09:00</published><updated>2019-11-23T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/mixup</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/mixup">&lt;h3 id=&quot;mixup-beyond-empirical-risk-minimization-review&quot;&gt;Mixup: BEYOND EMPIRICAL RISK MINIMIZATION Review&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다.&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Mixup&lt;/strong&gt; 입니다. (&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;arXiv:1710.09412&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이번에는 &lt;b&gt;Data Augmentation&lt;/b&gt; 에 관련된 논문입니다. 
&lt;b&gt;저는 Augmentation 관련 논문에서 핵심적인 부분은 결국, 한정된 Data를 어떻게 다뤄야 효과적으로 학습할 수 있을까? 에 관한 대답이라고 생각합니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;논문에서 저자는 일반적으로 Dataset에 의존하여 학습하는 것을 비판하고 있습니다. 여기서 비판이란 Dataset의 원론적인 비판이 아닌, Dataset을 그대로 학습하는 것에 대한 문제점을 지적하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;“Train Data와 조금만 다른 Data를 설명할 수 없다.” 이 부분에서 Train Data에 dependent 한 문제점을 지적하고 있습니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Deeplearning이 활성화된 시기부터 계속적으로 연구되고있는 부분이기도 합니다.&lt;/p&gt;

&lt;p&gt;저자는 한정적인 Dataset에서 어떻게하면 더 General 하게 model 을 만들 수 있을까에 대한 고민을 많이 한 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;BackGround&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;모델의 학습을 위해서 아래와 같은 expected risk를 최소화 하여야 합니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_02.png&quot; width=&quot;600&quot; height=&quot;130&quot; /&gt;[figure_01] (P(x,y) = 결합분포, L = loss-funtion)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
 그러나 대부분의 경우 Joint distribution(결합 확률분포(두 개 이상의 확률변수에 대한 확률분포))을 모릅니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 일반적인 학습의 경우 아래와 같이 학습할 Dataset을 사용하여 &lt;b&gt;empirical distribution&lt;/b&gt;을 아래와 같이 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Empirical distribution(ERM)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_03.png&quot; width=&quot;600&quot; height=&quot;170&quot; /&gt;[figure_02]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  따라서, &lt;b&gt;위에서 구한 Empirical distribution&lt;/b&gt; P를 통하여 &lt;b&gt;Expected risk R&lt;/b&gt;을 아래와 같이 나타낼 수 있습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_04.png&quot; width=&quot;800&quot; height=&quot;100&quot; /&gt;[figure_03]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  간단하게 설명하자면, [figure_01] 을 통해 model을 학습하여야 하는데, dP(x,y) 즉, (joint distribution)을 모르는 경우가 대부분이니까, Train data를 사용하여 [figure_02]와 같이 joint distribution을 empirical distribution으로 대체하여 사용한다. 라는 것입니다.&lt;/p&gt;

&lt;p&gt;결과적으로 [figure_03]과 같은 식이 됩니다.&lt;/p&gt;

&lt;p&gt;그리고 이를 &lt;b&gt;ERM(Empirical Risk Minimization)&lt;/b&gt; 이라고 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Vicinity distribution(VRM)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위에 설명한 joint distribution 을 vicinity distribution을 통해 대체 할 수 도 있습니다. 다음과 같습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_05.png&quot; width=&quot;600&quot; height=&quot;140&quot; /&gt;[figure_04] virtual feature-target pair(x ̃,y ̃),&lt;br /&gt; vicinity of the training feature-target pair (xi,yi).&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  간단하게 trian data에 가상의 어떤 data를 섞었다고 생각하면 된다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_06.png&quot; width=&quot;600&quot; height=&quot;50&quot; /&gt;[figure_05]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
이때 논문에서는 위와 같이 구성한다면, 즉, x ̃와 xi의 차를 평균으로 갖고, 분산값을 Sigma^2으로 갖는 정규분포가 vicinity distribution이 된다고 설명하고 있고, 이 효과는 학습데이터에 Gaussian noise을 더한 것으로 이해하면 된다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;이를 VRM (Vicinal Risk Minimization)이라고 한다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;u&gt;나는 지금까지 ERM/VRM을 이해하는 것이 논문에서 제안하는 바를 이해하는데 충분한 배경이 되었다.
&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;이제 본격적으로 &lt;b&gt;MixUp의 idea&lt;/b&gt;를 살펴보면, 다음과 같이 정리할 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_07.png&quot; width=&quot;100%&quot; height=&quot;100&quot; /&gt;[figure_06] Mixup&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에서 본 VRM과 수식이 많이 유사하다. 
당연한 것이 VRM에서 가상의 어떤 data를 섞는 다고 했던 부분을 Dataset에서 가져와서 쓰겠다는 것이다.&lt;/p&gt;

&lt;p&gt;실제 학습에서 사용되는 코드는 다음과 같다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_08.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_07] Mixup_pytorch_code&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;code를 보면 lambda 값은 beta분포에서 뽑게 되는데, 분포가 [0,1] 사이에서 뽑아지게 됨으로, interpolation의 비율(가상의 data를 train data에 섞는 비율) 을 랜덤하면서도 적절하게 가져갈 수 있게 된다. 여기서 alpha 값은 1로 고정한다.&lt;/p&gt;

&lt;p&gt;그렇게 되면 학습하는 이미지는 다음과 같이 볼 수 있다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_01.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_08] Mixup_pytorch_image&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (CIFAR10 &amp;amp; 100)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_09.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_09] Mixup_pytorch_image&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 CIFAR 10 &amp;amp; 100 에서 기존의 성능보다 1.1% ~ 4.5% 성능을 높이게 되었다.&lt;/p&gt;

&lt;p&gt;즉, 같은 model 같은 Dataset으로 x% 만큼 general한 model을 뽑게 되었다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (corrupted label)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_11.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_10] corrupted label Acc examples&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 표를 보면 손상된 label에 대한 정확도가 기존에 방법보다 굉장히 많은 Acc를 갖는 것을 확인 할 수 있다. 또, 이 실험을 통해서 dropout과 mixup이 긍정적인 결과를 도출해 낸다는 것 또한 확인 할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (Adversarial example)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_10.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_11] Robustness to adversarial examples&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 부분도 요즘 핫한 부분인데, 간단하게 말해서 사람눈에는 똑같이 Panda, car로 구분되지만 computer입장에서는 그렇지 못하게 만드는 즉, 오류를 범하게 만드는 noise를 첨가하는 attack 이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;FGSM,I-FGSM 의 경우 Attack mechanism 이라고 보시면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;기존 학습법 보다 Mixup의 방법이 더 높은 방어능력을 갖고 있다. 즉, 기존보다 더 adversarial attack 에 robust 하다고 볼 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (CIFAR10)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_12.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_12]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 실험 결과를 통하여 Mixup에서 weight decay값은 10^-4이 좋다는 것을 설명하고 있으며, 첨가하는 data를 어떻게 하면 좋을 지에 관한 내용이 포함되어 있습니다.&lt;/p&gt;

&lt;p&gt;가장 높은 Acc를 보인것은 AC + RP 입니다. 또, SC는 효과가 없다고 말하고 있습니다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AC : mix between all classes.
SC : mix within the same class.
RP : mix between random pairs.
KNN : mix between k-nearest neighbors(k=200)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;후기-implementation&quot;&gt;후기 (implementation)&lt;/h3&gt;

&lt;p&gt;우선, DataAugementation에 관한 논문을 처음 접했는데 굉장히 흥미로웠다. 2019_ICCV를 다녀와서 느낀점이 많은데, 그 중 하나는 Data를 어떻게 다룰것인가에 관한 관심이 생겼다는 것이다. 천천히 읽어가면서, CutMix까지 읽어볼 생각이다.&lt;/p&gt;

&lt;p&gt;이번논문을 통해, pytorch를 이용하여. 직접. 구현을 하였는데.&lt;/p&gt;

&lt;p&gt;preActresnet-18의 경우 오차가 0.5% 정도로 거의 paper-performance에 가깝게 구현되었다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_13.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_13] compare performance(even | paper | my)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;[figure_13]을 보면 알 수 있듯이 기존 model을 좀더 general 하게 가져갈 수 있다는 장점이 좋은 것 같다.&lt;/p&gt;

&lt;p&gt;그리고, 혹시나 구현중에 model의 train loss 가 잘 떨어지지 않고, train Acc가 낮다고 잘 못 한거 아닌가 라는 생각을 할 수 있는데, 직접구현한 결과 mixup은 일반적으로 학습하는 cifar100 data를  막 95% 99% 학습할 수가 없다.&lt;/p&gt;

&lt;p&gt;왜냐면 train data 가 그만큼 어렵기 때문이다.&lt;/p&gt;

&lt;p&gt;그러나 test loss 는 더 낮고 test Acc는 더높기 때문에 general 한 결과를 얻어낼 수 있기 때문에 긍정적인것 같다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;[References]&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;  &lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;mixup: Beyond Empirical Risk Minimization
&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content><author><name>Daehan Kim</name></author><category term="paper-review" /><summary type="html">Mixup: BEYOND EMPIRICAL RISK MINIMIZATION Review</summary></entry><entry><title type="html">Manifold Mixup: Better Representations by Interpolating Hidden States</title><link href="https://blog.airlab.re.kr/2019/11/manifold-mixup" rel="alternate" type="text/html" title="Manifold Mixup: Better Representations by Interpolating Hidden States" /><published>2019-11-22T11:00:00+09:00</published><updated>2019-11-22T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/manifold-mixup</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/manifold-mixup">&lt;h3 id=&quot;manifold-mixup-better-representations-by-interpolating-hidden-states-리뷰&quot;&gt;Manifold Mixup: Better Representations by Interpolating Hidden States 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Manifold Mixup: Better Representations by Interpolating Hidden States”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Manifold Mixup: Better Representations by Interpolating Hidden States은(이하 Manifold Mixup) 2019ICML에 통과된 논문으로 카테고리는 Classification, Data Augmentation 입니다. 또한 딥러닝의 대가 Yoshua Bengio가 저자로 참여된 논문입니다. Manifold Mixup은 Mixup 논문에서 영감을 얻었으며, 인풋으로 들어오는 이미지 뿐만아니라, hidden states사이에서도 mixup을 하자는게 주된 내용이고, CIFAR100에서는 mixup을 능가하는 성능을 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Manifold Mixup을 이해하기 위해서는 우선 매니폴드 가 무엇인지 먼저 아셔야 합니다. 저는 이 논문을 이해할 정도로만 매니폴드를 설명할 것 이니, 혹시 더 궁금하시다면, &lt;a href=&quot;https://www.youtube.com/watch?v=o_peo6U7IRM&amp;amp;t=4692s&quot;&gt;https://www.youtube.com/watch?v=o_peo6U7IRM&amp;amp;t=4692s&lt;/a&gt; 이활석님의 매니폴드 설명을 참고하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img2.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림이 매니폴드를 나타내는 그림 입니다. 매니폴드란 데이터가 사는 공간입니다. 위의 사진처럼 한 매니폴드가 있으면, 그 위에 모든 데이터를 표현할 수 있고, 그림처럼 똑같은 4 이더라도 다 다른 위치에 있습니다. 또한 사람눈으로는 매우 닮아있는 4 라고 할지라도 매니폴드상 거리가 멀수도, 가까울 수도 있습니다. 직관적으로 딥러닝은 이러한 매니폴드에서 공통된 특성을 가로지르는 하나의 선을 찾는 것 이라고 생각하셔도 될 것 같습니다.
이제 매니폴드를 간략하게 알았으니 다음 설명으로 넘어가겠습니다.&lt;/p&gt;

&lt;p&gt;논문 저자는 친절하게 Manifold Mixup을 한줄로 요약해 줍니다. “Manifold Mixup improves the hidden representations and decision boundaries of neural networks at multiple layers.” 즉 매니폴드 믹스업이란, “다중 레이어가 있을때 그냥 믹스업 처럼 인풋만 이미지를 섞어버리면 불공평하니, 다중 레이어 모든 핏쳐맵에서 섞자!” 입니다. 실제로 이 말이 이 논문의 전부이며 앞으로는 이 말을 증명하고 실험하는 과정입니다. 또한 이 논문은 라벨 스무딩의 효과가 있다. 라고 생각하시면 이해하기 편하실것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img3.PNG&quot; alt=&quot;Figure1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img4.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림들은 저자가 2D spiral dataset으로 시각화한 그림입니다. 상단의 그림의 왼쪽은 아무것도 사용하지 않은 base이며, 상단의 오른쪽은 Manifold Mixup 입니다. 그림에서 보이는것과 같이 두개의 정확도는 비슷하지만, base는 오버피팅이 생긴 것을 한 눈에 알수 있고, Manifold Mixup은 오버피팅이 일어나지 않은 것을 볼수 있습니다. 또한 아래의 그림은 기존 유명 regularizers 들과의 성능을 정성적으로 비교한 것이며, 정성적으로는 Manifold Mixup이 좋아 보이나, “Manifold Mixup이 다른 타 regularizers들을 능가하는 방법이다!” 라고 생각하지 마시고, 그냥 저런 유명 방법들과 견줄만한 방법이다. 정도로만 생각하시면 좋을것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;manifold-mixup&quot;&gt;Manifold Mixup&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img5.PNG&quot; alt=&quot;Figure1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img6.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식들은 Manifold Mixup을 이해하기 쉽게 저자가 수식으로 표현한 것이며, 상단의 수식은 매니폴드에 존재하는 hidden states를 섞고, label도 그 수치만큼 섞겠다는 이야기 입니다. 아래의 수식은 전체 학습 프로세스가 어떻게 작동되는지 설명한 수식이며, 학습이 진행되면 input을 포함한 hidden states에 Mixup이 동작한다는 수식입니다. 또한 Beta는 Beta분포를 따르는 것을 의미합니다. Beta분포를 사용한 이유는 랜덤하게 뽑으면 섞이는 두 대상이 일정하게 섞일 확률이 높기 때문에, 한쪽이 더 우세하게 섞기 위하여 Beta분포를 사용한 것 입니다.&lt;/p&gt;

&lt;h3 id=&quot;empirical-investigation-of-flattening&quot;&gt;Empirical Investigation of Flattening&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img7.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 Flattening 하는 것이 왜 좋은가를 실험하기 위하여 MNIST데이터 셋에서 Singular Value Decomposition (SVD)을 통하여 실험합니다.
SVD를 직관적으로 설명하면 선형대수에서 배우는 특이값 분해로 같은 이미지에서 투영변환, 스케일변환, 회전변환을 하였을때 딥러닝 관점에서 augmentation이 된 데이터들 즉 매니폴드를 지나가는 직선의 거리 라고 생각하시면 될 것 같습니다. 이러한 SVD 값이 Maniflod Mixup이 가장 작았습니다. 위의 그림은 이것을 시각화 한 것이고, Manifold Mixup을 사용한 방법에서 MNIST 데이터들이 잘 분류된걸 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img8.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 논문저자는 위의 그래프와 같이 다양한 어규멘테이션에서 실험을 합니다. 논문저자가 주장한대로 Maniflod Mixup을하면 매니폴드를 직관하는 선을 찾기 쉬워져 같은 데이터라면 결과가 좋아야 합니다. 위 그래프도 논문저자가 주장한대로 매니폴드 믹스업은 다양한 데이터에서 강인합니다. 하지만 아쉬운 점은 이 논문 저자는 극한의 튜닝을 했습니다. epoch를 2000까지 돌리는둥 ,,, 최소 1200epoch를 학습합니다.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img9.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 실험 입니다. 논문 저자는 CIFAR10, 100 TINY Imagenet에서 실험을 하였습니다. 위의 표는 그 결과 입니다. 보이시는것과 같이 CIFAR10,100에서는 베타분포를 만들때 사용하는 알파값이 2일때 성능이 가장 좋고 Manifold Mixup에서 성능이 가장 좋습니다. 하지만 TINY Imagenet에서는 mixup보다 성능이 낮은데, 아마 tiny라 그런것이고, full imagenet이면 manifold mixup이 더 좋습니다. 그 결과는 cutmix논문을 참고하시면 확인하실수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문은 저의 직관과 비슷하여 재미있게 읽었던 논문 입니다. 하지만 epoch를 1500까지 맞춰줘야 논문 성능을 구현 할수 있는점(저는 1200에 구현했습니다 ㅎㅎ) 하이퍼파라메터를 공개하지 않은점. 수학적으로 너무 무거운점이 아쉬웠습니다.&lt;/p&gt;</content><author><name>Minseok Seo</name></author><category term="paper-review" /><summary type="html">Manifold Mixup: Better Representations by Interpolating Hidden States 리뷰</summary></entry><entry><title type="html">Batch Normalization-Accelerating Deep Network Training by Reducing Internal Covariate Shift</title><link href="https://blog.airlab.re.kr/2019/11/Batch-Normalization(+group-normalization)" rel="alternate" type="text/html" title="Batch Normalization-Accelerating Deep Network Training by Reducing Internal Covariate Shift" /><published>2019-11-18T19:00:00+09:00</published><updated>2019-11-18T19:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/Batch%20Normalization(+group%20normalization)</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/Batch-Normalization(+group-normalization)">&lt;p&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 강혜윤입니다.&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/strong&gt;(Batch Norm) (&lt;a href=&quot;https://arxiv.org/pdf/1502.03167.pdf&quot;&gt;arXiv:1605.07146&lt;/a&gt;)이며 2015년 발표가 된 연구입니다.&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DNN 학습 시키는 것이 어려운 이유 : 학습을 시키는 과정에서 이전 layer의 parameter 변화로 다음 layer의 input의 분포가 변화함 (weight 값의 변화)  &lt;br /&gt;
=&amp;gt; “internal covariate shift”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization에서 normalization을 모델 아키텍처의 일부로 만들고, 각 mini-batch에 대해 normalization을 수행&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Batch normalization의 장점 : &lt;br /&gt;
① 높은 learning rate 사용 가능 &lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;② 초기화에 크게 신경 쓰지 않아도 됨&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;③ regularizer의 역할로 몇몇 경우에 Dropout을 사용하지 않아도 됨&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;[Introduction]&lt;/h3&gt;
&lt;p&gt;▶ SGD(Stochastic Gradient Descent)&lt;/p&gt;

&lt;p&gt;① Gradient Descent : loss function을 계산할 때 전체 data에 대해 계산, 계산 량이 가장 많음&lt;br /&gt;
 (배치는 전체 데이터 셋라고 가정)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;② Mini-batch Gradient Descent : training data의 배치(batches)만 이용해서 그라디언트(gradient)를 구하는 것&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;③ Stochastic Gradient Descent : loss function을 계산할 때 일부 data에 대해 계산 (mini-batch), 계산 량을 줄임&lt;br /&gt;
 (확률적(Stochastic) : 용어는 각 배치를 포함하는 하나의 예가 무작위로 선택된다는 것을 의미)&lt;br /&gt;
 미니배치(mini-batch)가 데이터 한 개로 이루어짐&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DNN을 효율적으로 학습시킬 수 있도록 함&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;SGD의 변형인 momentum, Adagrad 등도 있음&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;loss값을 최소화하기 위해 parameterθ을 최적화시킴&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/1.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SGD 사용 시 특징&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;① Stochastic Gradient는 실제 Gradient의 추정 값이며 이것은 미니배치의 크기 N이 커질수록 더 정확한 추정 값을 가지게 된다.&lt;br /&gt;
 ② 미니배치를 뽑아서 연산을 수행하기 때문에 최신 컴퓨팅 플랫폼에 의하여 병렬적인 연산 수행이 가능하여 더욱 효율적이다.&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존의 문제점&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;① neural network를 학습하기 위한 hyper parameter들의 초기 값 설정을 굉장히 신중하게 해줘야 함 (그렇지 않으면 covariate shift가 발생할 수 있음)&lt;/p&gt;

&lt;p&gt;② Saturation problem&lt;br /&gt;
  기울기가 0이 되어 weight 업데이트량이 0이 되는 현상&lt;br /&gt;
  sigmoid 함수에서 큰 값을 가지면 기울기가 0이 되어 사라짐&lt;br /&gt;
  -&amp;gt; ReLU 함수 사용과 신중하게 initialization하고 작은 learning rate를 사용하면서 해결 (완전한 해결책은 아님)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch Normalization의 등장&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;① internal covariate shift 문제를 줄일 수 있음&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;② DNN 학습을 가속화 시킬 수 있음&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;[batch normalization 공식]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/2.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;mini batch 단위에서 정규화 수행&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Mini batch 내의 한 example 내에서의 Activation 들은 각각 독립적&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Γ: scale 조정, β: shift 조정&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiments_mnist&quot;&gt;[Experiments_MNIST]&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/3.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;⦁Batch Normalization makes the distribution more stable and reduces the internal covariate shift.&lt;/p&gt;

&lt;p&gt;[Experiments_ImageNet classification]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/6.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;group-normalization과-batch-normalization-비교&quot;&gt;[Group normalization과 Batch normalization 비교]&lt;/h3&gt;

&lt;p&gt;Normalization formulation
&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/9.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;① Batch normalization
&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/10.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;
 -&amp;gt; batch 단위로 정규화 수행 &lt;br /&gt;
 -&amp;gt; batch norm의 문제점 : batch size 단위로 정규화 시키는 과정에서 batch size에 의존적이게 되고, batch size가 작을 경우 학습이 잘 이루어지지 않음 &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;② Group normalization
&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/11.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 문제점을 해결하기 위해 정규화를 batch 단위로 하지 않고, channel을 그룹으로 나누어 그 그룹을 단위로 정규화 수행&lt;br /&gt;
 (더 이상 batch size에 의존적이지 않음)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-18-BN/12.PNG&quot; alt=&quot;Batch Normalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;-&amp;gt; 위에 실험을 보면 batch size에 의존적인 BN은 batch size 별로 학습에 차이를 보였고, batch size에 비의존적인 GN은 어떤 batch size에도 학습이 잘 이루어지는 것을 확인할 수 있다.&lt;/p&gt;</content><author><name>Hyeyun Kang</name></author><category term="paper-review" /><summary type="html">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</summary></entry><entry><title type="html">Shake-Shake regularization</title><link href="https://blog.airlab.re.kr/2019/11/shake-shake-regularization" rel="alternate" type="text/html" title="Shake-Shake regularization" /><published>2019-11-16T11:00:00+09:00</published><updated>2019-11-16T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/shake-shake-regularization</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/shake-shake-regularization">&lt;h3 id=&quot;shake-shake-regularization-리뷰&quot;&gt;Shake-Shake regularization 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Shake-Shake regularization”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 크게보면 Data augmentation에 속하는 논문 입니다. 지금까지는 Data augmentation을 이미지에 했다면, Shake-Shake regularization 논문은 Internal Representations에 augmentation을 합니다. (Internal Representations은 feature map과, weight를 의미합니다.) 이런 Internal Representations augmentation을 통하여 그 당시 CIFAR10, CIFAR100에서 state of the art를 달성합니다.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img1.PNG&quot; alt=&quot;Figure1&quot; /&gt;
위 그림과 같이 컴퓨터는 사람과는 다르게 물체를 일정 규칙이 있는 스칼라 값으로 인식합니다. 그렇다면 컴퓨터 입장에서는 feature map 그리고 이미지 는 일정 규칙이 있는 데이터이기 때문에 현재처럼 이미지에서만 Data augmentation을 하지말고, Internal Representations에도 Data augmentation을 해주자고 주장합니다. 또한 Internal Representations에 Data augmentation을 해주면 stochastically “blending” 효과가 있다고 주장합니다.&lt;/p&gt;

&lt;h2 id=&quot;model-description-on-3-branch-resnets&quot;&gt;Model description on 3-branch ResNets&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img2.PNG&quot; alt=&quot;Figure2&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img3.PNG&quot; alt=&quot;Figure3&quot; /&gt;
논문 저자는 Shake-Shake regularization을 적용하기 간편한 ResNet 구조에서 실험을 하고, 이해를 돕기 위하여 위와 같은 간단한 수식을 말합니다. 위에 보이는 수식 1은 일반적은 resnet입니다. W는 weight이고, x는 텐서, F는 residual function입니다. 논문 저자는 수식 2와 같이, F앞에 일정 α(0과 1 사이의 랜덤한 값)을 곱해줘 Internal Representations에 Data augmentation을 해주는 효과를 냅니다.&lt;/p&gt;

&lt;h2 id=&quot;training-procedure&quot;&gt;Training procedure&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/cover.PNG&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 shake-shake-regularization의 전체 학습 절차 입니다. 학습의 forward 부분에서도 α[0~1] 사이의 값을 곱해줘서 data augmentation 효과를 주고, backward 부분에도 β[0~1]의 값을 곱해줘서 기존의 연구 되었던 gradient noise를 주는 방식을 gradient augmentaion으로 대체해 줄 수 있다고 논문 저자는 주장합니다. 또한 테스트시에는 0.5로 값을 고정해 줍니다.&lt;/p&gt;

&lt;h2 id=&quot;cifar-10-cifar-100&quot;&gt;CIFAR-10, CIFAR-100&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img4.PNG&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 본 논문의 저자는 다양한 실험을 진행하였습니다. Forward pass는 Even / Shake, Backward pass는 Even / Shake / Keep, Mini-batch update rule 은 Image / Batch로 진행하였습니다. Even은 α값을 0.5로 고정하는 방법이고, Shake는 α값은 0~1사이의 값을 랜덤하고 곱해줍니다. Backward의 Kepp은 Forward에 사용한 α값을 그대로 사용하는것 입니다. 또한 Image는 미니배치에서 모든 이미지에 α,β 를 적용하는것이고, Batch 는 미니 배치 단위로 다 같은 α,β를 적용하는것 입니다. 위 그림과 같이 shake shake image가 가장 성능이 좋았고, Even shake batch가 가장 성능이 안 좋았습니다. shake shake image 가 가장 많은 augmentation을 적용한 것이니 당연한 결과라고 생각하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img5.PNG&quot; alt=&quot;Figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 배치사이즈를 128에서 32로 줄인 다음 CIFAR100에서 실험한 테이블 입니다. 위에 보이는 결과와 같이 배치사이즈가 줄어 들때는 S S I 조합 보다는 S E I 조합이 조금 더 경쟁력 있다는 것을 알 수 있습니다. 논문저자도 이와 관련해서 해석을 하지 못했습니다. 저 또한 그 이유를 해석하는데 어려움이 있어 혹시 아시는분은 댓글 부탁 드립니다. ㅠㅡㅠ&lt;/p&gt;

&lt;h2 id=&quot;comparisons-with-state-of-the-art-results&quot;&gt;Comparisons with state-of-the-art results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img6.PNG&quot; alt=&quot;Figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 표는 기존의 state-of-the-art 결과와 비교한 표 입니다. 보이시는 것과 같이 저자가 주장한 방법이 state-of-the-art를 달성하였습니다. CIFAR10 에서는 S S I 가 CIFAR100에는 S E I 가 가장 좋았습니다. 이 결과를 분석해보면, gradient에 β를 곱해주는 방법은 좋은 성능을 보장하지 못합니다. 제 생각에는 저자가 주장하는 방법은 gradient noise방법을 완벽하게 대체하지는 못하는 것 같습니다. 또한 논문 저나는 imagenet실험을 하지 않아 많이 아쉽습니다.(제 생각에는 imagenet에서는 잘 안될것 같긴 합니다.)&lt;/p&gt;

&lt;h2 id=&quot;correlation-between-residual-branches&quot;&gt;Correlation between residual branches&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img7.PNG&quot; alt=&quot;Figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 논문저자는 Shake-Shake regularization을 분석합니다. residual branches사이의 공분산을 구하여 서로의 관계성을 계산합니다. 위 그림은 feature map을 펼친후 나온 스칼라 값들로 관계성을 구한것 입니다. 거의 아무것도 안해준 조합인 Even Even Batch 와 거의 모든 augmentation을 해준 Shake Shake Image 조합을 비교 하였습니다. 그 결과, Shake Shake Image 조합에서 residual branches 값들 사이의 관계성이 작게 나왔으므로, Shake-Shake regularization을 해준다면 overfitting이 일어날 확률을 낮춰줄 수 있음을 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;후기&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img8.PNG&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 사실 이 논문 저자는 실험을 상당히 많이 했습니다. BatchNorm을 사용하지 않거나, skip connection을 제거하는등 많은 실험을 하였습니다. 하지만 본 리뷰에서 그런 실험을 리뷰하지 않는 이유는 너무나도 당연하기 때문입니다. BatchNorm을 사용하면 성능저하가 있고, skip connection을 제거하면 성능의 약간의 향상은 있지만 유의미 하지는 않습니다. 또한 그 행동이 CIFAR10,100이여서 상승한 것이지 imagenet이었다면 어떻게 될지 모른다는 생각을 했습니다. 이 논문 자체로는 유의미하게 좋은 논문이라고 생각들진 않지만, shake drop, mixup, cutmix등 다른 augmentation 논문을 읽을때 큰 도움이 된다고 생각이 됩니다. (또한 이 논문이 쓸 내용이 없을떄 어떻게 논문을 꽉 꽉 채울수 있나 좋은 교본으로 느껴졌습니다 ㅋㅋㅋ)&lt;/p&gt;</content><author><name>Minseok Seo</name></author><category term="paper-review" /><summary type="html">Shake-Shake regularization 리뷰</summary></entry><entry><title type="html">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title><link href="https://blog.airlab.re.kr/2019/11/gcnet" rel="alternate" type="text/html" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond" /><published>2019-11-16T11:00:00+09:00</published><updated>2019-11-16T11:00:00+09:00</updated><id>https://blog.airlab.re.kr/2019/11/gcnet</id><content type="html" xml:base="https://blog.airlab.re.kr/2019/11/gcnet">&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!
오늘 소개할 논문은 GCNet으로 (&lt;a href=&quot;https://arxiv.org/abs/1904.11492&quot;&gt;arXiv:1904.11492&lt;/a&gt;)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.&lt;/p&gt;

&lt;p&gt;이전에 소개시켜드렸던 &lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;Non-local&lt;/a&gt;을 시각화하고, 문제점을 지적하는 논문으로 의미가 크다고 생각합니다.&lt;/p&gt;

&lt;h2 id=&quot;non-local&quot;&gt;Non-local&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/non-local.png&quot; width=&quot;60%&quot; alt=&quot;non-local&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Non-local Block은 Global Conext를 query-specific하게 접근하는 방식으로 많이 알려져 왔습니다.&lt;/p&gt;

&lt;p&gt;위 그림의 NL 블록을 보더라도 Shape이  HW x HW 로 각 포인트에 대하여 전체 포인트에 대한 Score Map을 만드는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 GCNet에서는 이러한 필터를 시각화해보았는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure2.png&quot; width=&quot;60%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 결과를 보면 각 Point(query)에 대한 NL의 시각화 맵을 볼 수 있는데, 여기서 재밌는 것은 서로 다른 Point에 대하여 모든 결과들이 거의 유사한 heatmap을 보인다는 것입니다. 이는 정량적으로도 확인을 할 수 있는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure3.png&quot; width=&quot;60%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Method는 여러 Non-local의 종류이고, E-Gaussian(Embedding Gaussian)이 Original NL 입니다.&lt;/p&gt;

&lt;p&gt;여기서의 Cosine distance를 보더라고, 서로 다른 InPut에 대하여 큰 차이가 없는 Output이 나오는 것을 정량적으로도 확인을 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;simplifying-the-non-local-block-snl&quot;&gt;Simplifying the Non-local Block (SNL)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure7.png&quot; width=&quot;50%&quot; alt=&quot;figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과처럼 모든 point에서 유사한 heatmap이 나온다고 하면, 충분히 NL의 크기를 줄일 수 있을 것이라고 생각이 드실 겁니다. 논문에서도 이러한 부분을 지적하며 경량화된 Simplifying Non-local Block을 제시하는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure4.png&quot; width=&quot;50%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식은 기존의 NL로 각 point i에 대하여, 모든 point j가 연산되는 i, j가 서로 연관적인 것으로 표현이 되고 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 위의 결과처럼 모든 포인트에 대하여, 비슷한 결과가 나오고 있으니 i와 j는 독립적(query independent)으로 아래와 같이 변경해 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure5.png&quot; width=&quot;50%&quot; alt=&quot;figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 조금 더 연산량을 줄이기 위해 W(v)를 밖으로 뺌으로서 아래의 SNL이 완셩이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure6.png&quot; width=&quot;50%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 Mask RCNN에서의 결과를 보게 되면 NL과 SNL의 성능이 크게 차이가 나지 않음을(아니 조금더 좋음을?)볼 수 있습니다. 또한 연산량과, Prams를 보더라도 SNL이 훨씬 효율적이여 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure8.png&quot; width=&quot;50%&quot; alt=&quot;figure7&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;global-context-block&quot;&gt;Global Context Block&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure9.png&quot; width=&quot;80%&quot; alt=&quot;figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진처럼 Global Context 모듈들은 Context modeling과 Transform 모델로 분리되어 표현이 될 수 있습니다. 여기서 GCNet은 SNL의 context modeling block 과 SE block의 Transform block이 합쳐진 형태로 구성이 되었습니다. 때문에 SE의 Transform에서 Conv를 2번 나누어 계산 하는 효과인 연산량을 더 낮추는 효과도 가지게 되었습니다.(SNL Transform 연산량: C&lt;em&gt;C, GCNet 연산량: C&lt;/em&gt;C/r + C/r*C)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure10.png&quot; width=&quot;50%&quot; alt=&quot;figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 위해 하이퍼 파라미터(r) 하나가 추가 되었는데요, 논문에서는 r을 4로 설정한 것이 가장 높은 성능을 보인다고 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure11.png&quot; width=&quot;50%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추가적으로 Layer Norm을 추가하여, Normalize 및 Regularize의 효과를 보았다고 합니다.&lt;/p&gt;

&lt;p&gt;따라서, 최종적으로 GCNet은 아래 수식 하나로 표현이 되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure11&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문의 실험은 Mask RCNN과 Classification, Action Recognition에서 실험되었고, 모든 보틀넥에서 GCBlock을 사용한 것이 가장 좋은 성능을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;마치며&quot;&gt;마치며..&lt;/h2&gt;

&lt;p&gt;이번 논문은 NL의 문제점을 지적하고, NL을 시각화 하는 것만으로도 충분하게 의미기 큰 논문이었던 것 같습니다. 또한 NL의 파라미터와 연산량을 줄여 기존에 한개 혹은 두개만 사용가능 했던 것을 모든 보틀넥에 삽입하여 성능을 크게 키운 것도 인상적입니다.&lt;/p&gt;

&lt;p&gt;하지만, all GCNet처럼, all SNL도 실험 결과가 있었으면 하는 아쉬움도 남습니다.&lt;/p&gt;

&lt;p&gt;이번에 ICCV2019를 다녀와서 다음번에 ICCV에 소개되었던 재밌는 논문들을 더 소개시켜드리도록 하겠습니다. 감사합니다 :)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cao, Yue, et al. “GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond.” arXiv preprint arXiv:1904.11492 (2019).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;https://j911.me/2019/11/nonlocal.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaemin Lee</name></author><category term="paper-review" /><summary type="html">안녕하세요. AiRLab(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다! 오늘 소개할 논문은 GCNet으로 (arXiv:1904.11492)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.</summary></entry></feed>