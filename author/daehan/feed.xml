<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/daehan/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-12-11T13:45:59+09:00</updated>
  <id>https://blog.airlab.re.kr/author/daehan/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Mixup: BEYOND EMPIRICAL RISK MINIMIZATION</title>
      <link href="https://blog.airlab.re.kr/2019/11/mixup" rel="alternate" type="text/html" title="Mixup: BEYOND EMPIRICAL RISK MINIMIZATION" />
      <published>2019-11-23T11:00:00+09:00</published>
      <updated>2019-11-23T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/mixup</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/mixup">&lt;h3 id=&quot;mixup-beyond-empirical-risk-minimization-review&quot;&gt;Mixup: BEYOND EMPIRICAL RISK MINIMIZATION Review&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다.&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Mixup&lt;/strong&gt; 입니다. (&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;arXiv:1710.09412&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이번에는 &lt;b&gt;Data Augmentation&lt;/b&gt; 에 관련된 논문입니다. 
&lt;b&gt;저는 Augmentation 관련 논문에서 핵심적인 부분은 결국, 한정된 Data를 어떻게 다뤄야 효과적으로 학습할 수 있을까? 에 관한 대답이라고 생각합니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;논문에서 저자는 일반적으로 Dataset에 의존하여 학습하는 것을 비판하고 있습니다. 여기서 비판이란 Dataset의 원론적인 비판이 아닌, Dataset을 그대로 학습하는 것에 대한 문제점을 지적하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;“Train Data와 조금만 다른 Data를 설명할 수 없다.” 이 부분에서 Train Data에 dependent 한 문제점을 지적하고 있습니다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Deeplearning이 활성화된 시기부터 계속적으로 연구되고있는 부분이기도 합니다.&lt;/p&gt;

&lt;p&gt;저자는 한정적인 Dataset에서 어떻게하면 더 General 하게 model 을 만들 수 있을까에 대한 고민을 많이 한 것으로 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;BackGround&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;모델의 학습을 위해서 아래와 같은 expected risk를 최소화 하여야 합니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_02.png&quot; width=&quot;600&quot; height=&quot;130&quot; /&gt;[figure_01] (P(x,y) = 결합분포, L = loss-funtion)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
 그러나 대부분의 경우 Joint distribution(결합 확률분포(두 개 이상의 확률변수에 대한 확률분포))을 모릅니다.&lt;/p&gt;

&lt;p&gt;그렇기 때문에 일반적인 학습의 경우 아래와 같이 학습할 Dataset을 사용하여 &lt;b&gt;empirical distribution&lt;/b&gt;을 아래와 같이 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Empirical distribution(ERM)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_03.png&quot; width=&quot;600&quot; height=&quot;170&quot; /&gt;[figure_02]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  따라서, &lt;b&gt;위에서 구한 Empirical distribution&lt;/b&gt; P를 통하여 &lt;b&gt;Expected risk R&lt;/b&gt;을 아래와 같이 나타낼 수 있습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_04.png&quot; width=&quot;800&quot; height=&quot;100&quot; /&gt;[figure_03]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  간단하게 설명하자면, [figure_01] 을 통해 model을 학습하여야 하는데, dP(x,y) 즉, (joint distribution)을 모르는 경우가 대부분이니까, Train data를 사용하여 [figure_02]와 같이 joint distribution을 empirical distribution으로 대체하여 사용한다. 라는 것입니다.&lt;/p&gt;

&lt;p&gt;결과적으로 [figure_03]과 같은 식이 됩니다.&lt;/p&gt;

&lt;p&gt;그리고 이를 &lt;b&gt;ERM(Empirical Risk Minimization)&lt;/b&gt; 이라고 합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Vicinity distribution(VRM)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위에 설명한 joint distribution 을 vicinity distribution을 통해 대체 할 수 도 있습니다. 다음과 같습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_05.png&quot; width=&quot;600&quot; height=&quot;140&quot; /&gt;[figure_04] virtual feature-target pair(x ̃,y ̃),&lt;br /&gt; vicinity of the training feature-target pair (xi,yi).&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  간단하게 trian data에 가상의 어떤 data를 섞었다고 생각하면 된다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_06.png&quot; width=&quot;600&quot; height=&quot;50&quot; /&gt;[figure_05]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
이때 논문에서는 위와 같이 구성한다면, 즉, x ̃와 xi의 차를 평균으로 갖고, 분산값을 Sigma^2으로 갖는 정규분포가 vicinity distribution이 된다고 설명하고 있고, 이 효과는 학습데이터에 Gaussian noise을 더한 것으로 이해하면 된다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;이를 VRM (Vicinal Risk Minimization)이라고 한다.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;u&gt;나는 지금까지 ERM/VRM을 이해하는 것이 논문에서 제안하는 바를 이해하는데 충분한 배경이 되었다.
&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;이제 본격적으로 &lt;b&gt;MixUp의 idea&lt;/b&gt;를 살펴보면, 다음과 같이 정리할 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-Mixup/img_07.png&quot; width=&quot;100%&quot; height=&quot;100&quot; /&gt;[figure_06] Mixup&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위에서 본 VRM과 수식이 많이 유사하다. 
당연한 것이 VRM에서 가상의 어떤 data를 섞는 다고 했던 부분을 Dataset에서 가져와서 쓰겠다는 것이다.&lt;/p&gt;

&lt;p&gt;실제 학습에서 사용되는 코드는 다음과 같다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_08.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_07] Mixup_pytorch_code&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;code를 보면 lambda 값은 beta분포에서 뽑게 되는데, 분포가 [0,1] 사이에서 뽑아지게 됨으로, interpolation의 비율(가상의 data를 train data에 섞는 비율) 을 랜덤하면서도 적절하게 가져갈 수 있게 된다. 여기서 alpha 값은 1로 고정한다.&lt;/p&gt;

&lt;p&gt;그렇게 되면 학습하는 이미지는 다음과 같이 볼 수 있다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_01.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_08] Mixup_pytorch_image&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (CIFAR10 &amp;amp; 100)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_09.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_09] Mixup_pytorch_image&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 CIFAR 10 &amp;amp; 100 에서 기존의 성능보다 1.1% ~ 4.5% 성능을 높이게 되었다.&lt;/p&gt;

&lt;p&gt;즉, 같은 model 같은 Dataset으로 x% 만큼 general한 model을 뽑게 되었다는 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (corrupted label)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_11.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_10] corrupted label Acc examples&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 표를 보면 손상된 label에 대한 정확도가 기존에 방법보다 굉장히 많은 Acc를 갖는 것을 확인 할 수 있다. 또, 이 실험을 통해서 dropout과 mixup이 긍정적인 결과를 도출해 낸다는 것 또한 확인 할 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (Adversarial example)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_10.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_11] Robustness to adversarial examples&lt;br /&gt; &lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 부분도 요즘 핫한 부분인데, 간단하게 말해서 사람눈에는 똑같이 Panda, car로 구분되지만 computer입장에서는 그렇지 못하게 만드는 즉, 오류를 범하게 만드는 noise를 첨가하는 attack 이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;FGSM,I-FGSM 의 경우 Attack mechanism 이라고 보시면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;기존 학습법 보다 Mixup의 방법이 더 높은 방어능력을 갖고 있다. 즉, 기존보다 더 adversarial attack 에 robust 하다고 볼 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Experiments (CIFAR10)&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_12.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_12]&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 실험 결과를 통하여 Mixup에서 weight decay값은 10^-4이 좋다는 것을 설명하고 있으며, 첨가하는 data를 어떻게 하면 좋을 지에 관한 내용이 포함되어 있습니다.&lt;/p&gt;

&lt;p&gt;가장 높은 Acc를 보인것은 AC + RP 입니다. 또, SC는 효과가 없다고 말하고 있습니다.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;AC : mix between all classes.
SC : mix within the same class.
RP : mix between random pairs.
KNN : mix between k-nearest neighbors(k=200)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;후기-implementation&quot;&gt;후기 (implementation)&lt;/h3&gt;

&lt;p&gt;우선, DataAugementation에 관한 논문을 처음 접했는데 굉장히 흥미로웠다. 2019_ICCV를 다녀와서 느낀점이 많은데, 그 중 하나는 Data를 어떻게 다룰것인가에 관한 관심이 생겼다는 것이다. 천천히 읽어가면서, CutMix까지 읽어볼 생각이다.&lt;/p&gt;

&lt;p&gt;이번논문을 통해, pytorch를 이용하여. 직접. 구현을 하였는데.&lt;/p&gt;

&lt;p&gt;preActresnet-18의 경우 오차가 0.5% 정도로 거의 paper-performance에 가깝게 구현되었다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-11-23-mixup/img_13.png&quot; width=&quot;100%&quot; height=&quot;100%&quot; /&gt;[figure_13] compare performance(even | paper | my)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;[figure_13]을 보면 알 수 있듯이 기존 model을 좀더 general 하게 가져갈 수 있다는 장점이 좋은 것 같다.&lt;/p&gt;

&lt;p&gt;그리고, 혹시나 구현중에 model의 train loss 가 잘 떨어지지 않고, train Acc가 낮다고 잘 못 한거 아닌가 라는 생각을 할 수 있는데, 직접구현한 결과 mixup은 일반적으로 학습하는 cifar100 data를  막 95% 99% 학습할 수가 없다.&lt;/p&gt;

&lt;p&gt;왜냐면 train data 가 그만큼 어렵기 때문이다.&lt;/p&gt;

&lt;p&gt;그러나 test loss 는 더 낮고 test Acc는 더높기 때문에 general 한 결과를 얻어낼 수 있기 때문에 긍정적인것 같다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;[References]&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;  &lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;mixup: Beyond Empirical Risk Minimization
&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Daehan Kim</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Mixup: BEYOND EMPIRICAL RISK MINIMIZATION Review</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Fast-R-CNN</title>
      <link href="https://blog.airlab.re.kr/2019/10/Fast-R-CNN" rel="alternate" type="text/html" title="Fast-R-CNN" />
      <published>2019-10-04T00:00:00+09:00</published>
      <updated>2019-10-04T00:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/10/Fast%20R-CNN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/10/Fast-R-CNN">&lt;p&gt;Fast-R-CNN Review&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. 지난번 R-CNN 에 이어서 Fast-R-CNN 을 Review 해보려고 합니다.. ^_^&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Fast-R-CNN&lt;/strong&gt;(&lt;a href=&quot;https://https://arxiv.org/abs/1504.08083&quot;&gt;arXiv:1504.08083&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduciton&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduciton&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;당시 deep ConvNet[14,16]이 발전하면서 image classification과 object detection의 Acc가 상당히 향상 되었다고 합니다. &lt;br /&gt;
당연히 classification보다 object detection이 복잡합니다. 때문에, [9,11,19,25]는 느리고 비효율적인 multi-stage pipline model을 train합니다.&lt;br /&gt;
[14,16]은 Alex_net과 Backpropagation입니다.&lt;br /&gt;
[9,11,19,25]는 R-CNN,SPP,Overfeat,segDeepM입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Multi-stage_pipline&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;
R-CNN에서 3개의 module을 따로 학습 해야하는 것을 생각하면 될 것 같습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;논문에서는&lt;/strong&gt; detection task가 object의 정밀한 localization을 필요로 하기 때문에 &lt;u&gt;두가지 issue&lt;/u&gt;가 생긴다고 합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Issue1]&lt;/strong&gt; : 수많은 객체 위치 후보들을 처리해야한다.&lt;br /&gt;
이는 Speed,accuracy,simplicification을 손상시킵니다.&lt;br /&gt;
&lt;strong&gt;[Issue2]&lt;/strong&gt; : 정확한 localization을 위해 rough_localization을 다듬어야한다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;때문에 논문에서는&lt;/strong&gt; [9,11]Network를 base로 하여 train과정을 단순화 합니다. (Sigle-stage traing algorthm을 제안합니다.) 결과적으로, R-CNN,SPP_net보다 VGGnet을 각각 9배,3배 빠르게 학습합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;R-CNN의 단점&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Multi-stage pipline(train)(Region proposal,SVM,bounding-box)&lt;/li&gt;
  &lt;li&gt;SVM과 bounding-box regressor의 경우(train) 각 image의 proposal의 feature들이 disk에 기록됩니다. (VGG16의 경우 2.5일이 소요됩니다._VOC07)&lt;/li&gt;
  &lt;li&gt;object detection이 느리다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;논문에서는&lt;/strong&gt; Fast-R-CNN을 설명하기전, R-CNN과 SPPnet을 비교합니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPPnet은 sharing compute를 이용하여 R-CNN의 속도를 높이기 위해 제안되었다.
    &lt;ul&gt;
      &lt;li&gt;여기서 말하는 sharing compute는 R-CNN과 달리 SPPnet은 한번의 convnet을 통과한 feature를 이용해 detection을 함으로써, end-to-end 학습이 된다는 이야기를 하는 것 입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결과적으로, test:10~100배, train:3배 빠릅니다. &lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;그러나&lt;/strong&gt;, SPPnet도 주목할만한 &lt;strong&gt;단점&lt;/strong&gt;이 있습니다. (drawback)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPPnet도 R-CNN과 마찬가지로, train은 multi-stage pipline(feature extraction/fine-tune(log_loss)/SVM train, bounding-box regressors 를 가집니다.) 또, feature가 disk에 기록됩니다.&lt;/li&gt;
  &lt;li&gt;SPPnet에서 제안된 algorithm은 convolutional layer를 update할수 없고 convolution layer 전에 SPP를 할 수 없다.(network perpomance를 제한한다.)&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Contributions&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Fast-R-CNN)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;image의 region_proposal을 crop&amp;amp;warp한 R-CNN과 달리 Fast-R-CNN은 입력이미지와 RoI_projection(object proposal)을 입력으로 받는다.&lt;/li&gt;
  &lt;li&gt;network를 통해 convfeature map을 생성하기위해 전체 이미지를 처리한다.&lt;/li&gt;
  &lt;li&gt;각 object proposal에 대해 RoIPooling layer가 고정길이 vector를 추출한다. (각 feature vector는 fc_layer에 연속적으로 전달된다.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Multi-task loss&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure10.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Multi-task loss)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;RoI pooling layer&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;
입력이미지에 한번만 CNN을 적용하고 RoI pooling을 이용하여 object 판별을 위한 feature를 추출하자는 것이 핵심이다.&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RoI로 추출할 feature size = HxW(eg.7x7)&lt;/li&gt;
  &lt;li&gt;Feature map 위에 RoI의 좌표 (r,c,h,w)&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure7.gif&quot; width=&quot;678&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(RoI pooling layer)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Feature map위에서 h/H x w/W 만큼 Grid를 만들어 pooling 하면 결과적으로 원하는 HxW (첫 fc-layer와 호환이 되도록 하는 사이즈)feature size가 됩니다.&lt;br /&gt;
(서로 다른 size의 region proposal이 입력되더라도 같은 size의 stride로 7x7을 만들어준다.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Initializing from pre-trained networks&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 pre-train된 ImageNet networks를 각각 실험합니다. pre-train network가 Fast-R-CNN을 initialize할때 3가지의 transform을 거칩니다.&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;마지막 maxpooling layer를 RoI pooling layer로 대체 한다.&lt;/li&gt;
  &lt;li&gt;network_last fully connected layer와 softmax layer는  two sibling layer로 대체됩니다.&lt;/li&gt;
  &lt;li&gt;network는 two input 을 받도록 합니다. (image list와 해당 image의 RoI 목록)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Mini-batch sampling&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;fine-tuning에서, N=2(image),R=128 로 하여 R/N으로 image당 64RoI를 뽑습니다. CNN연산량을 줄이는 효과가 있습니다.       다음 그림과 같은 방식을 hierarchical sampling이라고 합니다.(계층적 샘플링)
Ground-truth와 IoU &amp;gt; 0.5 (Positive), [0.1,0.5] 일 경우 Negative로 구분합니다. (즉, IoU가 너무 낮은것은 sample로 추가하지 않는다.) 
//train 할때, image horizontally flipped(p=0.5)의 augmentation만 사용하고 다른 augmentation 은 사용하지 않았다고 합니다. //&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure6.png&quot; width=&quot;678&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Mini-batch sampling)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;scale-invariance&quot;&gt;&lt;u&gt;Scale invariance&lt;/u&gt;&lt;br /&gt;&lt;/h4&gt;
&lt;p&gt;논문에서 Scal invariance 를 해결하기 위해 2가지 방법을 사용하였습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[brute force learning]&lt;/strong&gt;&lt;br /&gt;
train/test 에서 미리 정해진 픽셀 크기로 처리된다. 때문에, network는 train_data 로 부터 정해진 scale의 detection을 해야합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[using image pyramids]&lt;/strong&gt;&lt;br /&gt;
image pyramid를 통해 network는 scale invariance를 제공한다. Test 에서는 image pyramid는 각region_proposal을 nomalize하는데 사용됩니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 GPU memory issue로 인하여 소규모 네트워크에 대해서만 multi_scale train을 하였다고 합니다. 즉, multi_scale에 관한 실험은 별로 진행하지 않았다는 것이며 논문의 내용은 scale invariance에 더 초점을 맞추고 있습니다.
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;&lt;u&gt;Conclusion&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;저자가 말한대로 Fast-R-CNN은 R-CNN과 SPPnet과 다르게 깔끔함을 강조하고 있다.
Fast-R-CNN의 특징은 다음과 같다. &lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이미지당 CNN을 한번만 수행한다.&lt;/li&gt;
  &lt;li&gt;RoI pooling layer의 도입.(마지막 max-pooling 대신 사용되었다.)&lt;/li&gt;
  &lt;li&gt;Classification 에서 SVM을 사용하던 것 과 달리 Softmax로 변경되었다. 즉, 하나의 network가 된것이다.&lt;/li&gt;
  &lt;li&gt;Multitask loss 에서 보면 bounding Box regressor 또한 network의 부분이 되었다. &lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;이러한 특징을 바탕으로&lt;/strong&gt; , R-CNN/SPPnet과 달리 mAP가 높고, multi-task loss 를 이용하여 single-satge train이 가능하고, end-to-end학습이 가능하고 feature caching을 하지 않으므로 disk 공간을 필요로 하지 않는다는 장점이 있다.&lt;/p&gt;

&lt;p&gt;Fast-R-CNN의 다음 버전인, Faster-R-CNN은 region proposal 생성 방식의 개선을 주 논점으로 다룹니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Daehan Kim</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Fast-R-CNN Review</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">R-CNN</title>
      <link href="https://blog.airlab.re.kr/2019/10/R-CNN" rel="alternate" type="text/html" title="R-CNN" />
      <published>2019-10-01T00:00:00+09:00</published>
      <updated>2019-10-01T00:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/10/R-CNN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/10/R-CNN">&lt;p&gt;R-CNN Review&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. R-CNN을 시작으로 paper review post 를 작성하려고 합니다. ^_^&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;R-CNN&lt;/strong&gt;(Rich feature hierarchies for accurate object detection and semantic segmentation) (&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot;&gt;arXiv:1311.2524&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduciton&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduciton&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;논문 발표 이전&lt;strong&gt;SIFT&lt;/strong&gt;와 &lt;strong&gt;HOG&lt;/strong&gt; 에 상당한 기반을 두고 visual recognition task를 수행했다고 합니다. 그러나 낮은 perpomance 로인해 R-CNN을 고안 하였습니다. 
여기서 SIFT와 HOG를 간단하게 설명하면, 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;SIFT&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure1.jpg&quot; alt=&quot;figure1&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt; SIFT feature vector는 feature 주변의 영상패치를 4x4 블럭으로 나누고, (1) 각 블럭에 속한 pixel들의 gradient방향과 크기에 대한 histogram을 구하고 bin값으로 연결한 128차원 벡터 입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;크기, 형태, 방향에 강인하면서도 구분력이 뛰어나다.&lt;/li&gt;
  &lt;li&gt;기하학적 정보는 무시하고 feature 단위로 매칭을 수행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;HOG&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure2.jpg&quot; alt=&quot;figure2&quot; /&gt;&lt;br /&gt;
 HOG 는 대상 영역을 일정 셀로 분할한뒤, 각 셀마다 edge pixel들의 방향에 대한 histogram을 구한 뒤 이를 bin값으로 연결한 벡터 이다. (edge의 방향 histogram으로 본다.)&lt;br /&gt;
블록 단위로는 기하학적 정보를 유지하고, 그 내부는 histogram을 사용하여 local변화에 어느정도 강인하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;template matching과 histogram maching에 중간 단계의 매칭방법이다.&lt;/li&gt;
  &lt;li&gt;기하학적 정보와 local 한 정보를 모두(일정 양) 가진다.&lt;/li&gt;
  &lt;li&gt;figure2를 보면 알겠지만 edge의 방향정보를 사용하기 때문에 특이한(독특한) edge를 갖는 object를 식별하는데 적합한 영상 feature이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;SIFT/HOG&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SIFT&lt;/th&gt;
      &lt;th&gt;HOG&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;이용하는 정보&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;local information&lt;/td&gt;
      &lt;td&gt;edge information&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;형태변화가 심한 경우 detection&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;esay&lt;/td&gt;
      &lt;td&gt;difficult&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;example&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;액자속 그림(패턴)&lt;/td&gt;
      &lt;td&gt;자동차, 배드민턴 라켓&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;**&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;복잡한 image&lt;/td&gt;
      &lt;td&gt;단순하고 독특한 image&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;이 논문에서는 &lt;strong&gt;HOG 기반의 시스템 보다 CNN을 사용하여 높은 performance를 보여주기 위해 2가지 문제&lt;/strong&gt;에 초점을 맞춥니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;u&gt;[One]&lt;/u&gt; sliding-window 의 문제점을 파악하고 region proposal Algorithm을 사용했다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure4.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(sliding-window method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure5.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(region-proposal method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sliding window는 탐색해야하는 영역의 수가 이미지 전체이기 때문에 비효율적이고 입력 영상에 ‘물체가 있을거 같은’ 영역을 빠른 속도로 찾아내는 region proposal algorithm을 사용 하였다.&lt;br /&gt;
&lt;strong&gt;결과적으로 search space가 훨씬 줄어들기 때문에 빠르고 효율적 이다.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;[Two]&lt;/u&gt; large CNN을 training하기 위한 충분한 dataset이 없었고, ILSVRC DATA로 pre-training 된 model을 가져와서 fine-tune 하여 PASCAL DATA에 적용하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;object-detection-with-r-cnn&quot;&gt;&lt;strong&gt;&lt;u&gt;object detection with R-CNN&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;                   R-CNN은 3가지의 module 로 구성되어 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure3.png&quot; alt=&quot;figure3&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;region proposal을 얻어내는 module&lt;/li&gt;
  &lt;li&gt;CNN에 넣어 feture map을 얻어내는 module&lt;/li&gt;
  &lt;li&gt;CNN을 통해 추출된 feture map을 이용하여 Linear SVM을 사용하여 분류하는 module&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이때, CNN model은 Alexnet을 거의 그대로 가져와 사용하였다.&lt;br /&gt;
당연히 CNN은 고정 크기의 input을 입력으로 받기 때문에. region proposal을 CNN에 넣기 위해 crop &amp;amp; warp 를 한다.
논문에 나와있지만, 읽다보면 왜 R-CNN은 Classifier로 Softmax를 쓰지 않고 SVM을 사용했는지에 대한 의문이 들 수 있다. 근데 이는 수식적에 근간을 두고 사용한 것이 아닌 실험적인 결과로 Softmax를 사용했을때 mAP가 낮아졌기 때문에 SVM을 사용한 것 이다.(54.2% -&amp;gt; 50.9%)&lt;/p&gt;

&lt;p&gt;SVM을 간단하게 설명하면 CNN으로 부터 추출된 featuer vector들을 class별로 점수를 주고 object인지 아닌지 object라면 어떤 object 인지 판별하는 classifier이다.&lt;/p&gt;

&lt;p&gt;figure를 보면 Bbox reg 라는 부분이 있는데 bounding Box Regression인데 이는 필수로 사용해야 하는 부분은 아니라고 한다. 그러나 사용한 mAP가 더 좋기 때문에 사용하는 것을 권장한다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure6.png&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Bounding-box method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
ground-truth(G) 와 Predicted-Box(P) 가 있을때, P를 G로 mapping 해주는 것이다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure7.png&quot; width=&quot;600&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Result)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
결과적으로, 다음과 같은 output을 얻을 수 있다.&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;논문을-읽으면서-여러자료들을-참고하여-r-cnn의-단점을-이해해-보았다&quot;&gt;&lt;u&gt;논문을 읽으면서 여러자료들을 참고하여 R-CNN의 단점을 이해해 보았다.&lt;/u&gt;&lt;/h4&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure8.png&quot; width=&quot;900&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Table)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
위와 같이 VOC2010 test에서 좋은 성능을 보였지만 단점은 존재한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;region proposal된 약 2K의 image를 모두 crop하고 warp하는 과정에서 cpu를 통해 이뤄지기 때문에, 매우 비효율 적이라는 점을 이해하였다.&lt;br /&gt;
&lt;strong&gt;직접 경험해본 일화&lt;/strong&gt;중 하나는, 1980x1080 를 224x224 image preprocessing 하는 과정이 오래걸려서 GPU가 놀고 있는 시간이 많았던 적이 있다. (224x224 image를 따로 저장하여 문제 해결.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3가지의 module을 따로 학습해야하기 때문에 비효율적이다.
&lt;strong&gt;때문에 Fast_R-CNN에서 Multi-task loss&lt;/strong&gt;를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;end-to-end 학습이 불가능 하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Daehan Kim</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">R-CNN Review</summary>
      

      
      
    </entry>
  
</feed>
