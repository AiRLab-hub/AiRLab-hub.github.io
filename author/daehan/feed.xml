<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/daehan/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-11-16T22:27:16+09:00</updated>
  <id>https://blog.airlab.re.kr/author/daehan/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Fast-R-CNN</title>
      <link href="https://blog.airlab.re.kr/2019/10/Fast-R-CNN" rel="alternate" type="text/html" title="Fast-R-CNN" />
      <published>2019-10-04T00:00:00+09:00</published>
      <updated>2019-10-04T00:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/10/Fast%20R-CNN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/10/Fast-R-CNN">&lt;p&gt;Fast-R-CNN Review&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. 지난번 R-CNN 에 이어서 Fast-R-CNN 을 Review 해보려고 합니다.. ^_^&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;Fast-R-CNN&lt;/strong&gt;(&lt;a href=&quot;https://https://arxiv.org/abs/1504.08083&quot;&gt;arXiv:1504.08083&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduciton&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduciton&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;당시 deep ConvNet[14,16]이 발전하면서 image classification과 object detection의 Acc가 상당히 향상 되었다고 합니다. &lt;br /&gt;
당연히 classification보다 object detection이 복잡합니다. 때문에, [9,11,19,25]는 느리고 비효율적인 multi-stage pipline model을 train합니다.&lt;br /&gt;
[14,16]은 Alex_net과 Backpropagation입니다.&lt;br /&gt;
[9,11,19,25]는 R-CNN,SPP,Overfeat,segDeepM입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Multi-stage_pipline&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;
R-CNN에서 3개의 module을 따로 학습 해야하는 것을 생각하면 될 것 같습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;논문에서는&lt;/strong&gt; detection task가 object의 정밀한 localization을 필요로 하기 때문에 &lt;u&gt;두가지 issue&lt;/u&gt;가 생긴다고 합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[Issue1]&lt;/strong&gt; : 수많은 객체 위치 후보들을 처리해야한다.&lt;br /&gt;
이는 Speed,accuracy,simplicification을 손상시킵니다.&lt;br /&gt;
&lt;strong&gt;[Issue2]&lt;/strong&gt; : 정확한 localization을 위해 rough_localization을 다듬어야한다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;때문에 논문에서는&lt;/strong&gt; [9,11]Network를 base로 하여 train과정을 단순화 합니다. (Sigle-stage traing algorthm을 제안합니다.) 결과적으로, R-CNN,SPP_net보다 VGGnet을 각각 9배,3배 빠르게 학습합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;R-CNN의 단점&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Multi-stage pipline(train)(Region proposal,SVM,bounding-box)&lt;/li&gt;
  &lt;li&gt;SVM과 bounding-box regressor의 경우(train) 각 image의 proposal의 feature들이 disk에 기록됩니다. (VGG16의 경우 2.5일이 소요됩니다._VOC07)&lt;/li&gt;
  &lt;li&gt;object detection이 느리다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;논문에서는&lt;/strong&gt; Fast-R-CNN을 설명하기전, R-CNN과 SPPnet을 비교합니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPPnet은 sharing compute를 이용하여 R-CNN의 속도를 높이기 위해 제안되었다.
    &lt;ul&gt;
      &lt;li&gt;여기서 말하는 sharing compute는 R-CNN과 달리 SPPnet은 한번의 convnet을 통과한 feature를 이용해 detection을 함으로써, end-to-end 학습이 된다는 이야기를 하는 것 입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결과적으로, test:10~100배, train:3배 빠릅니다. &lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;그러나&lt;/strong&gt;, SPPnet도 주목할만한 &lt;strong&gt;단점&lt;/strong&gt;이 있습니다. (drawback)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SPPnet도 R-CNN과 마찬가지로, train은 multi-stage pipline(feature extraction/fine-tune(log_loss)/SVM train, bounding-box regressors 를 가집니다.) 또, feature가 disk에 기록됩니다.&lt;/li&gt;
  &lt;li&gt;SPPnet에서 제안된 algorithm은 convolutional layer를 update할수 없고 convolution layer 전에 SPP를 할 수 없다.(network perpomance를 제한한다.)&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Contributions&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure4.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Fast-R-CNN)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;image의 region_proposal을 crop&amp;amp;warp한 R-CNN과 달리 Fast-R-CNN은 입력이미지와 RoI_projection(object proposal)을 입력으로 받는다.&lt;/li&gt;
  &lt;li&gt;network를 통해 convfeature map을 생성하기위해 전체 이미지를 처리한다.&lt;/li&gt;
  &lt;li&gt;각 object proposal에 대해 RoIPooling layer가 고정길이 vector를 추출한다. (각 feature vector는 fc_layer에 연속적으로 전달된다.)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Multi-task loss&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure10.png&quot; width=&quot;1000&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Multi-task loss)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;RoI pooling layer&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;
입력이미지에 한번만 CNN을 적용하고 RoI pooling을 이용하여 object 판별을 위한 feature를 추출하자는 것이 핵심이다.&lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RoI로 추출할 feature size = HxW(eg.7x7)&lt;/li&gt;
  &lt;li&gt;Feature map 위에 RoI의 좌표 (r,c,h,w)&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure7.gif&quot; width=&quot;678&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(RoI pooling layer)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
Feature map위에서 h/H x w/W 만큼 Grid를 만들어 pooling 하면 결과적으로 원하는 HxW (첫 fc-layer와 호환이 되도록 하는 사이즈)feature size가 됩니다.&lt;br /&gt;
(서로 다른 size의 region proposal이 입력되더라도 같은 size의 stride로 7x7을 만들어준다.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Initializing from pre-trained networks&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 pre-train된 ImageNet networks를 각각 실험합니다. pre-train network가 Fast-R-CNN을 initialize할때 3가지의 transform을 거칩니다.&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;마지막 maxpooling layer를 RoI pooling layer로 대체 한다.&lt;/li&gt;
  &lt;li&gt;network_last fully connected layer와 softmax layer는  two sibling layer로 대체됩니다.&lt;/li&gt;
  &lt;li&gt;network는 two input 을 받도록 합니다. (image list와 해당 image의 RoI 목록)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Mini-batch sampling&lt;/u&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;fine-tuning에서, N=2(image),R=128 로 하여 R/N으로 image당 64RoI를 뽑습니다. CNN연산량을 줄이는 효과가 있습니다.       다음 그림과 같은 방식을 hierarchical sampling이라고 합니다.(계층적 샘플링)
Ground-truth와 IoU &amp;gt; 0.5 (Positive), [0.1,0.5] 일 경우 Negative로 구분합니다. (즉, IoU가 너무 낮은것은 sample로 추가하지 않는다.) 
//train 할때, image horizontally flipped(p=0.5)의 augmentation만 사용하고 다른 augmentation 은 사용하지 않았다고 합니다. //&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-04-Fast-R-CNN/figure6.png&quot; width=&quot;678&quot; hight=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Mini-batch sampling)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;scale-invariance&quot;&gt;&lt;u&gt;Scale invariance&lt;/u&gt;&lt;br /&gt;&lt;/h4&gt;
&lt;p&gt;논문에서 Scal invariance 를 해결하기 위해 2가지 방법을 사용하였습니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[brute force learning]&lt;/strong&gt;&lt;br /&gt;
train/test 에서 미리 정해진 픽셀 크기로 처리된다. 때문에, network는 train_data 로 부터 정해진 scale의 detection을 해야합니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;[using image pyramids]&lt;/strong&gt;&lt;br /&gt;
image pyramid를 통해 network는 scale invariance를 제공한다. Test 에서는 image pyramid는 각region_proposal을 nomalize하는데 사용됩니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;논문에서는 GPU memory issue로 인하여 소규모 네트워크에 대해서만 multi_scale train을 하였다고 합니다. 즉, multi_scale에 관한 실험은 별로 진행하지 않았다는 것이며 논문의 내용은 scale invariance에 더 초점을 맞추고 있습니다.
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;&lt;u&gt;Conclusion&lt;/u&gt;&lt;/h4&gt;
&lt;p&gt;저자가 말한대로 Fast-R-CNN은 R-CNN과 SPPnet과 다르게 깔끔함을 강조하고 있다.
Fast-R-CNN의 특징은 다음과 같다. &lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이미지당 CNN을 한번만 수행한다.&lt;/li&gt;
  &lt;li&gt;RoI pooling layer의 도입.(마지막 max-pooling 대신 사용되었다.)&lt;/li&gt;
  &lt;li&gt;Classification 에서 SVM을 사용하던 것 과 달리 Softmax로 변경되었다. 즉, 하나의 network가 된것이다.&lt;/li&gt;
  &lt;li&gt;Multitask loss 에서 보면 bounding Box regressor 또한 network의 부분이 되었다. &lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;이러한 특징을 바탕으로&lt;/strong&gt; , R-CNN/SPPnet과 달리 mAP가 높고, multi-task loss 를 이용하여 single-satge train이 가능하고, end-to-end학습이 가능하고 feature caching을 하지 않으므로 disk 공간을 필요로 하지 않는다는 장점이 있다.&lt;/p&gt;

&lt;p&gt;Fast-R-CNN의 다음 버전인, Faster-R-CNN은 region proposal 생성 방식의 개선을 주 논점으로 다룹니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Daehan Kim</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Fast-R-CNN Review</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">R-CNN</title>
      <link href="https://blog.airlab.re.kr/2019/10/R-CNN" rel="alternate" type="text/html" title="R-CNN" />
      <published>2019-10-01T00:00:00+09:00</published>
      <updated>2019-10-01T00:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/10/R-CNN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/10/R-CNN">&lt;p&gt;R-CNN Review&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 김대한 입니다. R-CNN을 시작으로 paper review post 를 작성하려고 합니다. ^_^&lt;/p&gt;

&lt;p&gt;이번에 읽은 논문은 &lt;strong&gt;R-CNN&lt;/strong&gt;(Rich feature hierarchies for accurate object detection and semantic segmentation) (&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot;&gt;arXiv:1311.2524&lt;/a&gt;)입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduciton&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduciton&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;논문 발표 이전&lt;strong&gt;SIFT&lt;/strong&gt;와 &lt;strong&gt;HOG&lt;/strong&gt; 에 상당한 기반을 두고 visual recognition task를 수행했다고 합니다. 그러나 낮은 perpomance 로인해 R-CNN을 고안 하였습니다. 
여기서 SIFT와 HOG를 간단하게 설명하면, 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;SIFT&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure1.jpg&quot; alt=&quot;figure1&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt; SIFT feature vector는 feature 주변의 영상패치를 4x4 블럭으로 나누고, (1) 각 블럭에 속한 pixel들의 gradient방향과 크기에 대한 histogram을 구하고 bin값으로 연결한 128차원 벡터 입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;크기, 형태, 방향에 강인하면서도 구분력이 뛰어나다.&lt;/li&gt;
  &lt;li&gt;기하학적 정보는 무시하고 feature 단위로 매칭을 수행한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;HOG&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure2.jpg&quot; alt=&quot;figure2&quot; /&gt;&lt;br /&gt;
 HOG 는 대상 영역을 일정 셀로 분할한뒤, 각 셀마다 edge pixel들의 방향에 대한 histogram을 구한 뒤 이를 bin값으로 연결한 벡터 이다. (edge의 방향 histogram으로 본다.)&lt;br /&gt;
블록 단위로는 기하학적 정보를 유지하고, 그 내부는 histogram을 사용하여 local변화에 어느정도 강인하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;template matching과 histogram maching에 중간 단계의 매칭방법이다.&lt;/li&gt;
  &lt;li&gt;기하학적 정보와 local 한 정보를 모두(일정 양) 가진다.&lt;/li&gt;
  &lt;li&gt;figure2를 보면 알겠지만 edge의 방향정보를 사용하기 때문에 특이한(독특한) edge를 갖는 object를 식별하는데 적합한 영상 feature이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;SIFT/HOG&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;SIFT&lt;/th&gt;
      &lt;th&gt;HOG&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;이용하는 정보&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;local information&lt;/td&gt;
      &lt;td&gt;edge information&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;형태변화가 심한 경우 detection&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;esay&lt;/td&gt;
      &lt;td&gt;difficult&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;example&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;액자속 그림(패턴)&lt;/td&gt;
      &lt;td&gt;자동차, 배드민턴 라켓&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;**&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;복잡한 image&lt;/td&gt;
      &lt;td&gt;단순하고 독특한 image&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;이 논문에서는 &lt;strong&gt;HOG 기반의 시스템 보다 CNN을 사용하여 높은 performance를 보여주기 위해 2가지 문제&lt;/strong&gt;에 초점을 맞춥니다.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;u&gt;[One]&lt;/u&gt; sliding-window 의 문제점을 파악하고 region proposal Algorithm을 사용했다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure4.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(sliding-window method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure5.png&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(region-proposal method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sliding window는 탐색해야하는 영역의 수가 이미지 전체이기 때문에 비효율적이고 입력 영상에 ‘물체가 있을거 같은’ 영역을 빠른 속도로 찾아내는 region proposal algorithm을 사용 하였다.&lt;br /&gt;
&lt;strong&gt;결과적으로 search space가 훨씬 줄어들기 때문에 빠르고 효율적 이다.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;u&gt;[Two]&lt;/u&gt; large CNN을 training하기 위한 충분한 dataset이 없었고, ILSVRC DATA로 pre-training 된 model을 가져와서 fine-tune 하여 PASCAL DATA에 적용하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;object-detection-with-r-cnn&quot;&gt;&lt;strong&gt;&lt;u&gt;object detection with R-CNN&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;                   R-CNN은 3가지의 module 로 구성되어 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure3.png&quot; alt=&quot;figure3&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;region proposal을 얻어내는 module&lt;/li&gt;
  &lt;li&gt;CNN에 넣어 feture map을 얻어내는 module&lt;/li&gt;
  &lt;li&gt;CNN을 통해 추출된 feture map을 이용하여 Linear SVM을 사용하여 분류하는 module&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이때, CNN model은 Alexnet을 거의 그대로 가져와 사용하였다.&lt;br /&gt;
당연히 CNN은 고정 크기의 input을 입력으로 받기 때문에. region proposal을 CNN에 넣기 위해 crop &amp;amp; warp 를 한다.
논문에 나와있지만, 읽다보면 왜 R-CNN은 Classifier로 Softmax를 쓰지 않고 SVM을 사용했는지에 대한 의문이 들 수 있다. 근데 이는 수식적에 근간을 두고 사용한 것이 아닌 실험적인 결과로 Softmax를 사용했을때 mAP가 낮아졌기 때문에 SVM을 사용한 것 이다.(54.2% -&amp;gt; 50.9%)&lt;/p&gt;

&lt;p&gt;SVM을 간단하게 설명하면 CNN으로 부터 추출된 featuer vector들을 class별로 점수를 주고 object인지 아닌지 object라면 어떤 object 인지 판별하는 classifier이다.&lt;/p&gt;

&lt;p&gt;figure를 보면 Bbox reg 라는 부분이 있는데 bounding Box Regression인데 이는 필수로 사용해야 하는 부분은 아니라고 한다. 그러나 사용한 mAP가 더 좋기 때문에 사용하는 것을 권장한다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure6.png&quot; width=&quot;400&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Bounding-box method)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
ground-truth(G) 와 Predicted-Box(P) 가 있을때, P를 G로 mapping 해주는 것이다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure7.png&quot; width=&quot;600&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Result)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
결과적으로, 다음과 같은 output을 얻을 수 있다.&lt;/p&gt;
&lt;hr /&gt;

&lt;h4 id=&quot;논문을-읽으면서-여러자료들을-참고하여-r-cnn의-단점을-이해해-보았다&quot;&gt;&lt;u&gt;논문을 읽으면서 여러자료들을 참고하여 R-CNN의 단점을 이해해 보았다.&lt;/u&gt;&lt;/h4&gt;
&lt;center&gt;&lt;img src=&quot;/assets/images/posts/2019-10-01-R-CNN/figure8.png&quot; width=&quot;900&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;
&lt;center&gt;(Table)&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
위와 같이 VOC2010 test에서 좋은 성능을 보였지만 단점은 존재한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;region proposal된 약 2K의 image를 모두 crop하고 warp하는 과정에서 cpu를 통해 이뤄지기 때문에, 매우 비효율 적이라는 점을 이해하였다.&lt;br /&gt;
&lt;strong&gt;직접 경험해본 일화&lt;/strong&gt;중 하나는, 1980x1080 를 224x224 image preprocessing 하는 과정이 오래걸려서 GPU가 놀고 있는 시간이 많았던 적이 있다. (224x224 image를 따로 저장하여 문제 해결.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;3가지의 module을 따로 학습해야하기 때문에 비효율적이다.
&lt;strong&gt;때문에 Fast_R-CNN에서 Multi-task loss&lt;/strong&gt;를 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;end-to-end 학습이 불가능 하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Daehan Kim</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">R-CNN Review</summary>
      

      
      
    </entry>
  
</feed>
