<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/jaemin/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-11-27T18:51:22+09:00</updated>
  <id>https://blog.airlab.re.kr/author/jaemin/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</title>
      <link href="https://blog.airlab.re.kr/2019/11/gcnet" rel="alternate" type="text/html" title="GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond" />
      <published>2019-11-16T11:00:00+09:00</published>
      <updated>2019-11-16T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/gcnet</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/gcnet">&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!
오늘 소개할 논문은 GCNet으로 (&lt;a href=&quot;https://arxiv.org/abs/1904.11492&quot;&gt;arXiv:1904.11492&lt;/a&gt;)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.&lt;/p&gt;

&lt;p&gt;이전에 소개시켜드렸던 &lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;Non-local&lt;/a&gt;을 시각화하고, 문제점을 지적하는 논문으로 의미가 크다고 생각합니다.&lt;/p&gt;

&lt;h2 id=&quot;non-local&quot;&gt;Non-local&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/non-local.png&quot; width=&quot;60%&quot; alt=&quot;non-local&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Non-local Block은 Global Conext를 query-specific하게 접근하는 방식으로 많이 알려져 왔습니다.&lt;/p&gt;

&lt;p&gt;위 그림의 NL 블록을 보더라도 Shape이  HW x HW 로 각 포인트에 대하여 전체 포인트에 대한 Score Map을 만드는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이번 GCNet에서는 이러한 필터를 시각화해보았는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure2.png&quot; width=&quot;60%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 결과를 보면 각 Point(query)에 대한 NL의 시각화 맵을 볼 수 있는데, 여기서 재밌는 것은 서로 다른 Point에 대하여 모든 결과들이 거의 유사한 heatmap을 보인다는 것입니다. 이는 정량적으로도 확인을 할 수 있는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure3.png&quot; width=&quot;60%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 Method는 여러 Non-local의 종류이고, E-Gaussian(Embedding Gaussian)이 Original NL 입니다.&lt;/p&gt;

&lt;p&gt;여기서의 Cosine distance를 보더라고, 서로 다른 InPut에 대하여 큰 차이가 없는 Output이 나오는 것을 정량적으로도 확인을 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;simplifying-the-non-local-block-snl&quot;&gt;Simplifying the Non-local Block (SNL)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure7.png&quot; width=&quot;50%&quot; alt=&quot;figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과처럼 모든 point에서 유사한 heatmap이 나온다고 하면, 충분히 NL의 크기를 줄일 수 있을 것이라고 생각이 드실 겁니다. 논문에서도 이러한 부분을 지적하며 경량화된 Simplifying Non-local Block을 제시하는데요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure4.png&quot; width=&quot;50%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식은 기존의 NL로 각 point i에 대하여, 모든 point j가 연산되는 i, j가 서로 연관적인 것으로 표현이 되고 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 위의 결과처럼 모든 포인트에 대하여, 비슷한 결과가 나오고 있으니 i와 j는 독립적(query independent)으로 아래와 같이 변경해 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure5.png&quot; width=&quot;50%&quot; alt=&quot;figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 조금 더 연산량을 줄이기 위해 W(v)를 밖으로 뺌으로서 아래의 SNL이 완셩이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure6.png&quot; width=&quot;50%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 Mask RCNN에서의 결과를 보게 되면 NL과 SNL의 성능이 크게 차이가 나지 않음을(아니 조금더 좋음을?)볼 수 있습니다. 또한 연산량과, Prams를 보더라도 SNL이 훨씬 효율적이여 보입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure8.png&quot; width=&quot;50%&quot; alt=&quot;figure7&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;global-context-block&quot;&gt;Global Context Block&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure9.png&quot; width=&quot;80%&quot; alt=&quot;figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진처럼 Global Context 모듈들은 Context modeling과 Transform 모델로 분리되어 표현이 될 수 있습니다. 여기서 GCNet은 SNL의 context modeling block 과 SE block의 Transform block이 합쳐진 형태로 구성이 되었습니다. 때문에 SE의 Transform에서 Conv를 2번 나누어 계산 하는 효과인 연산량을 더 낮추는 효과도 가지게 되었습니다.(SNL Transform 연산량: C&lt;em&gt;C, GCNet 연산량: C&lt;/em&gt;C/r + C/r*C)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure10.png&quot; width=&quot;50%&quot; alt=&quot;figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이를 위해 하이퍼 파라미터(r) 하나가 추가 되었는데요, 논문에서는 r을 4로 설정한 것이 가장 높은 성능을 보인다고 보여주고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure11.png&quot; width=&quot;50%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추가적으로 Layer Norm을 추가하여, Normalize 및 Regularize의 효과를 보았다고 합니다.&lt;/p&gt;

&lt;p&gt;따라서, 최종적으로 GCNet은 아래 수식 하나로 표현이 되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure11&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-gcnet/figure12.png&quot; width=&quot;50%&quot; alt=&quot;figure12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문의 실험은 Mask RCNN과 Classification, Action Recognition에서 실험되었고, 모든 보틀넥에서 GCBlock을 사용한 것이 가장 좋은 성능을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;마치며&quot;&gt;마치며..&lt;/h2&gt;

&lt;p&gt;이번 논문은 NL의 문제점을 지적하고, NL을 시각화 하는 것만으로도 충분하게 의미기 큰 논문이었던 것 같습니다. 또한 NL의 파라미터와 연산량을 줄여 기존에 한개 혹은 두개만 사용가능 했던 것을 모든 보틀넥에 삽입하여 성능을 크게 키운 것도 인상적입니다.&lt;/p&gt;

&lt;p&gt;하지만, all GCNet처럼, all SNL도 실험 결과가 있었으면 하는 아쉬움도 남습니다.&lt;/p&gt;

&lt;p&gt;이번에 ICCV2019를 다녀와서 다음번에 ICCV에 소개되었던 재밌는 논문들을 더 소개시켜드리도록 하겠습니다. 감사합니다 :)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cao, Yue, et al. “GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond.” arXiv preprint arXiv:1904.11492 (2019).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://j911.me/2019/11/nonlocal.html&quot;&gt;https://j911.me/2019/11/nonlocal.html&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요. AiRLab(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다! 오늘 소개할 논문은 GCNet으로 (arXiv:1904.11492)으로 이번 ICCV 2019 워크숍에서 소개가 되었던 논문입니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Non-local Neural Networks</title>
      <link href="https://blog.airlab.re.kr/2019/11/nonlocal" rel="alternate" type="text/html" title="Non-local Neural Networks" />
      <published>2019-11-03T11:00:00+09:00</published>
      <updated>2019-11-03T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/nonlocal</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/nonlocal">&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!
오늘 소개할 논문은 Non-local Neural Networks (&lt;a href=&quot;https://arxiv.org/abs/1711.07971&quot;&gt;arXiv:1711.07971&lt;/a&gt;)이며, CVPR2018에서 소개된 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;기존의 Convolution Neural Networks들은 3x3, 5x5과 같은 작은 Filter를 거치며 국소적인 피쳐의 특성을 추출해 낼 수 있기 때문에, Pooling과 함께 사용하며 많은 SOTA Architecture들을 만들어냈습니다.&lt;/p&gt;

&lt;p&gt;하지만, 이러한 CNN 구조는 피쳐의 특수한 특성을 추출 하는 것에 대하여는 훌륭한 퍼포먼스를 보이지만, 영상의 전체의 Context를 보는 것에는 한계를 보이고 있습니다. 이번에 소개시켜드리는 Non-local Neural Networks는 기존의 Non-local Mean Filter에서 착안을 하여 네트워크가 입력의 전체적인 Context역시 학습할 수 있도록 도와줍니다.&lt;/p&gt;

&lt;h3 id=&quot;non-local-mean-filter&quot;&gt;Non-local Mean Filter&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/non-local-filter.png&quot; width=&quot;300px&quot; alt=&quot;figure1&quot; /&gt;
Non-local Mean Filter는 기존의 영상처리에서 사용되던 denoising 알고리즘으로, 위 사진과 같이 특정 구역에 대하여, 모든 영역에 대하여 유사한 픽셀들을 찾아 평균을 처리하는 방식으로 denoising합니다.&lt;/p&gt;

&lt;h3 id=&quot;non-local-networks&quot;&gt;Non-local Networks&lt;/h3&gt;
&lt;p&gt;Non-local Networks는 Non-local Block으로 모듈화 되어 CNN 구조 사이에 삽입되어 사용될 수 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/non-local.png&quot; width=&quot;60%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 다이어그램이 Non-local Block의 다이어그램입니다. 입력된 Input에 대하여 1x1 Conv를 통하여 theta, phi, g를 만들어 낸 뒤 phi의 aixs를 교차해주어 theta와 연산 후 Softmax를 거쳐 모든 맵에 대한 Score Map를 만들어줍니다. 이 맵은 어텐션맵과 유사한 느낌으로 이해가 됩니다. 이 맵과 g를 연산하여 입력에 대하여 어텐션이 적용된 맵을 만들어준뒤 1x1 Conv를 거쳐 기존의 입력과 더하여 출력을하는 구조입니다. 따라서 Non-local 블록은 local한 정보가 아닌 Non-local한 영역까지 학습이 가능한 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;application&quot;&gt;Application&lt;/h3&gt;
&lt;p&gt;Non-local은 General하게 여러 도메인에서 사용될 수 있습니다. 유명한 네트워크들은 Action Recognition에서 I3D + ResNext101 구조에 Non-local Block를 붙인 구조와 Segmentation에 Non-local Block를 붙인 구조들이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I3D + Non-local&lt;/strong&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/nonlocal-i3d.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과를 보시면, Optical Flow를 fusion한 다른 Architecture보다 NL을 붙이고 RGB영상만 사용한 구조가 더 높은 성능을 보이고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CCNet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/ccnet.png&quot; width=&quot;50%&quot; alt=&quot;figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Segmentation Task에서도 NL은 좋은 성능을 보입니다. CCNet은 이번 ICCV2019에 소개된 네트워크인데, NL이 연산량이 많은 이슈를 Criss Cross 방식을 통해 연산량은 줄이면서 NL의 효과를 그대로 가져가는 구조를 소개하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-03-nonlocal/CCNet-result.png&quot; width=&quot;60%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과를 보면 NL 혹은 RCCA(Recurrent Criss-Cross Attention)를 붙인 네트워크가 CityScapes와 ADE20K 에서 SOTA를 달성한 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;CCNet은 다음번에 자세히 리뷰해 보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;simple-implementation-of-nl&quot;&gt;Simple implementation of NL&lt;/h3&gt;
&lt;p&gt;간단한 NL(2D)를 구현해본 코드를 공유합니다.
&lt;a href=&quot;https://github.com/J911/DeepLabV3-NonLocal/blob/master/models/NonLocalBlock.py&quot;&gt;https://github.com/J911/DeepLabV3-NonLocal/blob/master/models/NonLocalBlock.py&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Wang, Xiaolong, et al. “Non-local neural networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.&lt;/li&gt;
  &lt;li&gt;Huang, Zilong, et al. “Ccnet: Criss-cross attention for semantic segmentation.” arXiv preprint arXiv:1811.11721 (2018).&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요. AiRLab(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다! 오늘 소개할 논문은 Non-local Neural Networks (arXiv:1711.07971)이며, CVPR2018에서 소개된 논문입니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mean teachers are better role models</title>
      <link href="https://blog.airlab.re.kr/2019/09/mean-teacher" rel="alternate" type="text/html" title="Mean teachers are better role models" />
      <published>2019-09-19T11:00:00+09:00</published>
      <updated>2019-09-19T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/09/mean-teacher</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/09/mean-teacher">&lt;p&gt;Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!&lt;/p&gt;

&lt;p&gt;오늘 소개할 논문은 Mean teachers are better role models (&lt;a href=&quot;https://arxiv.org/abs/1703.01780&quot;&gt;arXiv:1703.01780&lt;/a&gt;)이며, NIPS 2017에서 소개된 논문입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Semi-Supervised Leaning에 관련된 논문이고, Π Model과 Temporal ensemble 이후에 소개되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure1.png&quot; width=&quot;80%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;π-model&quot;&gt;Π Model&lt;/h4&gt;
&lt;p&gt;Π Model은 위 그림의 위 쪽과 같은 구조와 같이, 라벨이 주어진 경우 Closs Entropy를 사용하여 학습을 하고, 라벨이 주어지지 않은 경우는 augmented input의 결과와 이전과 다른 dropout 과 augmention을 사용한 결과의 squared difference를 이용해 학습을 합니다.&lt;/p&gt;

&lt;h4 id=&quot;temporal-ensemble&quot;&gt;Temporal ensemble&lt;/h4&gt;
&lt;p&gt;Temporal ensemble과 Π Model의 차이점은 라벨이 주어지지 않았을 때 Temporal ensemble은 이전의 결과에 대한 값들을 앙상블한 결과를 가지고 학습을 하는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;mean-teacher&quot;&gt;Mean Teacher&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/cover.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;
위에서 소개한 두개의 모델은 모델을 공유하여 학습이 되지만, Mean Teacher의 경우 Student와 Teacher 모델로 나뉘어 학습이 진행됩니다. Teacher 모델은 Student 모델의 Weight를 공유하고, 대신에 Student모델의 아래의 방식과 같이 EMS Weight를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure2.png&quot; width=&quot;30%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 후 라벨이 주어지지 않은 경우 기존 방식과 유사하게 Teacher 모델과 Student 모델의 consistency cost (J)를 계산하여 학습이 진행되게 됩니다.
&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure3.png&quot; width=&quot;40%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 방법은 Temporal ensemble에 비하여 2가지 이점을 가지는데, 첫 번째는 Student와 Teacher 사이의 빠른 Feedback이 가능하여 더 높은 ACC를 가지게 되는 것이고, 두 번째는 large scale datasets을 online으로 학습할 수 있는 것 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/table1.png&quot; width=&quot;80%&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과와 같이 SVHM과 CIFAR10 에서의 Mean Teacher를 사용한 네트워크가 Π Model과 Temporal ensemble보다 더 높은 퍼포먼스를 보임을 확인할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tarvainen, Antti, and Harri Valpola. “Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.” Advances in neural information processing systems. 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rasmus, Antti, et al. “Semi-supervised learning with ladder networks.” Advances in neural information processing systems. 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Laine, Samuli, and Timo Aila. “Temporal ensembling for semi-supervised learning.” arXiv preprint arXiv:1610.02242 (2016).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Relational Knowledge Distillation</title>
      <link href="https://blog.airlab.re.kr/2019/07/rkd" rel="alternate" type="text/html" title="Relational Knowledge Distillation" />
      <published>2019-07-28T22:00:00+09:00</published>
      <updated>2019-07-28T22:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/rkd</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/rkd">&lt;p&gt;Relational Knowledge Distillation&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!&lt;/p&gt;

&lt;p&gt;오늘 소개할 논문은 Relational Knowledge Distillation(RKD) (&lt;a href=&quot;https://arxiv.org/abs/1904.05068&quot;&gt;arXiv:1904.05068&lt;/a&gt;)이며, CVPR2019에 Accepted 된 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;knowledge-distillation&quot;&gt;Knowledge distillation&lt;/h3&gt;

&lt;p&gt;Knowledge distillation은 Teacher Model이 Student Model에게 Knowledge를 transferring 하는 것 인데요, 이러한 연구의 대표적인 연구는 Hinton의 &lt;code class=&quot;highlighter-rouge&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/code&gt;가 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 논문의 시작은 computing resources의 양을 줄이이 위해 시작이 되었는데요,&lt;/p&gt;

&lt;p&gt;만약 모델의 ACC를 높이기 위해서는 모델을 크게 설계를 하면 되지만 모델이 커질 수록 파라미터의 양은 증가하고 더 많은 computing resource를 차지할 것입니다. 이러한 문제는 computing resource를 감당할 수 있는 인프라를 가 없는 사람들에게는 큰 이슈로 남게되고, 특히 서비스로 배포가 되어야하는 모델들은 경량화 되어야 한다는 조건이 많이 붙습니다.&lt;/p&gt;

&lt;p&gt;이러한 이슈로 큰 모델이 학습한 정보를 &lt;code class=&quot;highlighter-rouge&quot;&gt;증류(distillation)&lt;/code&gt;하여, 작은 모델에 주입할 수 있는 Knowledge distillation가 시작되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure2.png&quot; width=&quot;80%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 Softmax를 사용하는 네트워크가 있다고 해봅시다. 그러면 one hot으로 작성된 정답 라벨(hard label)과 predict을 비교하여 backward를 하게 됩니다.&lt;/p&gt;

&lt;p&gt;Teacher Model은 위와 같은 프로세스로 학습을 진행하고 큰 모델이기 때문에 좋을 ACC를 만들어 낼 것입니다. 그리고 Student Model은 Teacher Model의 softmax를 거치고 나온 0과 1사이의 확률 값을 soft label로 하여 학습하는 방법이 KD(Knowledge Distillation)입니다.&lt;/p&gt;

&lt;h3 id=&quot;relational-knowledge-distillation&quot;&gt;Relational Knowledge Distillation&lt;/h3&gt;

&lt;p&gt;RKD(Relational Knowledge Distillation)는 Knowledge Distillation에서 Knowledge를 어떻게 정의할 것 인가에서 시작이 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure1.png&quot; width=&quot;80%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;를 각 Input에서 나오는 Teacher의 output, &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt;를 Student 의 output이라고 했을 때 기존의 KD는 각각의 t가 s에 point-to-point로 연결되는 위 그림과 매칭됩니다. 때문에 이 논문에서는 기존의 KD를 IKD(Individual Knowledge Distillation)라고 표현합니다.&lt;/p&gt;

&lt;p&gt;IKD는 Individual한 output을 Knowledge로 하여 Distillation 한다고 하면, RKD는 output들의 관계(Relation)를 Knowledge로 하여 Distillation합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure3.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 연구에서 이야기 하는 관계는 각 output이 embedding space에 표현될 때의 node간의 distance와 angle입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure4.png&quot; width=&quot;60%&quot; alt=&quot;figure4&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure5.png&quot; width=&quot;60%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;distance는 위의 식으로 표현 될 수 있고 뮤는 node간 거리의 평균이라고 이해하면 될 것 같습니다. 따라서 아래와 같은 식을 otim하는 것으로 학습 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure6.png&quot; width=&quot;60%&quot; alt=&quot;figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;angle도 아래와 같이 표현이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure7.png&quot; width=&quot;60%&quot; alt=&quot;figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure8.png&quot; width=&quot;60%&quot; alt=&quot;figure8&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;p&gt;이 논문에서는 여러 Task(Metric learning, Image classification, Few-shot learning) 에서 위에서 소개한 distance-wise, angle-wise 를 통해 학습한 결과를 다른 네트워크와 비교하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metric learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure9.png&quot; width=&quot;100%&quot; alt=&quot;figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image classification&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure10.png&quot; width=&quot;60%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Few-shot learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure10.png&quot; width=&quot;60%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 결과 모든 실험에서 가장 좋은 성능을 보였으며, 네트워크가 깊어짐에 따라서 큰 폭으로 성능 향상이 발생하는 것을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Park, Wonpyo, et al. “Relational Knowledge Distillation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Relational Knowledge Distillation</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">DenseNet</title>
      <link href="https://blog.airlab.re.kr/2019/07/densenet" rel="alternate" type="text/html" title="DenseNet" />
      <published>2019-07-24T22:00:00+09:00</published>
      <updated>2019-07-24T22:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/densenet</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/densenet">&lt;p&gt;DenseNet: Densely Connected Convolutional Networks&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다. :)&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Densely Connected Convolutional Networks&lt;/strong&gt;(DenseNet) (&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;arXiv:1608.06993&lt;/a&gt;)이며 2016년 처음 발표가 된 연구입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/resnet-figure1.png&quot; alt=&quot;resnet-figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet에서의 shortcut Connection과 같은 Shorter Connection의 연구를 통해 과거보다 더 깊고, 더 정확하며, 더 효율적인 네트워크가 만들어져왔습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 ResNet과 비슷하지만 다른 네트워크(DenseNet)를 제안하고있습니다.&lt;/p&gt;

&lt;p&gt;DenseNet은 아래와 같이 각각의 레이어의 모든 레어어가 모두 feed-forward로 연결되어있는 구조로 되어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/figure1.png&quot; width=&quot;50%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 DenseBlock의 Convolution Layer는 이전의 모든 Convolution Layer의 Output들과 Concatenate되어 Convolution되게 됩니다. 여기서 ResNet과의 차이점이 들어나게 되는데 ResNet은 과거의 Feature Map이 서로 더해지는 반면에 DenseNet에서는 각 Feature Map들이 Concat 되는 것입니다. 이를 통해 DenseNet은 과거의 Feature Map과 현재의 Feature Map이 서로 섞이지 않고 학습이 될 수 있도록 합니다.&lt;/p&gt;

&lt;p&gt;이러한 구조는 아래와 같은 장점을 발생시킵니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Vanishing-gradient 문제 완화&lt;/li&gt;
  &lt;li&gt;더 강력한 피쳐 프로파게이션이 가능&lt;/li&gt;
  &lt;li&gt;피쳐 재사용을 촉진&lt;/li&gt;
  &lt;li&gt;파라미터의 수를 감소&lt;/li&gt;
  &lt;li&gt;Regularlizing 효과와 Overfitting 감소&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그리고 이 논문은 여러 벤치마크 DB인 CIFAR10, CIFAR100, SVHN, ImageNet에서 검증되었고, 논문발표 당시 state-of-the-art를 달성했습니다.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/table1.png&quot; width=&quot;80%&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DenseNet은 표와 같은 구조로 설계가 되어있습니다. 여기에서 차별점이 되는 Dense Block과, Transition Block을 한번 알아봅시다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dense Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dense Block은 DenseNet의 가장 중요한 컨셉이 포함된 블록입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/figure1.png&quot; width=&quot;50%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진과 같이 각 레이어는 이전의 레이어의 Output을 계속 Concat하여 Feed-forward 합니다. 그러다 보면 어느 지점에서는 채널이 아주 커지는 경우가 생길 수 있는데, 이 때문에 이 논문에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Growth rate&lt;/code&gt;이라는 개념을 도입합니다. 각 블록의 Convolution들은 Growth rate만큼의 채널만 Output하여 너무 큰 Output이 발생하지 않도록 해줍니다. 따라서, 위 그림은 Growth rate가 4인 경우의 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transition Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DenseNet은 Feature를 Down-sampling하기 위해 Transition Block을 사용합니다. Transition Block은  1x1 Convolution과 average pooling으로 구성이 돠어있으며, DenseNet의 하이퍼파라미터인 &lt;code class=&quot;highlighter-rouge&quot;&gt;theta&lt;/code&gt;값을 통해 다운샘플링하게 됩니다. 기본으로는 theta에 0.5를 주어 average pooling시 stride를 2 (1/theta)로 주어 다운샘플링하게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/table2.png&quot; width=&quot;80%&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문에서는 여러 환경에서의 실험을 진행하였는데, DesneNet이 가장 뛰여난 성능을 보이는 것으로 나타났습니다.(수치는 test error(%))&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-densenet-for-cifar10&quot;&gt;Implementation of DenseNet for CIFAR10&lt;/h3&gt;

&lt;p&gt;실제 논문을 &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;를 통해 모델을 구현해보았고, 코드는 &lt;a href=&quot;https://github.com/J911/DenseNet-pytorch&quot;&gt;https://github.com/J911/DenseNet-pytorch&lt;/a&gt;에 배포해 두었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/result.png&quot; width=&quot;80%&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;color&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;growth_rate&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;theta&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;dropout&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;scheduler&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;test error(%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Orange&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;lr_decay [60, 120, 160]&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Blue&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;lr_decay [60, 120, 160]&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pink&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CosineAnnealingLR&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.86&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Huang, Gao, et al. “Densely connected convolutional networks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">DenseNet: Densely Connected Convolutional Networks</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Wide Residual Networks</title>
      <link href="https://blog.airlab.re.kr/2019/07/WRN" rel="alternate" type="text/html" title="Wide Residual Networks" />
      <published>2019-07-17T19:00:00+09:00</published>
      <updated>2019-07-17T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/WRN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/WRN">&lt;p&gt;Wide Residual Networks&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다. 앞으로 연구실 세미나 준비를 통해 알게 된 내용을 연구실 블로그에 기록하게 되었습니다 :)&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Wide Residual Networks&lt;/strong&gt;(Wide ResNet) (&lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;arXiv:1605.07146&lt;/a&gt;)이며 2016년 발표가 된 연구입니다.&lt;/p&gt;

&lt;p&gt;같은 과거의 연구들은 점진적으로 CNN(Convolution Neural Networks)이 이미지 인식을 위해 더 깊어질 수 있도록 연구되어왔습니다.&lt;/p&gt;

&lt;p&gt;이를 위해 Optimizer, Initializer, Skip-connection과 같은 많은 방법들이 연구되어왔고, 그중, ResNet(Residual Networks)는 shortcut connection을 통해 네트워크가 깊어지면 깊어질수록 생기는 &lt;code class=&quot;highlighter-rouge&quot;&gt;vanishing gradient&lt;/code&gt;를 해결하면서도 더 깊게 네트워크를 쌓을 수 있도록 설계되었습니다.&lt;/p&gt;

&lt;h3 id=&quot;width-vs-depth-in-residual-networks&quot;&gt;Width vs Depth in residual Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/figure1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet은 shorcut connection을 통해 많은 layer을 학습 할 수 있도록 하였습니다. 하지만 망이 깊어지면 깊어질 수록 의미있는 정보(context)를 갖는 필터의 수의 비가 적어지는 문제가 발생하게 되었습니다.&lt;/p&gt;

&lt;p&gt;때문에 저자는 Block의 수를 증가시키지 않고, Residual Block의 Channel을 증가시키는 방향으로 연구를 시도하였습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 Residual Block을 (a), (c)의 구조와 같이 3x3 컨볼루션이 두 개로 이루어진 경우를 B(3,3)으로 표기하였습니다. 이와 마찬가지로 (b)의 경우는 B(1,3,1) (d)는 B(3,1,3)으로 표기가 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table1.png&quot; alt=&quot;Table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;B(3,3)의 경우 구조가 위와 같이 정의됩니다. K 는 widen factor, N은 블록의 수(각 Residual Block의 수)입니다. 이해를 돕기 위해 K가 2인 경우 아래와 같이 구현이 될 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WideResNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# pseudo code...&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;/h3&gt;
&lt;p&gt;Dropout은 Coadaptive하고 overfitting을 막기 위해 많은 네트워크에 적용되어 왔습니다. 추가적으로 Internal Covariate Shift 이슈를 막기위한 Batch Norm 과 같은 방법들도 연구가되었는데, 이 방법들은 Regularizer의 효과도 볼 수 있습니다.
이 논문에서는 Residual Block의 컨볼루션 사이에 Dropout(Dorp rate 0.3)을 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental result&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table5.png&quot; alt=&quot;Table5&quot; /&gt;
본 논문에서는 CIFAR10과 CIFAR100을 여러 모델을 통해 실험하였고, 위의 결과와 같이 WRN(depth 28, k 10)이 test error 4로 가장 높은 성능을 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/figure2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;실선은 test error, 점선은 train error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table6.png&quot; alt=&quot;Table6&quot; /&gt;
추가적으로 이 논문에서는 Dropout을 사용한 것과 사용하지 않은 것을 비교하였는데, 이 결과 역시 드롭아웃을 사용한 것이 더 좋은 결과를 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-wrn&quot;&gt;Implementation of WRN&lt;/h3&gt;

&lt;p&gt;실제 논문을 &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;를 통해 모델을 구현해보았고, 코드는 &lt;a href=&quot;https://github.com/J911/WRN-pytorch&quot;&gt;https://github.com/J911/WRN-pytorch&lt;/a&gt;에 배포해 두었습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Zagoruyko, Sergey, and Nikos Komodakis. “Wide residual networks.” arXiv preprint arXiv:1605.07146 (2016).&lt;/li&gt;
  &lt;li&gt;He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Wide Residual Networks</summary>
      

      
      
    </entry>
  
</feed>
