<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/jaemin/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-10-08T16:15:48+09:00</updated>
  <id>https://blog.airlab.re.kr/author/jaemin/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Mean teachers are better role models</title>
      <link href="https://blog.airlab.re.kr/2019/09/mean-teacher" rel="alternate" type="text/html" title="Mean teachers are better role models" />
      <published>2019-09-19T11:00:00+09:00</published>
      <updated>2019-09-19T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/09/mean-teacher</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/09/mean-teacher">&lt;p&gt;Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!&lt;/p&gt;

&lt;p&gt;오늘 소개할 논문은 Mean teachers are better role models (&lt;a href=&quot;https://arxiv.org/abs/1703.01780&quot;&gt;arXiv:1703.01780&lt;/a&gt;)이며, NIPS 2017에서 소개된 논문입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Semi-Supervised Leaning에 관련된 논문이고, Π Model과 Temporal ensemble 이후에 소개되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure1.png&quot; width=&quot;80%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;Π-model&quot;&gt;Π Model&lt;/h4&gt;
&lt;p&gt;Π Model은 위 그림의 위 쪽과 같은 구조와 같이, 라벨이 주어진 경우 Closs Entropy를 사용하여 학습을 하고, 라벨이 주어지지 않은 경우는 augmented input의 결과와 이전과 다른 dropout 과 augmention을 사용한 결과의 squared difference를 이용해 학습을 합니다.&lt;/p&gt;

&lt;h4 id=&quot;temporal-ensemble&quot;&gt;Temporal ensemble&lt;/h4&gt;
&lt;p&gt;Temporal ensemble과 Π Model의 차이점은 라벨이 주어지지 않았을 때 Temporal ensemble은 이전의 결과에 대한 값들을 앙상블한 결과를 가지고 학습을 하는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;mean-teacher&quot;&gt;Mean Teacher&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/cover.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;
위에서 소개한 두개의 모델은 모델을 공유하여 학습이 되지만, Mean Teacher의 경우 Student와 Teacher 모델로 나뉘어 학습이 진행됩니다. Teacher 모델은 Student 모델의 Weight를 공유하고, 대신에 Student모델의 아래의 방식과 같이 EMS Weight를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure2.png&quot; width=&quot;30%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 후 라벨이 주어지지 않은 경우 기존 방식과 유사하게 Teacher 모델과 Student 모델의 consistency cost (J)를 계산하여 학습이 진행되게 됩니다.
&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/figure3.png&quot; width=&quot;40%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 방법은 Temporal ensemble에 비하여 2가지 이점을 가지는데, 첫 번째는 Student와 Teacher 사이의 빠른 Feedback이 가능하여 더 높은 ACC를 가지게 되는 것이고, 두 번째는 large scale datasets을 online으로 학습할 수 있는 것 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-mean-teacher/table1.png&quot; width=&quot;80%&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과와 같이 SVHM과 CIFAR10 에서의 Mean Teacher를 사용한 네트워크가 Π Model과 Temporal ensemble보다 더 높은 퍼포먼스를 보임을 확인할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tarvainen, Antti, and Harri Valpola. “Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.” Advances in neural information processing systems. 2017.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rasmus, Antti, et al. “Semi-supervised learning with ladder networks.” Advances in neural information processing systems. 2015.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Laine, Samuli, and Timo Aila. “Temporal ensembling for semi-supervised learning.” arXiv preprint arXiv:1610.02242 (2016).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Relational Knowledge Distillation</title>
      <link href="https://blog.airlab.re.kr/2019/07/rkd" rel="alternate" type="text/html" title="Relational Knowledge Distillation" />
      <published>2019-07-28T22:00:00+09:00</published>
      <updated>2019-07-28T22:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/rkd</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/rkd">&lt;p&gt;Relational Knowledge Distillation&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다!&lt;/p&gt;

&lt;p&gt;오늘 소개할 논문은 Relational Knowledge Distillation(RKD) (&lt;a href=&quot;https://arxiv.org/abs/1904.05068&quot;&gt;arXiv:1904.05068&lt;/a&gt;)이며, CVPR2019에 Accepted 된 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;knowledge-distillation&quot;&gt;Knowledge distillation&lt;/h3&gt;

&lt;p&gt;Knowledge distillation은 Teacher Model이 Student Model에게 Knowledge를 transferring 하는 것 인데요, 이러한 연구의 대표적인 연구는 Hinton의 &lt;code class=&quot;highlighter-rouge&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/code&gt;가 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 논문의 시작은 computing resources의 양을 줄이이 위해 시작이 되었는데요,&lt;/p&gt;

&lt;p&gt;만약 모델의 ACC를 높이기 위해서는 모델을 크게 설계를 하면 되지만 모델이 커질 수록 파라미터의 양은 증가하고 더 많은 computing resource를 차지할 것입니다. 이러한 문제는 computing resource를 감당할 수 있는 인프라를 가 없는 사람들에게는 큰 이슈로 남게되고, 특히 서비스로 배포가 되어야하는 모델들은 경량화 되어야 한다는 조건이 많이 붙습니다.&lt;/p&gt;

&lt;p&gt;이러한 이슈로 큰 모델이 학습한 정보를 &lt;code class=&quot;highlighter-rouge&quot;&gt;증류(distillation)&lt;/code&gt;하여, 작은 모델에 주입할 수 있는 Knowledge distillation가 시작되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure2.png&quot; width=&quot;80%&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 Softmax를 사용하는 네트워크가 있다고 해봅시다. 그러면 one hot으로 작성된 정답 라벨(hard label)과 predict을 비교하여 backward를 하게 됩니다.&lt;/p&gt;

&lt;p&gt;Teacher Model은 위와 같은 프로세스로 학습을 진행하고 큰 모델이기 때문에 좋을 ACC를 만들어 낼 것입니다. 그리고 Student Model은 Teacher Model의 softmax를 거치고 나온 0과 1사이의 확률 값을 soft label로 하여 학습하는 방법이 KD(Knowledge Distillation)입니다.&lt;/p&gt;

&lt;h3 id=&quot;relational-knowledge-distillation&quot;&gt;Relational Knowledge Distillation&lt;/h3&gt;

&lt;p&gt;RKD(Relational Knowledge Distillation)는 Knowledge Distillation에서 Knowledge를 어떻게 정의할 것 인가에서 시작이 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure1.png&quot; width=&quot;80%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;를 각 Input에서 나오는 Teacher의 output, &lt;code class=&quot;highlighter-rouge&quot;&gt;s&lt;/code&gt;를 Student 의 output이라고 했을 때 기존의 KD는 각각의 t가 s에 point-to-point로 연결되는 위 그림과 매칭됩니다. 때문에 이 논문에서는 기존의 KD를 IKD(Individual Knowledge Distillation)라고 표현합니다.&lt;/p&gt;

&lt;p&gt;IKD는 Individual한 output을 Knowledge로 하여 Distillation 한다고 하면, RKD는 output들의 관계(Relation)를 Knowledge로 하여 Distillation합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure3.png&quot; width=&quot;80%&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 연구에서 이야기 하는 관계는 각 output이 embedding space에 표현될 때의 node간의 distance와 angle입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure4.png&quot; width=&quot;60%&quot; alt=&quot;figure4&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure5.png&quot; width=&quot;60%&quot; alt=&quot;figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;distance는 위의 식으로 표현 될 수 있고 뮤는 node간 거리의 평균이라고 이해하면 될 것 같습니다. 따라서 아래와 같은 식을 otim하는 것으로 학습 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure6.png&quot; width=&quot;60%&quot; alt=&quot;figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;angle도 아래와 같이 표현이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure7.png&quot; width=&quot;60%&quot; alt=&quot;figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure8.png&quot; width=&quot;60%&quot; alt=&quot;figure8&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;p&gt;이 논문에서는 여러 Task(Metric learning, Image classification, Few-shot learning) 에서 위에서 소개한 distance-wise, angle-wise 를 통해 학습한 결과를 다른 네트워크와 비교하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metric learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure9.png&quot; width=&quot;100%&quot; alt=&quot;figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image classification&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure10.png&quot; width=&quot;60%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Few-shot learning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-rkd/figure10.png&quot; width=&quot;60%&quot; alt=&quot;figure10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그 결과 모든 실험에서 가장 좋은 성능을 보였으며, 네트워크가 깊어짐에 따라서 큰 폭으로 성능 향상이 발생하는 것을 보였습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Park, Wonpyo, et al. “Relational Knowledge Distillation.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Relational Knowledge Distillation</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">DenseNet</title>
      <link href="https://blog.airlab.re.kr/2019/07/densenet" rel="alternate" type="text/html" title="DenseNet" />
      <published>2019-07-24T22:00:00+09:00</published>
      <updated>2019-07-24T22:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/densenet</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/densenet">&lt;p&gt;DenseNet: Densely Connected Convolutional Networks&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다. :)&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Densely Connected Convolutional Networks&lt;/strong&gt;(DenseNet) (&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;arXiv:1608.06993&lt;/a&gt;)이며 2016년 처음 발표가 된 연구입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/resnet-figure1.png&quot; alt=&quot;resnet-figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet에서의 shortcut Connection과 같은 Shorter Connection의 연구를 통해 과거보다 더 깊고, 더 정확하며, 더 효율적인 네트워크가 만들어져왔습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 ResNet과 비슷하지만 다른 네트워크(DenseNet)를 제안하고있습니다.&lt;/p&gt;

&lt;p&gt;DenseNet은 아래와 같이 각각의 레이어의 모든 레어어가 모두 feed-forward로 연결되어있는 구조로 되어있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/figure1.png&quot; width=&quot;50%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;각 DenseBlock의 Convolution Layer는 이전의 모든 Convolution Layer의 Output들과 Concatenate되어 Convolution되게 됩니다. 여기서 ResNet과의 차이점이 들어나게 되는데 ResNet은 과거의 Feature Map이 서로 더해지는 반면에 DenseNet에서는 각 Feature Map들이 Concat 되는 것입니다. 이를 통해 DenseNet은 과거의 Feature Map과 현재의 Feature Map이 서로 섞이지 않고 학습이 될 수 있도록 합니다.&lt;/p&gt;

&lt;p&gt;이러한 구조는 아래와 같은 장점을 발생시킵니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Vanishing-gradient 문제 완화&lt;/li&gt;
  &lt;li&gt;더 강력한 피쳐 프로파게이션이 가능&lt;/li&gt;
  &lt;li&gt;피쳐 재사용을 촉진&lt;/li&gt;
  &lt;li&gt;파라미터의 수를 감소&lt;/li&gt;
  &lt;li&gt;Regularlizing 효과와 Overfitting 감소&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그리고 이 논문은 여러 벤치마크 DB인 CIFAR10, CIFAR100, SVHN, ImageNet에서 검증되었고, 논문발표 당시 state-of-the-art를 달성했습니다.&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/table1.png&quot; width=&quot;80%&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DenseNet은 표와 같은 구조로 설계가 되어있습니다. 여기에서 차별점이 되는 Dense Block과, Transition Block을 한번 알아봅시다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dense Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dense Block은 DenseNet의 가장 중요한 컨셉이 포함된 블록입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/figure1.png&quot; width=&quot;50%&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진과 같이 각 레이어는 이전의 레이어의 Output을 계속 Concat하여 Feed-forward 합니다. 그러다 보면 어느 지점에서는 채널이 아주 커지는 경우가 생길 수 있는데, 이 때문에 이 논문에서는 &lt;code class=&quot;highlighter-rouge&quot;&gt;Growth rate&lt;/code&gt;이라는 개념을 도입합니다. 각 블록의 Convolution들은 Growth rate만큼의 채널만 Output하여 너무 큰 Output이 발생하지 않도록 해줍니다. 따라서, 위 그림은 Growth rate가 4인 경우의 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transition Block&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DenseNet은 Feature를 Down-sampling하기 위해 Transition Block을 사용합니다. Transition Block은  1x1 Convolution과 average pooling으로 구성이 돠어있으며, DenseNet의 하이퍼파라미터인 &lt;code class=&quot;highlighter-rouge&quot;&gt;theta&lt;/code&gt;값을 통해 다운샘플링하게 됩니다. 기본으로는 theta에 0.5를 주어 average pooling시 stride를 2 (1/theta)로 주어 다운샘플링하게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/table2.png&quot; width=&quot;80%&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문에서는 여러 환경에서의 실험을 진행하였는데, DesneNet이 가장 뛰여난 성능을 보이는 것으로 나타났습니다.(수치는 test error(%))&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-densenet-for-cifar10&quot;&gt;Implementation of DenseNet for CIFAR10&lt;/h3&gt;

&lt;p&gt;실제 논문을 &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;를 통해 모델을 구현해보았고, 코드는 &lt;a href=&quot;https://github.com/J911/DenseNet-pytorch&quot;&gt;https://github.com/J911/DenseNet-pytorch&lt;/a&gt;에 배포해 두었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-24-densenet/result.png&quot; width=&quot;80%&quot; alt=&quot;table2&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;color&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;growth_rate&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;theta&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;dropout&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;scheduler&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;test error(%)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Orange&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;lr_decay [60, 120, 160]&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5.45&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Blue&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;lr_decay [60, 120, 160]&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7.08&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Pink&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;32&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CosineAnnealingLR&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.86&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Huang, Gao, et al. “Densely connected convolutional networks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">DenseNet: Densely Connected Convolutional Networks</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Wide Residual Networks</title>
      <link href="https://blog.airlab.re.kr/2019/07/WRN" rel="alternate" type="text/html" title="Wide Residual Networks" />
      <published>2019-07-17T19:00:00+09:00</published>
      <updated>2019-07-17T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/WRN</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/WRN">&lt;p&gt;Wide Residual Networks&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 이재민입니다. 앞으로 연구실 세미나 준비를 통해 알게 된 내용을 연구실 블로그에 기록하게 되었습니다 :)&lt;/p&gt;

&lt;p&gt;제가 이번에 읽은 논문은 &lt;strong&gt;Wide Residual Networks&lt;/strong&gt;(Wide ResNet) (&lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;arXiv:1605.07146&lt;/a&gt;)이며 2016년 발표가 된 연구입니다.&lt;/p&gt;

&lt;p&gt;같은 과거의 연구들은 점진적으로 CNN(Convolution Neural Networks)이 이미지 인식을 위해 더 깊어질 수 있도록 연구되어왔습니다.&lt;/p&gt;

&lt;p&gt;이를 위해 Optimizer, Initializer, Skip-connection과 같은 많은 방법들이 연구되어왔고, 그중, ResNet(Residual Networks)는 shortcut connection을 통해 네트워크가 깊어지면 깊어질수록 생기는 &lt;code class=&quot;highlighter-rouge&quot;&gt;vanishing gradient&lt;/code&gt;를 해결하면서도 더 깊게 네트워크를 쌓을 수 있도록 설계되었습니다.&lt;/p&gt;

&lt;h3 id=&quot;width-vs-depth-in-residual-networks&quot;&gt;Width vs Depth in residual Networks&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/figure1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet은 shorcut connection을 통해 많은 layer을 학습 할 수 있도록 하였습니다. 하지만 망이 깊어지면 깊어질 수록 의미있는 정보(context)를 갖는 필터의 수의 비가 적어지는 문제가 발생하게 되었습니다.&lt;/p&gt;

&lt;p&gt;때문에 저자는 Block의 수를 증가시키지 않고, Residual Block의 Channel을 증가시키는 방향으로 연구를 시도하였습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 Residual Block을 (a), (c)의 구조와 같이 3x3 컨볼루션이 두 개로 이루어진 경우를 B(3,3)으로 표기하였습니다. 이와 마찬가지로 (b)의 경우는 B(1,3,1) (d)는 B(3,1,3)으로 표기가 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table1.png&quot; alt=&quot;Table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;B(3,3)의 경우 구조가 위와 같이 정의됩니다. K 는 widen factor, N은 블록의 수(각 Residual Block의 수)입니다. 이해를 돕기 위해 K가 2인 경우 아래와 같이 구현이 될 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WideResNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# pseudo code...&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;/h3&gt;
&lt;p&gt;Dropout은 Coadaptive하고 overfitting을 막기 위해 많은 네트워크에 적용되어 왔습니다. 추가적으로 Internal Covariate Shift 이슈를 막기위한 Batch Norm 과 같은 방법들도 연구가되었는데, 이 방법들은 Regularizer의 효과도 볼 수 있습니다.
이 논문에서는 Residual Block의 컨볼루션 사이에 Dropout(Dorp rate 0.3)을 사용합니다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental result&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table5.png&quot; alt=&quot;Table5&quot; /&gt;
본 논문에서는 CIFAR10과 CIFAR100을 여러 모델을 통해 실험하였고, 위의 결과와 같이 WRN(depth 28, k 10)이 test error 4로 가장 높은 성능을 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/figure2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;실선은 test error, 점선은 train error&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-17-WRN/table6.png&quot; alt=&quot;Table6&quot; /&gt;
추가적으로 이 논문에서는 Dropout을 사용한 것과 사용하지 않은 것을 비교하였는데, 이 결과 역시 드롭아웃을 사용한 것이 더 좋은 결과를 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;implementation-of-wrn&quot;&gt;Implementation of WRN&lt;/h3&gt;

&lt;p&gt;실제 논문을 &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;를 통해 모델을 구현해보았고, 코드는 &lt;a href=&quot;https://github.com/J911/WRN-pytorch&quot;&gt;https://github.com/J911/WRN-pytorch&lt;/a&gt;에 배포해 두었습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Zagoruyko, Sergey, and Nikos Komodakis. “Wide residual networks.” arXiv preprint arXiv:1605.07146 (2016).&lt;/li&gt;
  &lt;li&gt;He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jaemin Lee</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Wide Residual Networks</summary>
      

      
      
    </entry>
  
</feed>
