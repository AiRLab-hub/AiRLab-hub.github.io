<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/juhui/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-11-27T18:56:34+09:00</updated>
  <id>https://blog.airlab.re.kr/author/juhui/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">CAM: Learning Deep Features For Discriminative Localization</title>
      <link href="https://blog.airlab.re.kr/2019/11/CAM" rel="alternate" type="text/html" title="CAM: Learning Deep Features For Discriminative Localization" />
      <published>2019-11-27T11:30:00+09:00</published>
      <updated>2019-11-27T11:30:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/CAM</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/CAM">&lt;p&gt;안녕하세요 AiRLab 박주희입니다.
오늘 소개할 논문은 Learning Deep Features For Discriminative Localization 으로 CAM이라고도 불리며,CVPR2016 에서 소개된 논문입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;introduction&quot;&gt;&lt;strong&gt;&lt;u&gt;Introduction&lt;/u&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;CNN은 이미지 레벨의 라벨 교육만 받았음에도 object를 localization하는 능력이 뛰어납니다.
이 논문에서는 단순히 object를 localization 하는것을 넘어 이미지의 어떤 영역이 차별화 되어 사용되고 있는지를 정확히 파악하는 능력을 Weakly-Supervised Object Localization과 Visualizing CNNs을 통해 generalize 할 수있다는 것을보여줍니다. 또한 GAP를 사용한 CAM (Class Activation Map)이라는 방법을 제시하여 이미지를 차별화 하였습니다. 방금 언급한 내용들을 밑에서 더 자세히 설명을 해 보도록 하겠습니다.&lt;/p&gt;

&lt;h4 id=&quot;weakly-supervised-object-localization&quot;&gt;Weakly-Supervised Object Localization&lt;/h4&gt;
&lt;p&gt;앞선 연구들에서는 실제로 Localization 능력을 평가하지 않았고, end-to-end로 train하지 않았으며 Object Localization을 위해 네트워크의 multiple forward pass가 필요 했습니다. 이 때문에 실제 데이터 셋으로 확장되기 어려웠고 이 논문에서는 &lt;b&gt;end-to-end로 train&lt;/b&gt;하고 &lt;b&gt;single forward pass&lt;/b&gt;로 Object Localization 할 수있음을 보였습니다. 이 접근 방식과 가장 유사한 방식은 Global Max Pooling 인데 object 한 지점을 localize 하는 방법입니다. 이 방법은 object의 전체범위를 결정하기보다 경계선에 놓여있는 한 점에 한정 됩니다. 이와 비슷한 방법인 Global Average Pooling이 이 논문에서 처음 제시한 방법은 아니지만 더욱 정확한 discriminative localization에 적용 할수있다는 것이 이 논문의 핵심입니다. 여기서 이 기술의 단순성으로 빠르고 정확한 Localization을 위해 다양한 Computer Vision에 적용 될 것이라고 믿고 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;visualizing-cnns&quot;&gt;Visualizing CNNs&lt;/h4&gt;
&lt;p&gt;앞선 연구들에서는 fully-connected layer를 무시하고 전체적으로 불완전하며 Conv층만 분석을 했습니다. 이 논문에서는 fc층을 없애고 대부분의 성능을 유지하였는데 이로써 네트워크의 처음부터 끝까지 이해를 할 수있게 되었고 이 방식은 차별화되는 이미지의 영역을 정확히 강조 할 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;cam-class-activation-map&quot;&gt;CAM (Class Activation Map)&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/fig3.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
CNN에서 Global Average Pooling을 사용하여 CAM을 생성 합니다.&lt;br /&gt;
특정 카테고리에 대한 CAM은 해당 카테고리를 식별하기위해 CNN이 사용하는 차별화된 이미지 영역을 나타냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/fig2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
위 그림은 map 생성 절차입니다.
Network In Network와 GoogleNet과 유사한 network의 아키텍쳐를 사용하여 주로 conv층으로 구성되며 최종 출력 직전의 conv feature map에 fc대신 GAP를 사용하였습니다.
GAP는 마지막 conv layer에서 각 단위의 feature map의 spatial 평균을 출력합니다.
이 값의 가중합은 최종 출력 생성에 사용되는데 이와 유사하게 이 논문에서는 CAM을 얻기 위해 conv layer의 feature map의 가중치 합을 계산하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/function1.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/function2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
F&lt;sub&gt;k&lt;/sub&gt;는 feature map을 의미하며, softmax의 식은 weight들과 F&lt;sub&gt;k&lt;/sub&gt;들과의 sum을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/fig4.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
이 그림은 MAP을 생성하기 위해 다른 클래스들을 사용할 때 단일 영상에 대한 CAM의 차이를 강조합니다. 
다른 카테고리에 대한 차별적인 영역이 특정 이미지에 대해서도 다른 것을 관찰합니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 GAP loss가 물체의 범위를 식별하도록 촉진한다고 믿는데 map의 평균을 구할 때 모든 낮은 activation이 특정 map의 출력을 감소시키기 때문에 물체의 모든 차별적인 부분을 찾아냄으로써 그 값을 최대화할 수 있기 때문입니다. (GMP는 차별적인 부분을 제외하고 모든 영상에 대한 점수는 최대값만 수행하기 때문에 score에 영향을 미치지 않습니다.)&lt;/p&gt;

&lt;h4 id=&quot;weakly-supervised-object-localization-실험&quot;&gt;Weakly-Supervised Object Localization 실험&lt;/h4&gt;
&lt;p&gt;논문에서는 classification과 localization에 대해 실험을 했습니다. 실험 설정은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;기본적인 설정으로 fc layer를 GAP로 변경하였고 여러 conv layer를 제거함으로써 mapping resolution을 향상시켰습니다.
또한 AlexNet, VGGnet, GoogleNet을 각각 수정하여 AlexNet-GAP, VGGnet-GAP, GoogleNet-GAP네트워크를 만들었습니다.&lt;/p&gt;

&lt;h3 id=&quot;classification-results&quot;&gt;Classification Results&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table1.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
전체적으로 conv layer를 제거하고 GAP를 사용한 네트워크들이 성능이 1-2%떨어진 모습을 볼 수있습니다.
AlexNet*-GAP는 GAP전에 conv layer를 2개 더 추가한 네트워크입니다. 
 localization에 대한 높은 성능을 얻기 위해서는 classification 성능이 중요한 것을 알 수있습니다.&lt;/p&gt;

&lt;p&gt;### Localization Results
 &lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table1.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Simple thresholding technique를 사용하여 heat 된 map의 부분을 찾는 방식을 사용하였습니다. 
(Simple thresholding technique 이란 Activation map에서 max 값을 찾아 그 20% 이상이 되는 영역을 찾고 labeling을 통해 가장 큰 덩어리를 찾고 그것을 둘러싼 bounding box를 찾는 방법입니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
이 테이블은 이전 연구인 backpropagation기법을 쓴 네트워크와 CAM을 적용한 네트워크를 비교한 것입니다.&lt;br /&gt;
GoogleNet-GAP가 가장 성능이 좋은 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-26-CAM/table3.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
Weakly-Supervised방식과 fully-supervised방식을 쓴 네트워크를 비교한 테이블입니다. 
GoogleNet-GAP(heuristics)가 37.1%로 CAM방법중 가장 성능이 좋지만 fully-supervised 방식을 쓴 AlexNet과 성능이 비슷하며 GoogleNet끼리 비교를 했을때는 약간의 성능 저하가 보입니다. 
따라서 이 논문에서는 fully-supervised 네트워크와 비교를 하기에는 아직 무리가 있다고 판단을 하였습니다. 
여기서 약간 다른 bounding box 선택 기법을 사용함으로써 GoogleNet-GAP방식에 비해  GoogleNet-GAP(heuristics) 가 5.8% 향상 되었는데 이 방식은 1등 예측 class와 2등 예측 class의 activation map으로부터 하나는 타이트하고 하나는 루즈한 bounding box를 총 두개 선택합니다. 
그리고 3등 예측 class의 bounding box로부터 루즈한 박스를 고르는 방법으로 이는 classification의 accuracy와 localization의 accuracy사이에서 trade-off 관계입니다. 
따라서 이 localization accuracy가 향상됨을 볼 수 있습니다.&lt;/p&gt;

&lt;h4 id=&quot;결론&quot;&gt;결론&lt;/h4&gt;
&lt;p&gt;결론적으로 Learning Deep Features For Discriminative Localization은 CNN에서 GAP를 이용하여 bounding box annotation없이 object localization을 하는 &lt;b&gt;CAM&lt;/b&gt; 이라는 기법을 제안한 논문입니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Juhui Park</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요 AiRLab 박주희입니다. 오늘 소개할 논문은 Learning Deep Features For Discriminative Localization 으로 CAM이라고도 불리며,CVPR2016 에서 소개된 논문입니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification</title>
      <link href="https://blog.airlab.re.kr/2019/11/He-initialization" rel="alternate" type="text/html" title="Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification" />
      <published>2019-11-27T10:30:00+09:00</published>
      <updated>2019-11-27T10:30:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/He-initialization</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/He-initialization">&lt;p&gt;안녕하세요 AiRLab 박주희입니다.
오늘 소개할 논문은 Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification (https://arxiv.org/pdf/1502.01852.pdf)이며, ICCV2015에서 소개된 논문입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;이 논문에서는 두가지 측면에 대한 image classification을 위한 rectifier neural networks를 연구했습니다.
먼저 ReLU에서 파생된 Parametric Rectified LinearUnit (PReLU)을 제안합니다. PReLU는 추가 계산 cost가 거의 들지 않고, overfitting의 위험도 적습니다.
두번째로 rectifier의 비 선형성을 고려한 강력한 초기화 방법을 도출했습니다. 이 방법은 깊은 모델에서 직접적으로 사용 할 수있고, 더 깊고 넓은 network architecture를 살펴볼수있습니다.&lt;/p&gt;

&lt;p&gt;이런 학습가능한 활성함수와 초기화 방법을 통해 ImageNet 2012 classification dataset에서 4.94% top-5 error를 달성하였습니다.
이 결과는 보고된 인간 수준 성능(5.1%)를 능가하는 최초의 결과 입니다.&lt;/p&gt;

&lt;h3 id=&quot;parametric-rectifiers&quot;&gt;Parametric Rectifiers&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/ReLU_vs_PReLU.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
ReLU와 PReLU그래프 입니다. 여기서 PReLU 그래프의 경우, 음의 부분은 일정한 값을 가지지 않고 적응적으로 학습 합니다.
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/PReLU_Definition.PNG&quot; width=&quot;30%&quot; alt=&quot;error&quot; /&gt;
a&lt;sub&gt;i&lt;/sub&gt;=0 일경우 ReLU가 되고, a&lt;sub&gt;i&lt;/sub&gt;가 학습 가능한 파라미터일 경우 PReLU가 되며 a&lt;sub&gt;i&lt;/sub&gt;=0.01 일 경우 Leaky ReLU가 됩니다.
이때 PReLU는 매우 적은 수의 추가 매개변수를 도입하였습니다. (추가 파라미터의 수는 총 채널수와 동일하고, 이는 총 가중치 수를 고려할때 무시가 가능합니다.)
그렇기 때문에 Overfitting에 대해 걱정을 하지 않아도 되는 이점이 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;initialization-of-filter-weights-for-rectifiers&quot;&gt;Initialization of Filter Weights for Rectifiers&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;“Xavier” initialization VS “He” initialization&lt;/b&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/Xavier_vs_He.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
“Xavier”초기화 방법은 무작위 초기화가 아닌 입력과 출력의 특성을 고려한 방법으로, 선형인 경우에서만 사용 가능하지만
“He”초기화 방법은 비선형일 경우에도 사용이 가능한 “Xavier”방법의 변형입니다. 이 방법은 비 선형적인 ReLU와 PReLU함수에서도 사용이 가능합니다.
위 그래프의 빨간색이 “He”초기화 방법을 파란색은 “Xavier”초기화 방법을 나타냅니다.
He 초기화 방법은 가중치 분포를 2로 나누어 비 선형함수에서 쓰기 더욱 적합합니다. 
두 초기화 방법 모두 수렴 가능하지만 “He”초기화 방법이 더 빨리 수렴하는 것을 볼 수있습니다.
또한 오른쪽 그래프(30-layer모델)를 보면 “He”초기화 방법은 수렴을 하지만 “Xavier”초기화 방법은 학습을 완전히 지연시키고 gradient가 감소되는것을 관찰 할 수있습니다.&lt;/p&gt;

&lt;p&gt;따라서 “He”초기화 방법이 더 깊은 모델에서 적용이 가능하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/table2.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
하지만 ImageNet에서는 아직 큰 이점을 찾지 못했습니다.
30-layer 모델의 경우 38.56/16.59의 top-1/top-5 error를 가지는 반면에 위 표(14-layer)의 33.82/13.34 보다 훨씬 좋지 않음을 볼 수있는데
이는 layer가 깊을 수록 training error가 증가하기 때문이며 이 문제는 여전히 open problem 입니다.&lt;/p&gt;

&lt;p&gt;결론적으로 “He”초기화 방법은 깊은 모델에서의 정확성에 대한 이점은 보여주지 못했지만 깊이 증가에 대한 더 많은 연구를 위한 토대를 마련했습니다.&lt;/p&gt;

&lt;h3 id=&quot;experiments-on-imagenet&quot;&gt;Experiments on ImageNet&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;ReLU VS PReLU&lt;/b&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/Fig4.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
여기서 PReLU는 channel-wise 버전을 사용했고, ReLU와 PReLU 모두 같은 epoch으로 train하였습니다.
위 그래프는 training 동안 train/val error를 나타냈습니다.&lt;/p&gt;

&lt;p&gt;PReLU는 ReLU에 비해 더 빨리 수렴되는 것을 볼 수있으며 PReLU의 train error와 val error 모두 ReLU보다 낮습니다.
따라서 PReLU가 ReLU에 비해 더 좋은 성능을 가지고 있음을 다시 한번 입증 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Single-model Results and Multi-model Results&lt;/b&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/realsinglemodel.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt; 
A+ReLU가 VGG-19에 보고된 7.1% single-model의 결과 보다 상당히 좋습니다.
이는 얕은 모델을 미리 train 하지 않고 end-to-end train을 했기 때문이라고 보고 있습니다.
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/multimodel.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;
또한 C+PReLU는 5.7%로 multi-model 보다 좋은 결과를 가졌고, model B와 modle C를 비교했을때 C가 더 나음을 볼수있습니다.
(model B는 model A에 비해 deep하고, model C는 wide한 model입니다.)&lt;/p&gt;

&lt;p&gt;따라서 모델이 충분히 깊을때 폭이 정확도에 필수적인 요소인것을 알 수있습니다.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Surpassing Human-Level Performance?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;ImageNet 데이터셋에서 인간 성능이 약 5.1% top-5 error 인데 비해 이 연구는 4.94%의 error 결과를 도출 했습니다. 이는 인간 수준의 성과를 초과했음을 의미합니다.
&lt;img src=&quot;/assets/images/posts/2019-11-27-He-initialization/Fig5.PNG&quot; width=&quot;50%&quot; alt=&quot;error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이들의 방법으로 위 그림을 coucal”, “komondor”, “yellow lady’s slipper” 라고 성공적으로 인식을 하는 반면에 인간은 개,새,꽃 이라고 단순하게 인식을 합니다.
이렇게 특정 데이터셋에서는 우수한 결과를 도출하지만  일반적인 객체 인식에서 문맥의 이해나 고도의 지식이 필요한 경우에는 실수를 저지르기때문에 machine vision이  human vision을 능가하는것은 아닙니다.
그럼에도 이 결과는 시각적 인식에서 인간 수준의 성능과 일치하는 machine algorithm의 잠재적 가능성을 보여줍니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Juhui Park</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">안녕하세요 AiRLab 박주희입니다. 오늘 소개할 논문은 Delving Deep into Rectifiers Surpassing Human-Level Performance on Imagenet Classification (https://arxiv.org/pdf/1502.01852.pdf)이며, ICCV2015에서 소개된 논문입니다.</summary>
      

      
      
    </entry>
  
</feed>
