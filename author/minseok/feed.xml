<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/minseok/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-07-29T10:47:00+09:00</updated>
  <id>https://blog.airlab.re.kr/author/minseok/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Bag of Tricks</title>
      <link href="https://blog.airlab.re.kr/2019/07/bag-of-trick" rel="alternate" type="text/html" title="Bag of Tricks" />
      <published>2019-07-27T05:00:00+09:00</published>
      <updated>2019-07-27T05:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/bag-of-trick</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/bag-of-trick">&lt;p&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Bag of Tricks for Image Classification with Convolutional Neural Networks”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문의 저자들은 Image Classification에서 모델 구조의 변경(vgg,resnet,densenet)은 집중적인 주목을 받고 많은 사람들이 중요하다고 생각하지만, 나머지 Data Augmentations 및 Optimization Methods 과 같은 학습 방법들은 주목을 받지 못함에 안타까워 하고 있습니다. 또한 해당 방법들은 논문에서 간단하게 언급되거나, 심지어 논문에는 없고 소스 코드에만 존재하는 트릭들도 있습니다. 이 논문의 저자들은 이러한 학습 방법들을 자세히 조사하고 평가합니다. 아래의 사진을 보시면 트릭들만 잘 사용해도 4%의 정확도를 올릴 수 있는 것을 보실 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/cover.png&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;p&gt;우선 여러 트릭들을 비교 하려면 동일한 환경을 만들어야 합니다. 이 논문의 저자들은 ResNet50을 기본 구조로 하고 아래의 나열된 방법으로 전처리를 했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly sample an image and decode it into 32-bitfloating point raw pixel values in[0,255].
(사실 저희가 훈련시키는 대부분의 이미지가 float 32bit에 srgb형식이기 때문에 특별한 경우가 아니라면 그냥 기본 상태 입니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly crop a rectangular region whose aspect ratiois randomly sampled in[3/4,4/3]and area randomly sampled in[8%,100%], then resize the cropped regioninto a 224-by-224 square image.
(aspect ratiois을 지정하고 랜덤하게 이미지를 [224,224] 형식으로 크롭합니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Flip horizontally with 0.5 probability.
(50% 확률로 이미를 반전 합니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scale hue, saturation, and brightness with coefficients uniformly drawn from[0.6,1.4].
(색조, 채도 및 밝기를 균일하게 스케일합니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add PCA noise with a coefficient sampled from a normal distribution N(0,0.1).
(PCA노이즈를 이미지에 줍니다.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;모든 Conv,fc 층에는 Xavier Initialization을 하였고, Batch Normalization의 감마와 베타는 1,0 그리고 Optimizer는 NAG를 선택했습니다.
pytorch에서는 SGD의 파라메터중 nesterov를 True로 해주시면 됩니다.&lt;/p&gt;

&lt;p&gt;위에서 언급한 베이스 라인들은 너무 일반적인 내용이라 추가 설명은 하지 않겠습니다. 아래 보이는 사진은 베이스라인으로 구현한 Image Classification의 정확도와 각 논문에서 주장하는 정확도의 비교입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/table1.png&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;large-batch-training&quot;&gt;Large-batch training&lt;/h3&gt;

&lt;p&gt;최근 GPU 기술이 좋아지면서 Large-batch training이 가능해 졌습니다. 하지만 배치 사이즈가 증가하면 수렴속도가 늦어진다고 알려져 있고, 그 뜻은 테스트에서 정확도가 떨어질 확률이 있다는 것과 같습니다. 이 단란에서는 이러한 문제점을 경험적으로 풀어냅니다.&lt;/p&gt;

&lt;h4 id=&quot;linear-scaling-learning-rate&quot;&gt;Linear scaling learning rate.&lt;/h4&gt;

&lt;p&gt;배치사이즈를 증가시키면 러닝레이트 또한 선형적으로 증가시켜줘야 합니다. 256배치 사이즈의 기준이 되는 러닝레이트를 0.1로 설정하고, 0.1 x b/256 이라는 공식을 제안합니다.(배치사이즈를 증가시키면 당연히 표본이 많이 선택되기 때문에 분산을 줄어듭니다. 분산이 줄어들면 안정적 이기 때문에 러닝레이트를 높이 설정해도 됩니다.)&lt;/p&gt;

&lt;h4 id=&quot;learning-rate-warmup&quot;&gt;Learning rate warmup.&lt;/h4&gt;

&lt;p&gt;초기의 parameters은 랜덤하게 초기화되기 때문에 불안정합니다. 그렇기 때문에 초기에 러닝레이트를 0에서 점점 키워 나가면서 초반 불안정을 해소합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figure1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;zero-감마&quot;&gt;Zero 감마.&lt;/h4&gt;

&lt;p&gt;residual block이 존재하는 모델에서는 bn의 감마를 0으로 초기합니다.(residual block쪽에서만) 그렇다면 초기의 네트워크가 짧아져서 학습이 더욱 빠르다고 합니다. 개인적인 사견이지만 모든 레이어에 일관적으로 초기화 해주는 것은 매우 간단하지만 특정 레이어만 다른 방법으로 초기화 해주는것은 매우 귀찮습니다. 그리고 실험해 본 결과 정확도에서 향상도 없고 학습속도가 그렇게 빨라지는 것도 아니기 때문에 그냥 감마는 1 베타는 0으로 초기화 하는것을 추천 드립니다.&lt;/p&gt;

&lt;h4 id=&quot;no-bias-decay&quot;&gt;No bias decay.&lt;/h4&gt;

&lt;p&gt;bias 는 decay를 안해줘야 오버피팅을 방지한다고 합니다. bn도 마찬가지로 decay를 안해줘야 오버피팅을 방지하는 효과가 있다고 합니다. pytorch에서는 bn의 감마와 베타가 weight, bias라는 이름으로 설정되어 있기 때문에 코드안에서 뭔가 특별한 것을 해줘야 합니다. 코드는 아래의 사진의 첨부했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figure2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;low-precision-training&quot;&gt;Low-precision training&lt;/h3&gt;

&lt;p&gt;GPU가 발전하면서 FP16의 연산도 이제는 가능합니다. 당연히 기존의 FP32보다 메모리 소모량도 적고 연산량도 적기 때문에 FP16에서 연산하고 다시 FP32로 옮겨오면 당연하게 학습속도가 빨라집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrue3.png&quot; alt=&quot;Figrue3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrues4.png&quot; alt=&quot;Figrues4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-tweaks&quot;&gt;Model Tweaks&lt;/h3&gt;

&lt;p&gt;원래의 resnet의 구조는 아래의 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus5.png&quot; alt=&quot;Figrus5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 레즈넷을 보면서 많은 사람들이 7x7 conv를 보면서 3x3 conv 3개로 대체할 수 있는데 왜 했지? 그런 생각을 한번쯤은 모두 해보셨을거라고 생각합니다. 논문 저자는 7x7 conv를 3x3 conv 3개로 대체 하였습니다. 그리고 이미지를 다운샘플링 하는 부분에서 1x1 conv의 stride가 2인데 그렇다면 짝수번째의 핏쳐들은 모두 드랍되는것인데 이 부분도 문제라고 다들 생각하셨을 겁니다.이 논문의 저자는 이 부분도 수정하였습니다. 아래 그림 c 가 7x7을 수정한 부분이고, 그림 d 가 stride 2 의 문제를 수정하고, avgpool을 추가한 구조 입니다. 그림 b는 기존의 제안되었던 stride문제를 해결한 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus6.png&quot; alt=&quot;Figrus6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus7.png&quot; alt=&quot;Figrus7&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-refinements&quot;&gt;Training Refinements&lt;/h3&gt;

&lt;p&gt;정확도를 높이기위한 부분입니다.&lt;/p&gt;

&lt;h4 id=&quot;cosine-learning-rate-decay&quot;&gt;Cosine Learning Rate Decay&lt;/h4&gt;

&lt;p&gt;이 논문에서는 Cosine Learning Rate Decay를 상당히 어렵게 설명합니다. 그냥 직관적으로 생각할때 Cosine Learning Rate Decay 에폭이 늘어 날때마다 당연하게도 러닝레이트는 줄어들어야 하는것은 당연하고, 그것을 매 에폭마다 설정해 주는 것 입니다. 학습 초반에는 수렴하지 않은 상태이기 때문에 큰 러닝 레이트를 가지고 있는 것이 유리하기 때문에 Cosine Learning Rate Decay는 처음에는 에폭이 조금 줄어들다가 수렴하기 시작하면 에폭이 많이 줄어 듭니다(기울기로 생각해 볼때).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus8.png&quot; alt=&quot;Figrus8&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;label-smoothing&quot;&gt;Label Smoothing&lt;/h4&gt;

&lt;p&gt;라벨 스무딩 입니다. 라베을 원핫 백퍼로 정답인 것은 1 아닌것은 0으로 만드는 것이 보통인데, 0.9 ,0.1 이런식으로 스무딩 해주는 것 입니다.
이부분은 잘 이해하지 못했지만, 직관적으로 한 쪽으로 너무 쏠려서 오버피팅이 될 확률이 적어 질 것 이라고 생각하고있습니다. 정확하게 아시는 분은 댓글 부탁 드립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrues9.png&quot; alt=&quot;Figrues9&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;knowledge-distillation&quot;&gt;Knowledge Distillation&lt;/h4&gt;

&lt;p&gt;선생님이 제자를 가르치듯이, 복잡하고 큰 모델이, 작은 모델의 학습을 도와 더 적은 복잡도로 좋은 성능을 내는 방법입니다. 선생 모델로 resnet152를 사용하고 학생모델로 resnet50을 선택했습니다.&lt;/p&gt;

&lt;h4 id=&quot;mixup-training&quot;&gt;Mixup Training&lt;/h4&gt;

&lt;p&gt;데이터 어규멘테이션의 한 방법이라고 생각하시면 됩니다. 두개의 클래스를 적당한 비율로 섞고 확률을 (0.7, 0.3) 이런식으로 표현합니다. 오버피팅을 방지하는 효과가 있고 경험적으로 너무 많이 섞으면 정확도가 오히려 떨어졌으며, 세그멘테이션 테스크에서는 매우 안좋은 결과가 나왔습니다. 개인적인 의견으로는 사용하지 않으셔도 될 것 같고, 네이버에서 이번에 발표한 cutmix 논문을 한번 참고 하시는게 좋으실 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문을 딥러닝 입문 초기에 읽었으면 더 좋았을 것 같다는 생각을 했습니다. 나중에 후배들이 생기면 이 논문을 제일 첫 번째로 권해주고 싶을 정도로, 딥러닝을 많이 해봐야 얻을 수 있는 직관들과 꿀팁들을 알려주는 논문이라 매우 좋았습니다. 논문 구현 코드는 제 깃허브&lt;a href=&quot;https://github.com/seominseok0429/pytorch-warmup-cosine-lr&quot;&gt;https://github.com/seominseok0429/pytorch-warmup-cosine-lr&lt;/a&gt;에 올려 두었습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Bag of Tricks for Image Classification with Convolutional Neural Networks 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">MobileNetV2</title>
      <link href="https://blog.airlab.re.kr/2019/07/mobilenetv2" rel="alternate" type="text/html" title="MobileNetV2" />
      <published>2019-07-22T19:00:00+09:00</published>
      <updated>2019-07-22T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/mobilenetv2</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/mobilenetv2">&lt;p&gt;MobileNetV2 : Inverted Residuals and Linear Bottlenecks 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 MobileNetV2 : Inverted Residuals and Linear Bottlenecks 입니다. 간단하게 한 줄로 이 논문을 소개하자면 모바일이나, 임베디드에서도 실시간을 작동할 수 있게 모델이 경량화 되면서도, 정확도 또한 많이 떨어지지 않게하여, 속도와 정확도 사이의 트레이드 오프 문제를 어느정도 해결한 네트워크 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure4.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 이 논문을 읽기전에 알아두면 좋은 Related Works는 아래 두 논문 입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Xception: Deep Learning with Depthwise Separable Convolutions(https://arxiv.org/abs/1610.02357)&lt;/li&gt;
  &lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(https://arxiv.org/abs/1704.04861)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;개인적으로 이 논문을 읽으면서 기존에 존재했던 “Xception”, “MobileNets” 과의 크게 다른점을 저는 느끼지 못했습니다(그렇기에 이 논문을 제대로 이해 하시려면 앞에서 언급한 두 논문을 읽어보시는걸 추천 드립니다). 이 논문은 저자들이 “Xception”에서 제안했던  Depthwise Separable Convolutions을 그대로 사용합니다. 또한 Depthwise Separable Convolutions이 사용하는 철학, 가설을 그대로 채택합니다. 의식의 흐름대로 읽다보면 Depthwise Separable Convolutions이 뭐지? 하는 질문이 당연히 드실꺼라고 생각합니다.&lt;/p&gt;

&lt;h3 id=&quot;depthwise-separable-convolutions이란&quot;&gt;Depthwise Separable Convolutions이란?&lt;/h3&gt;

&lt;p&gt;Depthwise Separable Convolutions을 아주 간단하게 요약하면 &lt;strong&gt;Depthwise Convolutions + Pointwise Convolutions&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Depthwise Separable Convolutions을 설명하기 전에 기존의 Convolutions을 생각해 봅시다.
&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure5.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에 보이시는 것 처럼 기존의 Convolutions은 채널과 과 필터가 동시에 고려되서 최종 아웃풋을 만듭니다. 하지만 이 논문의 저자는 cross-channels correlation(입력 채널들 사이의 유사도)과 spatial correlation(필터와 하나의 특정 채널 사이의 관계)이 완전하게 독립적이기 때문에 &lt;strong&gt;채널과 필터를 따로 분리해서 학습&lt;/strong&gt;을 진행해도 문제가 없다고 주장합니다. 실제로 연상량을 계산해보면 Traditional convolutions은 &lt;strong&gt;입력 이미지의 크기x입력 이미지의 채널x 커널사이즈 제곱x아웃풋채널&lt;/strong&gt; 이지만 Depthwise Separable Convolutions의 연산량은 &lt;strong&gt;입력 이미지의 크기x입력 이미지의 채널x (커널사이즈 제곱+아웃풋채널)&lt;/strong&gt; 이기 때문에 &lt;strong&gt;8~9배&lt;/strong&gt; 정도 연산량이 줄어듭니다.(커널사이즈는 3 이라고 가정합니다)&lt;/p&gt;

&lt;p&gt;다음으로 이 논문에서 주장하는 Linear Bottlenecks 입니다.&lt;/p&gt;

&lt;h3 id=&quot;linear-bottlenecks-이란&quot;&gt;Linear Bottlenecks 이란?&lt;/h3&gt;

&lt;p&gt;지금부터 조금 어려운 이야기를 직관적으로 쉽게 풀이하겠습니다(논문에서도 직관이라는 단어를 많이 사용합니다). 우선 manifold라는 말을 알고 있으셔야 합니다.
manifold란 어떤 이미지의 차원들이 존재하는 공간이라고 생각하시면 됩니다. 이 논문에서는 Manifold의 가설을 언급합니다(It has been longassumed  that  manifolds  of  interest  in  neural  networkscould be embedded in low-dimensional subspaces.). manifold 가설은 고차원의 정보는 사실 저차원으로 표현 가능하다는 것입니다. 예를 들어서 설명하면, 실제 세상에 존재하는 모든 사물들은 3차원 이라고 이야기를 하지만 사람들은 실제로 사물을 구분할 때는 2차원 정보를 받아들여 사물을 구분할 수 있다는 것 입니다. 즉 고차원 정보는 사실 저차원 정보로도 충분히 구분 할 수 있다는 것 입니다.&lt;/p&gt;

&lt;p&gt;지금까지 Manifold에 대하여 설명한 이유는 이 논문에서 Linear Bottlenecks을 만들때 1x1의 pointwise Convolutions을 하여 차원수를 줄이기 때문입니다. &lt;strong&gt;Manifold의 가설 그대로 고차원의 채널은 사실 저차원의 채널로 표현할 수 있다&lt;/strong&gt; 라는 논리 전개 입니다.(채널을 과도하게 줄이면 안됩니다. 예를들어서 사람은 3차원의 정보를 2차원으로 구분하지만 1차원으로는 구분 못하는 것과 같습니다.)&lt;/p&gt;

&lt;p&gt;Linear Bottlenecks에서 주장하는 또 다른 하나는 ReLU는 필연적으로 정보 손실을 야기하기 때문에 어떤 특별한 작용을 해줘서 그 정보손실을 방어해야 한다는 것 입니다. 이제 그 특별한 작용에 대하여 말씀드리겠습니다.&lt;/p&gt;

&lt;p&gt;시작하기 전에 가장 간단히 한줄로 요약하면 &lt;strong&gt;“채널수가 충분히 많으면 ReLU를 사용해도 중요 정보는 보존된다!”&lt;/strong&gt; 입니다. 이 문장을 계속 상기시키면서 글을 읽으시면 이해하시는데 도움이 되실 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure8.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 보이시는 그림처럼 채널이 1인 데이터가 ReLU를 지나면 중요 정보가 삭제 될 수 도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure9.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 위에 보이시는 그림처럼 채널이 2 인 데이터가 ReLU를 지나면 중요 정보가 삭제 되더라도 다른 채널에서는 아직까지 존재할 가능성이 채널이 많으면 많을수록 높기 때문에 &lt;strong&gt;채널이 많을때 ReLU를 사용하면 괜찮다는 것&lt;/strong&gt; 입니다.(어차피 나중에 전부 합쳐져서 예측하기 때문에)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure6.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 보이시는 그림은 이 주장을 실험적으로 증명한 것 입니다. 차원을 2, 3, 5, 15, 30 을 각각쓰고 ReLU를 쓰고 원래대로 복원하였습니다. 그림에서 보이는 것 과 같이 차원이 적을때는 ReLU를 쓰면 정보가 손실되어 원본 영상을 복원할 수 없지만 차원을 충분히 늘리고 ReLU를 쓰면 15, 30 과 같이 잘 복원 할 수 있다는 것 입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 Linear Bottlenecks은 ReLU를 적용하지 않습니다. 위에서 말씀드린것과 같이 차원이 매우많이 축소된 상태이기 때문에 ReLU를 사용하면 정보손실이 있을 수도 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure3.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험적으로 증명을 했는데, Linear Bottlenecks에 ReLU6을 썻을때와 안썻을때의 정확도의 차이 입니다.(ReLU6를 사용하는 이유는 연산량에 있어서 이득을 볼 수 있다고 알아보았는데 정확하진 않습니다. 혹시 정확한 이유를 아시는분은 댓글 부탁드립니다.) 또 shortcut 위치에 대한 실험도 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;inverted-residuals이란&quot;&gt;Inverted Residuals이란?&lt;/h3&gt;

&lt;p&gt;앞에서 설명드린 Depthwise Separable Convolutions과 Linear Bottlenecks을 결합하면 Inverted Residuals 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure1.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존의 Residuals을 거꾸로 뒤집은 모양이라 Inverted Residuals이라고 부르는것 같습니다. 앞에서 언급한 논리되로 ReLU를 사용해야 하기 때문에 채널을 확장(pointwise Convolutions)하고 Depthwise Convolutions을 진행합니다. 또 Linear Bottlenecks에서 대로 다시 채널수를 줄입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure2.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;stride가 1일때는 shortcut이 있지만 strdie가 2 일때는 shortcut 이 없습니다. 이유는 논문에서 설명하지 않고 있지만 이미지의 크기가 줄어들때 정보의 선형성이 보장되 않기 때문이라고 추측하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;memory-efficient-inference&quot;&gt;Memory Efficient Inference&lt;/h3&gt;

&lt;p&gt;논문에서는 gpu에서 내부 메모리와 외부 메모리가 있기 때문에 내부로 올릴때의 크기과 나갈때의 크기만 중요하기때문에 메모리 스왑적인 부분에서 봤을때도 이 논문에서 제안한 Inverted Residuals구조가 효율적이라고 주장하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;논문리뷰 끝입니다. 논문의 Conclusions은 개인적인 견해가 필요하고, 이 부분은 이 글을 읽고 있는 독자 여러분이 편견없이 논문을 읽으면 좋겠다고 생각하여 리뷰하지 않겠습니다.&lt;/p&gt;

&lt;p&gt;코드는 cifar10에 적용한 것이고 &lt;a href=&quot;https://github.com/seominseok0429/cifar10-mobilenetv2-pytorch&quot;&gt;https://github.com/seominseok0429/cifar10-mobilenetv2-pytorch&lt;/a&gt; 에 배포해 두었습니다.&lt;/p&gt;

&lt;p&gt;끝까지 읽어 주셔서 감사합니다!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">MobileNetV2 : Inverted Residuals and Linear Bottlenecks 리뷰</summary>
      

      
      
    </entry>
  
</feed>
