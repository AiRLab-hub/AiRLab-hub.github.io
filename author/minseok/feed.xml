<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://blog.airlab.re.kr/author/minseok/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.airlab.re.kr/" rel="alternate" type="text/html" />
  <updated>2019-12-11T13:45:59+09:00</updated>
  <id>https://blog.airlab.re.kr/author/minseok/feed.xml</id>

  
  
  

  
    <title type="html">AiRLab. Research Blog | </title>
  

  
    <subtitle>Artificial intelligence and Robotics Laboratory</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
      <link href="https://blog.airlab.re.kr/2019/11/manifold-mixup" rel="alternate" type="text/html" title="Manifold Mixup: Better Representations by Interpolating Hidden States" />
      <published>2019-11-22T11:00:00+09:00</published>
      <updated>2019-11-22T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/manifold-mixup</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/manifold-mixup">&lt;h3 id=&quot;manifold-mixup-better-representations-by-interpolating-hidden-states-리뷰&quot;&gt;Manifold Mixup: Better Representations by Interpolating Hidden States 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Manifold Mixup: Better Representations by Interpolating Hidden States”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Manifold Mixup: Better Representations by Interpolating Hidden States은(이하 Manifold Mixup) 2019ICML에 통과된 논문으로 카테고리는 Classification, Data Augmentation 입니다. 또한 딥러닝의 대가 Yoshua Bengio가 저자로 참여된 논문입니다. Manifold Mixup은 Mixup 논문에서 영감을 얻었으며, 인풋으로 들어오는 이미지 뿐만아니라, hidden states사이에서도 mixup을 하자는게 주된 내용이고, CIFAR100에서는 mixup을 능가하는 성능을 보입니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Manifold Mixup을 이해하기 위해서는 우선 매니폴드 가 무엇인지 먼저 아셔야 합니다. 저는 이 논문을 이해할 정도로만 매니폴드를 설명할 것 이니, 혹시 더 궁금하시다면, &lt;a href=&quot;https://www.youtube.com/watch?v=o_peo6U7IRM&amp;amp;t=4692s&quot;&gt;https://www.youtube.com/watch?v=o_peo6U7IRM&amp;amp;t=4692s&lt;/a&gt; 이활석님의 매니폴드 설명을 참고하시면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img2.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림이 매니폴드를 나타내는 그림 입니다. 매니폴드란 데이터가 사는 공간입니다. 위의 사진처럼 한 매니폴드가 있으면, 그 위에 모든 데이터를 표현할 수 있고, 그림처럼 똑같은 4 이더라도 다 다른 위치에 있습니다. 또한 사람눈으로는 매우 닮아있는 4 라고 할지라도 매니폴드상 거리가 멀수도, 가까울 수도 있습니다. 직관적으로 딥러닝은 이러한 매니폴드에서 공통된 특성을 가로지르는 하나의 선을 찾는 것 이라고 생각하셔도 될 것 같습니다.
이제 매니폴드를 간략하게 알았으니 다음 설명으로 넘어가겠습니다.&lt;/p&gt;

&lt;p&gt;논문 저자는 친절하게 Manifold Mixup을 한줄로 요약해 줍니다. “Manifold Mixup improves the hidden representations and decision boundaries of neural networks at multiple layers.” 즉 매니폴드 믹스업이란, “다중 레이어가 있을때 그냥 믹스업 처럼 인풋만 이미지를 섞어버리면 불공평하니, 다중 레이어 모든 핏쳐맵에서 섞자!” 입니다. 실제로 이 말이 이 논문의 전부이며 앞으로는 이 말을 증명하고 실험하는 과정입니다. 또한 이 논문은 라벨 스무딩의 효과가 있다. 라고 생각하시면 이해하기 편하실것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img3.PNG&quot; alt=&quot;Figure1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img4.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림들은 저자가 2D spiral dataset으로 시각화한 그림입니다. 상단의 그림의 왼쪽은 아무것도 사용하지 않은 base이며, 상단의 오른쪽은 Manifold Mixup 입니다. 그림에서 보이는것과 같이 두개의 정확도는 비슷하지만, base는 오버피팅이 생긴 것을 한 눈에 알수 있고, Manifold Mixup은 오버피팅이 일어나지 않은 것을 볼수 있습니다. 또한 아래의 그림은 기존 유명 regularizers 들과의 성능을 정성적으로 비교한 것이며, 정성적으로는 Manifold Mixup이 좋아 보이나, “Manifold Mixup이 다른 타 regularizers들을 능가하는 방법이다!” 라고 생각하지 마시고, 그냥 저런 유명 방법들과 견줄만한 방법이다. 정도로만 생각하시면 좋을것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;manifold-mixup&quot;&gt;Manifold Mixup&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img5.PNG&quot; alt=&quot;Figure1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img6.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 수식들은 Manifold Mixup을 이해하기 쉽게 저자가 수식으로 표현한 것이며, 상단의 수식은 매니폴드에 존재하는 hidden states를 섞고, label도 그 수치만큼 섞겠다는 이야기 입니다. 아래의 수식은 전체 학습 프로세스가 어떻게 작동되는지 설명한 수식이며, 학습이 진행되면 input을 포함한 hidden states에 Mixup이 동작한다는 수식입니다. 또한 Beta는 Beta분포를 따르는 것을 의미합니다. Beta분포를 사용한 이유는 랜덤하게 뽑으면 섞이는 두 대상이 일정하게 섞일 확률이 높기 때문에, 한쪽이 더 우세하게 섞기 위하여 Beta분포를 사용한 것 입니다.&lt;/p&gt;

&lt;h3 id=&quot;empirical-investigation-of-flattening&quot;&gt;Empirical Investigation of Flattening&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img7.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문 저자는 Flattening 하는 것이 왜 좋은가를 실험하기 위하여 MNIST데이터 셋에서 Singular Value Decomposition (SVD)을 통하여 실험합니다.
SVD를 직관적으로 설명하면 선형대수에서 배우는 특이값 분해로 같은 이미지에서 투영변환, 스케일변환, 회전변환을 하였을때 딥러닝 관점에서 augmentation이 된 데이터들 즉 매니폴드를 지나가는 직선의 거리 라고 생각하시면 될 것 같습니다. 이러한 SVD 값이 Maniflod Mixup이 가장 작았습니다. 위의 그림은 이것을 시각화 한 것이고, Manifold Mixup을 사용한 방법에서 MNIST 데이터들이 잘 분류된걸 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img8.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 논문저자는 위의 그래프와 같이 다양한 어규멘테이션에서 실험을 합니다. 논문저자가 주장한대로 Maniflod Mixup을하면 매니폴드를 직관하는 선을 찾기 쉬워져 같은 데이터라면 결과가 좋아야 합니다. 위 그래프도 논문저자가 주장한대로 매니폴드 믹스업은 다양한 데이터에서 강인합니다. 하지만 아쉬운 점은 이 논문 저자는 극한의 튜닝을 했습니다. epoch를 2000까지 돌리는둥 ,,, 최소 1200epoch를 학습합니다.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-22-manifold-mixup/img9.PNG&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 실험 입니다. 논문 저자는 CIFAR10, 100 TINY Imagenet에서 실험을 하였습니다. 위의 표는 그 결과 입니다. 보이시는것과 같이 CIFAR10,100에서는 베타분포를 만들때 사용하는 알파값이 2일때 성능이 가장 좋고 Manifold Mixup에서 성능이 가장 좋습니다. 하지만 TINY Imagenet에서는 mixup보다 성능이 낮은데, 아마 tiny라 그런것이고, full imagenet이면 manifold mixup이 더 좋습니다. 그 결과는 cutmix논문을 참고하시면 확인하실수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문은 저의 직관과 비슷하여 재미있게 읽었던 논문 입니다. 하지만 epoch를 1500까지 맞춰줘야 논문 성능을 구현 할수 있는점(저는 1200에 구현했습니다 ㅎㅎ) 하이퍼파라메터를 공개하지 않은점. 수학적으로 너무 무거운점이 아쉬웠습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Manifold Mixup: Better Representations by Interpolating Hidden States 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Shake-Shake regularization</title>
      <link href="https://blog.airlab.re.kr/2019/11/shake-shake-regularization" rel="alternate" type="text/html" title="Shake-Shake regularization" />
      <published>2019-11-16T11:00:00+09:00</published>
      <updated>2019-11-16T11:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/11/shake-shake-regularization</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/11/shake-shake-regularization">&lt;h3 id=&quot;shake-shake-regularization-리뷰&quot;&gt;Shake-Shake regularization 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Shake-Shake regularization”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 크게보면 Data augmentation에 속하는 논문 입니다. 지금까지는 Data augmentation을 이미지에 했다면, Shake-Shake regularization 논문은 Internal Representations에 augmentation을 합니다. (Internal Representations은 feature map과, weight를 의미합니다.) 이런 Internal Representations augmentation을 통하여 그 당시 CIFAR10, CIFAR100에서 state of the art를 달성합니다.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img1.PNG&quot; alt=&quot;Figure1&quot; /&gt;
위 그림과 같이 컴퓨터는 사람과는 다르게 물체를 일정 규칙이 있는 스칼라 값으로 인식합니다. 그렇다면 컴퓨터 입장에서는 feature map 그리고 이미지 는 일정 규칙이 있는 데이터이기 때문에 현재처럼 이미지에서만 Data augmentation을 하지말고, Internal Representations에도 Data augmentation을 해주자고 주장합니다. 또한 Internal Representations에 Data augmentation을 해주면 stochastically “blending” 효과가 있다고 주장합니다.&lt;/p&gt;

&lt;h2 id=&quot;model-description-on-3-branch-resnets&quot;&gt;Model description on 3-branch ResNets&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img2.PNG&quot; alt=&quot;Figure2&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img3.PNG&quot; alt=&quot;Figure3&quot; /&gt;
논문 저자는 Shake-Shake regularization을 적용하기 간편한 ResNet 구조에서 실험을 하고, 이해를 돕기 위하여 위와 같은 간단한 수식을 말합니다. 위에 보이는 수식 1은 일반적은 resnet입니다. W는 weight이고, x는 텐서, F는 residual function입니다. 논문 저자는 수식 2와 같이, F앞에 일정 α(0과 1 사이의 랜덤한 값)을 곱해줘 Internal Representations에 Data augmentation을 해주는 효과를 냅니다.&lt;/p&gt;

&lt;h2 id=&quot;training-procedure&quot;&gt;Training procedure&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/cover.PNG&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 shake-shake-regularization의 전체 학습 절차 입니다. 학습의 forward 부분에서도 α[0~1] 사이의 값을 곱해줘서 data augmentation 효과를 주고, backward 부분에도 β[0~1]의 값을 곱해줘서 기존의 연구 되었던 gradient noise를 주는 방식을 gradient augmentaion으로 대체해 줄 수 있다고 논문 저자는 주장합니다. 또한 테스트시에는 0.5로 값을 고정해 줍니다.&lt;/p&gt;

&lt;h2 id=&quot;cifar-10-cifar-100&quot;&gt;CIFAR-10, CIFAR-100&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img4.PNG&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 본 논문의 저자는 다양한 실험을 진행하였습니다. Forward pass는 Even / Shake, Backward pass는 Even / Shake / Keep, Mini-batch update rule 은 Image / Batch로 진행하였습니다. Even은 α값을 0.5로 고정하는 방법이고, Shake는 α값은 0~1사이의 값을 랜덤하고 곱해줍니다. Backward의 Kepp은 Forward에 사용한 α값을 그대로 사용하는것 입니다. 또한 Image는 미니배치에서 모든 이미지에 α,β 를 적용하는것이고, Batch 는 미니 배치 단위로 다 같은 α,β를 적용하는것 입니다. 위 그림과 같이 shake shake image가 가장 성능이 좋았고, Even shake batch가 가장 성능이 안 좋았습니다. shake shake image 가 가장 많은 augmentation을 적용한 것이니 당연한 결과라고 생각하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img5.PNG&quot; alt=&quot;Figure6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 배치사이즈를 128에서 32로 줄인 다음 CIFAR100에서 실험한 테이블 입니다. 위에 보이는 결과와 같이 배치사이즈가 줄어 들때는 S S I 조합 보다는 S E I 조합이 조금 더 경쟁력 있다는 것을 알 수 있습니다. 논문저자도 이와 관련해서 해석을 하지 못했습니다. 저 또한 그 이유를 해석하는데 어려움이 있어 혹시 아시는분은 댓글 부탁 드립니다. ㅠㅡㅠ&lt;/p&gt;

&lt;h2 id=&quot;comparisons-with-state-of-the-art-results&quot;&gt;Comparisons with state-of-the-art results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img6.PNG&quot; alt=&quot;Figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 표는 기존의 state-of-the-art 결과와 비교한 표 입니다. 보이시는 것과 같이 저자가 주장한 방법이 state-of-the-art를 달성하였습니다. CIFAR10 에서는 S S I 가 CIFAR100에는 S E I 가 가장 좋았습니다. 이 결과를 분석해보면, gradient에 β를 곱해주는 방법은 좋은 성능을 보장하지 못합니다. 제 생각에는 저자가 주장하는 방법은 gradient noise방법을 완벽하게 대체하지는 못하는 것 같습니다. 또한 논문 저나는 imagenet실험을 하지 않아 많이 아쉽습니다.(제 생각에는 imagenet에서는 잘 안될것 같긴 합니다.)&lt;/p&gt;

&lt;h2 id=&quot;correlation-between-residual-branches&quot;&gt;Correlation between residual branches&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img7.PNG&quot; alt=&quot;Figure8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 논문저자는 Shake-Shake regularization을 분석합니다. residual branches사이의 공분산을 구하여 서로의 관계성을 계산합니다. 위 그림은 feature map을 펼친후 나온 스칼라 값들로 관계성을 구한것 입니다. 거의 아무것도 안해준 조합인 Even Even Batch 와 거의 모든 augmentation을 해준 Shake Shake Image 조합을 비교 하였습니다. 그 결과, Shake Shake Image 조합에서 residual branches 값들 사이의 관계성이 작게 나왔으므로, Shake-Shake regularization을 해준다면 overfitting이 일어날 확률을 낮춰줄 수 있음을 보여줍니다.&lt;/p&gt;

&lt;h2 id=&quot;후기&quot;&gt;후기&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-11-16-shake-shake-regularization/img8.PNG&quot; alt=&quot;Figure9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 사실 이 논문 저자는 실험을 상당히 많이 했습니다. BatchNorm을 사용하지 않거나, skip connection을 제거하는등 많은 실험을 하였습니다. 하지만 본 리뷰에서 그런 실험을 리뷰하지 않는 이유는 너무나도 당연하기 때문입니다. BatchNorm을 사용하면 성능저하가 있고, skip connection을 제거하면 성능의 약간의 향상은 있지만 유의미 하지는 않습니다. 또한 그 행동이 CIFAR10,100이여서 상승한 것이지 imagenet이었다면 어떻게 될지 모른다는 생각을 했습니다. 이 논문 자체로는 유의미하게 좋은 논문이라고 생각들진 않지만, shake drop, mixup, cutmix등 다른 augmentation 논문을 읽을때 큰 도움이 된다고 생각이 됩니다. (또한 이 논문이 쓸 내용이 없을떄 어떻게 논문을 꽉 꽉 채울수 있나 좋은 교본으로 느껴졌습니다 ㅋㅋㅋ)&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Shake-Shake regularization 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">R(2+1)D</title>
      <link href="https://blog.airlab.re.kr/2019/09/R(2+1)D" rel="alternate" type="text/html" title="R(2+1)D" />
      <published>2019-09-18T19:00:00+09:00</published>
      <updated>2019-09-18T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/09/R(2+1)D</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/09/R(2+1)D">&lt;h3 id=&quot;a-closer-look-at-spatiotemporal-convolutions-for-action-recognition-리뷰&quot;&gt;A Closer Look at Spatiotemporal Convolutions for Action Recognition 리뷰&lt;/h3&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“A Closer Look at Spatiotemporal Convolutions for Action Recognition”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Facebook에서 “Du Tran”씨가 2018년도에 cvpr에서 발표한 논문입니다. 이 논문은 3D conv를 공간정보와 시간정보로 나눠 conv하는 구조를 처음 제안하였고, 다양한 spatiotemporal conv 방법들을 비교합니다. 또한 Sports-1m, kenetics, ucf101, hmdb51에서 state-of-the-art를 달성합니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;AlexNet이 처음공개된 이후로 이미지 인식 분야는 multi-scale convolutions, residual learning, dense connections 과 같은 많은 진보가 있었습니다. 반면에 비디오 분야에서는 “새롭다” 하는 큰 발전이 없습니다. 현재의 state-of-the-art인 I3D또한 bset hand-crafted 방법인 iDT랑 비교하였을때 그렇게 놀랄만한 진보는 아닙니다.(저의 의견이 아닙니다. 논문저자의 의견입니다 ㅎㅎ.) 또한 image-based 2D CNN의 Sports-1M에서의 성능은 state-of-the-art랑 비슷합니다.(저자는 3D conv의 진보는 없고, image-based 방법이랑 별 차이가 없다고 강조하는것 같습니다.) 이러한 결과에 기초하여, 시퀀스의 정적 프레임에 이미 포함 된 강력한 행동 클래스 정보 때문에, 시간적 추론이 정확한 행동 인식에 필수적이지 않다고 생각 할 수도 있습니다. 논문저자는 시간적 정보가 행동인식에 필수적이지 않다는 생각을 제거하기 위하여, 다양한 실험을 진행합니다. 논문저자들의 실험에 의하면, 3D ResNets이 Sports-1M 및 Kinetics과 같은 action recognition benchmarks에서 훈련되고 평가 될 때 동일한 깊이에 대해 2D ResNets보다 월등히 뛰어남을 보여줍니다. 이러한 실험결과에 기초하여, 논문 저자들은 2D cnn과 3d cnn 사이의 R(2+1)D를 제안합니다. (2+1)D의 의미는 3D conv를 두개의 개별적인 2D 공간 conv와 1D 시간 conv로 분해하는것 입니다. 첫번째 장점은 파라메터가 증가하지 않고, relu를 두번 사용할 수 있기때문에 비선형성을 증가시킬수 있습니다. 두번째 장점은 optimization하는 것을 도와주는 것입니다. 아래의 그림은 똑같은 구조여도 R(2+1)D가 더 최적화가 잘된다는 그래프 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;convolutional-residual-blocks-for-video&quot;&gt;Convolutional residual blocks for video&lt;/h2&gt;

&lt;p&gt;이 논문에서 사용하는 residual blocks은 전부 “vanilla” residual blocks을 사용합니다. 아래의 표현에서 3 x L x H x W,라는 표현들이 나오는데, 3은 rgb 3채널, L을 프레임 길이, H는 높이, W 넓이 입니다.&lt;/p&gt;

&lt;h4 id=&quot;r2d-2d-convolutions-over-the-entire-clip&quot;&gt;R2D: 2D convolutions over the entire clip&lt;/h4&gt;

&lt;p&gt;R2D는 2D CNN방법이고, 3L x H x W 입니다. 즉 프레임의 길이를 하나의 차원으로 보는것이 아니라, 채널로 보는것 입니다. 1프레임당 3채널씩 할당되는것 입니다.&lt;/p&gt;

&lt;h4 id=&quot;f-r2d-2d-convolutions-over-frames&quot;&gt;f-R2D: 2D convolutions over frames&lt;/h4&gt;

&lt;p&gt;f-R2D는 2D CNN방법이고, 프레임을 채널로 쌓아서 사용하는 방법이 아니라, 전체의 프레임을 개별적으로 2D CNN 하는 방법 입니다.&lt;/p&gt;

&lt;h4 id=&quot;r3d-3d-convolutions&quot;&gt;R3D: 3D convolutions&lt;/h4&gt;

&lt;p&gt;R3D는 가장 일반적은 3D conv 방법이고, 3 x L x H X W 입니다. 프레임을 시간축을 하나 추가하여 stack하고 3D conv를 하는것 입니다.&lt;/p&gt;

&lt;h4 id=&quot;mcxand-rmcx-mixed-3d-2d-convolutions&quot;&gt;MCxand rMCx: mixed 3D-2D convolutions&lt;/h4&gt;

&lt;p&gt;한 가지 가설은 모션 모델링(즉, 3D convolution)이 초기 계층에서 특히 유용할 수 있는 반면, 후반 계층에서는 필수적이지 않는다는 가설 입니다. 학습초기에는 3D ResNets (R3D)를 초기에 사용하고, 후반에는 2D conv를 사용합니다. 또 그 반대는 학습초기에는 2D conv를 사용하다가 후반에는 3D conv 사용하는 방법입니다.&lt;/p&gt;

&lt;h4 id=&quot;r21d-21d-convolutions&quot;&gt;R(2+1)D: (2+1)D convolutions&lt;/h4&gt;

&lt;p&gt;또 이 방법은 논문저자가 주장하는 방법인데, full 3D conv를 2D의 공간 conv와 1D 시간 conv로 분해합니다. 논문 저자는 3D convolutional filters(N×t×d×d)를  N x 1 x d x d 와 M x t x 1 x 1로 분해 합니다. 그렇게하면 기존의 3D conv와 동일하면서, 비선형성을 증가시키고 최적화에 도움을 줍니다. 아래의 그림은 앞에서 언급한 다양한 구조의 그림입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;이 논문의 저자는 Resnet을 사용하기 위하여, 오버피팅이 나타나지 않기 위하여 충분히 큰 데이터셋인 Sport 1M, Kinetics을 사용합니다. 또한 다른 작은 데이터셋에서 잘 작동하나 궁금하기 때문에, UCF101,HMDB51에 transfer learning을 적용합니다.&lt;/p&gt;

&lt;h4 id=&quot;experimental-setup&quot;&gt;Experimental setup&lt;/h4&gt;

&lt;p&gt;모든 프레임의 사이즈는 112 x 112로 사용하였고,자세한 구조는 아래의 그림을 봐주시면 됩니다. 다양한 디테일이 논문에 언급되는데 더 자세하게 알고 싶으시면 논문을 읽어보시는걸 추천드립니다.&lt;/p&gt;

&lt;h4 id=&quot;comparison-of-spatiotemporal-convolutions&quot;&gt;Comparison of spatiotemporal convolutions&lt;/h4&gt;

&lt;p&gt;R2D부터 R(2+1)D까지 모든 net을 clip단위와 video단위로 평가합니다. 아래의 첨부한 그림을 보면, 이 논문저자가 주장한 R(2+1)D 방법은 R3D방법과 파라메터 차이는 나지 않지만 정확도는 모든 부분에서 가장 높은걸 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img3.png&quot; alt=&quot;Figure3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;action-recognition-with-a-34-layer-r21d-net&quot;&gt;Action recognition with a 34-layer R(2+1)D net&lt;/h4&gt;

&lt;p&gt;이 부분에서는 다양한 데이터셋과 기존의 방법들에서 실험한 결과들을 정리합니다. 아래의 표를 확인해 보시면 i3d-two-stream방법을 제외하고는 모두 1등을 달성하였습니다. i3d-two-stream방법에 비하여 정확도가 낮은 이유는 옵티컬 플로우를 뽑는 방법이 i3d는 tvl1 방법이지만, 이논문의 저자는 Farneback’s 방법을 사용하기 때문이라고 논문 저자는 추측하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img4.png&quot; alt=&quot;Figure4&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-09-19-RD/img5.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문을 읽으면서 I3D의 논문과 매우 비슷하다고 생각했습니다. 하지만 이 논문은 hand-craft논문들도 잘 정리해 줘서 좋았고,Farneback 방법과 tvl1방법의 차이라던지, 다양한 기술들에 대하여 잘 기술한것 같습니다. 하지만 제가 개인적으로 생각할때 이 논문 또한 뭐 대단한 하게 새로운 아이디어라고 생각하지 않습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">A Closer Look at Spatiotemporal Convolutions for Action Recognition 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Group Normalization</title>
      <link href="https://blog.airlab.re.kr/2019/08/Group-Normalization" rel="alternate" type="text/html" title="Group Normalization" />
      <published>2019-08-21T05:00:00+09:00</published>
      <updated>2019-08-21T05:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/08/Group-Normalization</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/08/Group-Normalization">&lt;p&gt;Group Normalization 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Group Normalization”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 Yuxin Wu와 Kaiming He 씨가, batch의 크기가 어쩔수 없이 작아야 하는 상황(detection, segmentation and video)에서 batch norm의 한계점을 느끼고 이를 개선하는 방법을 제안 합니다.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Batch Normalization은 딥러닝에서 모델을 설계할 때 필수요소처럼 여겨지고 있습니다. 하지만 detection, segmentation, video와 같이 메모리 소비 때문에 어쩔 수 없이 batch의 크기가 제약될 때 Batch Normalization의 오류는 빠르게 증가합니다. 이 논문에서는 Batch Normalization을 대체할 Group Normalization을 제안하고, 다양한 task에서 이를 실험하고 결과를 보여줍니다. 당연히 결과는  Group Normalization 이 좋습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;related-work&quot;&gt;Related Work&lt;/h3&gt;

&lt;p&gt;Batch Normalization의 치명적인 단점을 해결하기 위한 노력은 과거에도 많이 있었습니다. Batch Normalization은 말 그대로 Batch 단위로 Normalization 하는것 인데 batch 크기에 영향을 많이 받습니다. 그렇기 때문에 논문에서 소개하는 Related Work은 대부분이 채널 단위로 Normalization을 합니다. 아래의 두 식은 일반적으로 사용하는 평균과 표준편차를 구하느 식 입니다. μ 는 평균이고 σ는 표준편차 입니다. i는 feature 입니다. 예를들어  2D 이미지에서 i는 (iN,iC,iH,iW) 이고 4개의 백터(N, C, H, W)를 가지고 있습니다. ε은 아주 작은값 입니다. X  와 평균 X’이 같으면 0이 되기 때문에 이를 방지하기 위한 값 입니다. S 는 평균과 표준편차가 계산된 픽셀의 집합입니다.&lt;/p&gt;

&lt;p&gt;앞으로 소개드릴 LN,IN,GN은 BN과 마찬가지로 , 학습 가능한 파라메터 γ,β 가 존재하고, 감마는 스케일 베타는 쉬프트 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/formul.png&quot; alt=&quot;formul1&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/formul1.png&quot; alt=&quot;formul2&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;layer-norm&quot;&gt;Layer Norm&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Layer Normalization은 Feature 차원 Normalization 입니다. batch 단위로 Normalization 하는 것이 아니라 데이터의 한 이터레이션 마다 평균과 표준편차를 구해주는 것 입니다. 아래의 그림과 식을보면 더 이해가 잘 되실 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/formul2.png&quot; alt=&quot;formul3&quot; /&gt;
&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure3.png&quot; alt=&quot;Figure3&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;instance-norm&quot;&gt;Instance Norm&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure4.png&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instance Normalization은 Layer Norm에서 각 채널마다 Normalization 해주는 방법 입니다. style transfer을 위해 고안된 방법이기 때문에 style transfer에서 BN을 대체하여 많이 사용하고, real-time generation에 효과적 이라고 알려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/formul4.png&quot; alt=&quot;formul4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;group-normalization&quot;&gt;Group Normalization&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure5.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Group Normalization은 Layer Norm과 Instance Norm의 중간쯤 이라고 생각하시면 이해하기 편하실꺼 같습니다. 채널을 그룹지어서 그룹단위로 Normalization 하는 방법입니다. 만약 그룹이 채널 전체면 Layer Norm이 되는것이고, 그룹이 채널 하나면 Instance Norm 이 되는것 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/formul5.png&quot; alt=&quot;formul5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;코드로 구현하기 매우 간단합니다.  아래는 pytorch로 구현한 코드 입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;GroupBatchnorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GroupBatchnorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group_num&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 전체 채널을 나눌 그룹 숫자입니다.&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 학습가능한 파라메터 gamma&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parameter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 학습가능한 파레메터 beta&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 0방지&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;group_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 그룹으로 묶고&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 평균&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 표준편차&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 원래대로 돌리기.&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Group의 개수 또한 유동적으로 변경되는 값입니다. 이 논문에서는 그룹을 32개 만드는게 가장 좋았다고 합니다. 아래 그림에 보이시는것 처럼 그룹이 1 개이면 LN과 같고 그룹당 채널이 1 이면 IN과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure6.png&quot; alt=&quot;Figure6&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;Experiments&lt;/h4&gt;

&lt;p&gt;아래 그림은 batch 크기가 32 images/GPU에서 BN, LN, IN, GN 의 train error 와 val error의 비교 입니다. 잘 알려져 있는것 처럼 BN이 가장 좋은 성능을 보입니다. 하지만 GN도 BN과 비교해서 그래서 크게 차이가 나는게 아닙니다. 또한 왼쪽 train error 그림을 보시면 GN이 조금더 train error가 작습니다. 이걸로 보아 GN이 최적화 하는 능력이 더 좋다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure7.png&quot; alt=&quot;Figure7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 그림은 batch 크기가 변함의 따라 BN과 GN의 성능 차이를 나타낸 그래프 입니다. BN은 batch 크기가 작아지면 작아질 수록 정확도가 매우 떨어졌으나, GN은 batch 크기가 변함의 따라 정확도의 차이가 거의 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-21-Group-Normalization/Figure8.png&quot; alt=&quot;Figure8&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;GN은 옛날부터 관심있던 논문인데 이제서야 제대로 읽었습니다.! 이 논문역시 다양한 Normalization 방법을 잘 정리해주고, 의식에 흐름대로 궁금증을 다 실험하고 풀어줘서 너무 좋았습니다.!! Experiments는 제가 중요하다고 생각하는 부분만 설명하고 , 나머지는 생략했기 때문에 더 자세한 내용을 알고 싶으신 분들은 본 논문을 참고하시기 바랍니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Group Normalization 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
      <link href="https://blog.airlab.re.kr/2019/08/kinetics-dataset" rel="alternate" type="text/html" title="Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset" />
      <published>2019-08-03T17:00:00+09:00</published>
      <updated>2019-08-03T17:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/08/kinetics-dataset</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/08/kinetics-dataset">&lt;p&gt;Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 딥마인드에서 발표한 kinetics dataset 논문입니다. 이 논문은 데이터셋 논문임에도 action recognition의 역사와 방향성을 제시해주기 때문에 action recognition에 입문하시는 분들이라면 꼭 읽어 보시는 걸 추천해 드립니다.&lt;/p&gt;

&lt;p&gt;기존의 action recognition 문제에서 UCF101 과 HMDB51 같은 규모가 작은 데이터셋은 좋은 성능을 내기 어려웠습니다. 그래서 이 논문 저자는 ImageNet처럼 action recognition에도 빅 데이터셋이 필요성을 느끼고 Kinetics 데이터셋을 만듭니다. Kinetics 데이터셋은 400개의 클래스들 과  한 클래스당 400개가 넘는 clips가 존재하는 빅데이터 셋입니다.(현재는 클래스 700 버전도 업로드 되었습니다.) Kinetics 데이터셋을 학습시킨 파라메터로 전이학습을 진행하여 UCF101 과 HMDB51 과 같은 작은 규모의 데이터셋에서도 좋은 성능을 냈습니다. 또한 Two-Stream Inflated 3D ConvNet (I3D)을 제안하고 전이학습을 진행하여 HMDB51에서는 80.9% UCF101에서는 98.0%를 달성하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/cover.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-old-ⅰ-convnetlstm&quot;&gt;The Old Ⅰ: ConvNet+LSTM&lt;/h3&gt;

&lt;p&gt;영상에서 25 프레임을 뽑아낸후, CNN을 돌려서 나온 결과를 LSTM으로 입력하여 sequential한 정보를 예측해 보겠다는 아이디어 입니다. 직관적으로도 배경을 제거하고 CNN에 들어가는 것이 아니기 때문에 액션보다는 배경에 큰 영향을 받고, 미세한 액션은 잘 찾지 못하는 단점이 있었습니다.(LSTM을 꼭 활용하고 싶으시다면 OCR처럼 액션을 하는 오브젝트를 디텍션한 후 크롭하는 방법을 추천 드립니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure1.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-old-ⅱ-3d-convnets&quot;&gt;The old Ⅱ: 3D ConvNets&lt;/h3&gt;

&lt;p&gt;action recognition에 처음 입문하시는 분들이 가장 먼저 직관적으로 예측가능 한 방법 같습니다. 하지만 3D conv는 3D 컨브보다 더 많은 파라메터가 필요하며, 이는 학습을 어렵게 만듭니다. 그리고 여전히 배경에 영향을 많이 받기 때문에 작은 행동들은 많이 놓치는 경향이 보였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure2.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;optical-flow-란&quot;&gt;Optical flow 란?&lt;/h3&gt;

&lt;p&gt;기존 영상처리에서 움직이는 객체를 추적할때 자주 사용하던 방법입니다. Optical flow를 사용하면 움직이는 객체의 x방향 y방향의 벡터를 뽑아 낼 수 있습니다. 이 논문에서는 TVL1방법을 사용 했습니다. TVL1 방법이란 두 프레임 사이의 변화한 점을 픽셀 단위로 추적하면서, 데이터 사이의 차이를 L1 norm으로 구하고, 전체 데이터의 분산을 사용하여 정규화 하는 방법입니다.(딥러닝을 사용하지 않고 Optical flow를 뽑는 방법중 가장 쓸만한 방법 입니다.)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;optical_flow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DualTVL1OpticalFlow_create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optical_flow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prvs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure3.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-old-ⅲ-two-stream-networks&quot;&gt;The old Ⅲ: Two-Stream Networks&lt;/h3&gt;

&lt;p&gt;아직도 활발한 연구가 진행되고 있는 방법이지만, 이 논문 저자는 old 라고 표기했기 때문에 old 라고 표현하겠습니다! 이 방법은 RGB와 Optical flow를 사용한 2D conv 방법입니다. Optical flow를 사용하여 행동을 예측하기 때문에 action을 비교적 잘 찾지만, 아직도 여전히 찝찝한 부분은 남아있습니다. 2D conv이기 때문에 rgb 한장 optical flow한장 들어가기 때문에 한 영상에서 뽑은 프레임 사이의 관계를 예측하는 부분에서는 아직까지 뭔가 찝찝합니다. 또한 이 찝찝함을 해결하기 위하여 rgb와 optical flow에서 나온 결과를 concat하여 3D를 만든후 3D conv를 하는 방법도 있습니다.(이 방법들의 단점을 직관적으로 생각해 보면 rgb는 3D컨브를 해야 RGB프레임 간 관계를 잘 예측 할 수 있기 떄문에 이 방법들은 RGB간 관계를 알기 힘들어서 정확도가 낮게 나온다고 생각합니다. 혹시 이 부분이 틀리다면 댓글로 지적 부탁드립니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure4.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-new-two-stream-inflated-3d-convnets&quot;&gt;The New: Two-Stream Inflated 3D ConvNets&lt;/h3&gt;

&lt;p&gt;RGB와 Optical flow를 동시에 활용한다는 면에서 Two-Stream 방법이고, 2D conv가 아니라 3D conv이기 떄문에 Two-Stream Inflated 3D ConvNets 이라고 정의하며, 앞에서 언급한 모든 방법들보다 이 논문의 실험에서는 정확도가 가장 높았습니다.
RGB를 3D conv 함으로써 시간정보를 계층적으로 만들 수 있지만, 그래도 여전히 action을 인식하기에는 부족한 면이 있기 때문에 Optical flow도 활용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure5.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;inflating-2d-convnets-into-3d&quot;&gt;Inflating 2D ConvNets into 3D&lt;/h3&gt;

&lt;p&gt;단순하게 2D conv를 3D conv로 변경하려면 시간축 디멘션 하나를 추가하고 pad를 줘서 shape를 맞춰주시면 됩니다. 논문에서 언급한 것 처럼 간단하게 N&lt;em&gt;N을 N&lt;/em&gt;N*N을 만들어 주시면 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;bootstrapping-3d-filters-from-2d-filter&quot;&gt;Bootstrapping 3D filters from 2D Filter&lt;/h3&gt;

&lt;p&gt;3D conv에서 ImageNet pre-trained 된 weight를 활용하려면 단순하게 weight를 N번 복제해 주시면 됩니다. 뭔가 직관적으로 하면 안될 것 같은 느낌이 들지만 이 논문에서는 실험적으로 이렇게 해서라도 ImageNet pre-trained된 weight를 사용하는게 좋다고 밝힙니다.&lt;/p&gt;

&lt;h3 id=&quot;pacing-receptive-field-growth-in-space-time-and-network-depth&quot;&gt;Pacing receptive field growth in space, time and network depth&lt;/h3&gt;

&lt;p&gt;직관적으로 당연히 공간정보와 시간정보의 stride가 적절하게 조절되어야 합니다. 공간정보는 조금 변하는데 시간정보의 stride가 많이 변하면 공간정보를 제대로 못보고, 그렇다고 시간정보의 stride가 조금 변하면 그것은 정지영상과 다름이 없어 행동을 잘 인식하지 못하게 됩니다. 이 논문 저자는 Inflated Inception-V1에서 첫 번째와 두 번째의 Max-Pool레이어에서는 시간축의 stride의 크기를 1 로 설정하면 경험적으로 더 좋았다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure6.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;two-3d-streams&quot;&gt;Two 3D Streams&lt;/h3&gt;

&lt;p&gt;RGB 정보만 이용해도 3D conv를 사용하면 시간정보를 볼 수 있지만, 이 논문에서는 그래도 Optical flow를 사용하면 경험적으로 더 정확도가 높았다고 합니다. 지금까지 내용을 간단하게 요약하면 아무리 3D conv를 사용하는게 좋고, 3D conv를 사용하더라도 Optical flow를 사용하는게 좋다 입니다.&lt;/p&gt;

&lt;h3 id=&quot;conculusion&quot;&gt;Conculusion&lt;/h3&gt;

&lt;p&gt;저는 원래 결론 쓰는 것을 좋아하진 않지만!(개인적인 견해가 들어갈 수 있기 때문에) 데이터셋 논문이기 때문에 결론을 작성하겠습니다. 논문에서 제시한 I3D방법을 사용하는 것이 정확도가 가장 높았고, 정확도 대비 파라메터가 그렇게 많지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure7.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파라메터도 적은 편 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure8.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;정확도도 i3d가 가장 높았으며 i3d 중에서도 optical flow도 활용하는 방법이 가장 좋았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure9.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞에서도 언급했던 것 처럼 Imagenet에서 pre-trained된 것을 활용하는게 더 성능이 좋았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-08-04-kinetics-dataset/figure10.PNG&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모든 방법에서 i3d 방법이 가장 좋았습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문은 action recognition에 입문하는 사람이라면 꼭 읽어 보시는걸 추천드리고, 아직 코드 구현은 못했습니다. 구현이 완료 되는데로 링크 첨부 하겠습니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Bag of Tricks</title>
      <link href="https://blog.airlab.re.kr/2019/07/bag-of-trick" rel="alternate" type="text/html" title="Bag of Tricks" />
      <published>2019-07-27T05:00:00+09:00</published>
      <updated>2019-07-27T05:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/bag-of-trick</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/bag-of-trick">&lt;p&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 &lt;strong&gt;“Bag of Tricks for Image Classification with Convolutional Neural Networks”&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;이 논문의 저자들은 Image Classification에서 모델 구조의 변경(vgg,resnet,densenet)은 집중적인 주목을 받고 많은 사람들이 중요하다고 생각하지만, 나머지 Data Augmentations 및 Optimization Methods 과 같은 학습 방법들은 주목을 받지 못함에 안타까워 하고 있습니다. 또한 해당 방법들은 논문에서 간단하게 언급되거나, 심지어 논문에는 없고 소스 코드에만 존재하는 트릭들도 있습니다. 이 논문의 저자들은 이러한 학습 방법들을 자세히 조사하고 평가합니다. 아래의 사진을 보시면 트릭들만 잘 사용해도 4%의 정확도를 올릴 수 있는 것을 보실 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/cover.png&quot; alt=&quot;cover&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;baseline&quot;&gt;Baseline&lt;/h3&gt;

&lt;p&gt;우선 여러 트릭들을 비교 하려면 동일한 환경을 만들어야 합니다. 이 논문의 저자들은 ResNet50을 기본 구조로 하고 아래의 나열된 방법으로 전처리를 했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly sample an image and decode it into 32-bitfloating point raw pixel values in[0,255].
(사실 저희가 훈련시키는 대부분의 이미지가 float 32bit에 srgb형식이기 때문에 특별한 경우가 아니라면 그냥 기본 상태 입니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomly crop a rectangular region whose aspect ratiois randomly sampled in[3/4,4/3]and area randomly sampled in[8%,100%], then resize the cropped regioninto a 224-by-224 square image.
(aspect ratiois을 지정하고 랜덤하게 이미지를 [224,224] 형식으로 크롭합니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Flip horizontally with 0.5 probability.
(50% 확률로 이미를 반전 합니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Scale hue, saturation, and brightness with coefficients uniformly drawn from[0.6,1.4].
(색조, 채도 및 밝기를 균일하게 스케일합니다.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add PCA noise with a coefficient sampled from a normal distribution N(0,0.1).
(PCA노이즈를 이미지에 줍니다.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;모든 Conv,fc 층에는 Xavier Initialization을 하였고, Batch Normalization의 감마와 베타는 1,0 그리고 Optimizer는 NAG를 선택했습니다.
pytorch에서는 SGD의 파라메터중 nesterov를 True로 해주시면 됩니다.&lt;/p&gt;

&lt;p&gt;위에서 언급한 베이스 라인들은 너무 일반적인 내용이라 추가 설명은 하지 않겠습니다. 아래 보이는 사진은 베이스라인으로 구현한 Image Classification의 정확도와 각 논문에서 주장하는 정확도의 비교입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/table1.png&quot; alt=&quot;table1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;large-batch-training&quot;&gt;Large-batch training&lt;/h3&gt;

&lt;p&gt;최근 GPU 기술이 좋아지면서 Large-batch training이 가능해 졌습니다. 하지만 배치 사이즈가 증가하면 수렴속도가 늦어진다고 알려져 있고, 그 뜻은 테스트에서 정확도가 떨어질 확률이 있다는 것과 같습니다. 이 단란에서는 이러한 문제점을 경험적으로 풀어냅니다.&lt;/p&gt;

&lt;h4 id=&quot;linear-scaling-learning-rate&quot;&gt;Linear scaling learning rate.&lt;/h4&gt;

&lt;p&gt;배치사이즈를 증가시키면 러닝레이트 또한 선형적으로 증가시켜줘야 합니다. 256배치 사이즈의 기준이 되는 러닝레이트를 0.1로 설정하고, 0.1 x b/256 이라는 공식을 제안합니다.(배치사이즈를 증가시키면 당연히 표본이 많이 선택되기 때문에 분산을 줄어듭니다. 분산이 줄어들면 안정적 이기 때문에 러닝레이트를 높이 설정해도 됩니다.)&lt;/p&gt;

&lt;h4 id=&quot;learning-rate-warmup&quot;&gt;Learning rate warmup.&lt;/h4&gt;

&lt;p&gt;초기의 parameters은 랜덤하게 초기화되기 때문에 불안정합니다. 그렇기 때문에 초기에 러닝레이트를 0에서 점점 키워 나가면서 초반 불안정을 해소합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figure1.png&quot; alt=&quot;Figure1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;zero-감마&quot;&gt;Zero 감마.&lt;/h4&gt;

&lt;p&gt;residual block이 존재하는 모델에서는 bn의 감마를 0으로 초기합니다.(residual block쪽에서만) 그렇다면 초기의 네트워크가 짧아져서 학습이 더욱 빠르다고 합니다. 개인적인 사견이지만 모든 레이어에 일관적으로 초기화 해주는 것은 매우 간단하지만 특정 레이어만 다른 방법으로 초기화 해주는것은 매우 귀찮습니다. 그리고 실험해 본 결과 정확도에서 향상도 없고 학습속도가 그렇게 빨라지는 것도 아니기 때문에 그냥 감마는 1 베타는 0으로 초기화 하는것을 추천 드립니다.&lt;/p&gt;

&lt;h4 id=&quot;no-bias-decay&quot;&gt;No bias decay.&lt;/h4&gt;

&lt;p&gt;bias 는 decay를 안해줘야 오버피팅을 방지한다고 합니다. bn도 마찬가지로 decay를 안해줘야 오버피팅을 방지하는 효과가 있다고 합니다. pytorch에서는 bn의 감마와 베타가 weight, bias라는 이름으로 설정되어 있기 때문에 코드안에서 뭔가 특별한 것을 해줘야 합니다. 코드는 아래의 사진의 첨부했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figure2.png&quot; alt=&quot;Figure2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;low-precision-training&quot;&gt;Low-precision training&lt;/h3&gt;

&lt;p&gt;GPU가 발전하면서 FP16의 연산도 이제는 가능합니다. 당연히 기존의 FP32보다 메모리 소모량도 적고 연산량도 적기 때문에 FP16에서 연산하고 다시 FP32로 옮겨오면 당연하게 학습속도가 빨라집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrue3.png&quot; alt=&quot;Figrue3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrues4.png&quot; alt=&quot;Figrues4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;model-tweaks&quot;&gt;Model Tweaks&lt;/h3&gt;

&lt;p&gt;원래의 resnet의 구조는 아래의 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus5.png&quot; alt=&quot;Figrus5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 레즈넷을 보면서 많은 사람들이 7x7 conv를 보면서 3x3 conv 3개로 대체할 수 있는데 왜 했지? 그런 생각을 한번쯤은 모두 해보셨을거라고 생각합니다. 논문 저자는 7x7 conv를 3x3 conv 3개로 대체 하였습니다. 그리고 이미지를 다운샘플링 하는 부분에서 1x1 conv의 stride가 2인데 그렇다면 짝수번째의 핏쳐들은 모두 드랍되는것인데 이 부분도 문제라고 다들 생각하셨을 겁니다.이 논문의 저자는 이 부분도 수정하였습니다. 아래 그림 c 가 7x7을 수정한 부분이고, 그림 d 가 stride 2 의 문제를 수정하고, avgpool을 추가한 구조 입니다. 그림 b는 기존의 제안되었던 stride문제를 해결한 구조입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus6.png&quot; alt=&quot;Figrus6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus7.png&quot; alt=&quot;Figrus7&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training-refinements&quot;&gt;Training Refinements&lt;/h3&gt;

&lt;p&gt;정확도를 높이기위한 부분입니다.&lt;/p&gt;

&lt;h4 id=&quot;cosine-learning-rate-decay&quot;&gt;Cosine Learning Rate Decay&lt;/h4&gt;

&lt;p&gt;이 논문에서는 Cosine Learning Rate Decay를 상당히 어렵게 설명합니다. 그냥 직관적으로 생각할때 Cosine Learning Rate Decay 에폭이 늘어 날때마다 당연하게도 러닝레이트는 줄어들어야 하는것은 당연하고, 그것을 매 에폭마다 설정해 주는 것 입니다. 학습 초반에는 수렴하지 않은 상태이기 때문에 큰 러닝 레이트를 가지고 있는 것이 유리하기 때문에 Cosine Learning Rate Decay는 처음에는 에폭이 조금 줄어들다가 수렴하기 시작하면 에폭이 많이 줄어 듭니다(기울기로 생각해 볼때).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrus8.png&quot; alt=&quot;Figrus8&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;label-smoothing&quot;&gt;Label Smoothing&lt;/h4&gt;

&lt;p&gt;라벨 스무딩 입니다. 라베을 원핫 백퍼로 정답인 것은 1 아닌것은 0으로 만드는 것이 보통인데, 0.9 ,0.1 이런식으로 스무딩 해주는 것 입니다.
이부분은 잘 이해하지 못했지만, 직관적으로 한 쪽으로 너무 쏠려서 오버피팅이 될 확률이 적어 질 것 이라고 생각하고있습니다. 정확하게 아시는 분은 댓글 부탁 드립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-27-bag-of-tricks/Figrues9.png&quot; alt=&quot;Figrues9&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;knowledge-distillation&quot;&gt;Knowledge Distillation&lt;/h4&gt;

&lt;p&gt;선생님이 제자를 가르치듯이, 복잡하고 큰 모델이, 작은 모델의 학습을 도와 더 적은 복잡도로 좋은 성능을 내는 방법입니다. 선생 모델로 resnet152를 사용하고 학생모델로 resnet50을 선택했습니다.&lt;/p&gt;

&lt;h4 id=&quot;mixup-training&quot;&gt;Mixup Training&lt;/h4&gt;

&lt;p&gt;데이터 어규멘테이션의 한 방법이라고 생각하시면 됩니다. 두개의 클래스를 적당한 비율로 섞고 확률을 (0.7, 0.3) 이런식으로 표현합니다. 오버피팅을 방지하는 효과가 있고 경험적으로 너무 많이 섞으면 정확도가 오히려 떨어졌으며, 세그멘테이션 테스크에서는 매우 안좋은 결과가 나왔습니다. 개인적인 의견으로는 사용하지 않으셔도 될 것 같고, 네이버에서 이번에 발표한 cutmix 논문을 한번 참고 하시는게 좋으실 것 같습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;

&lt;p&gt;이 논문을 딥러닝 입문 초기에 읽었으면 더 좋았을 것 같다는 생각을 했습니다. 나중에 후배들이 생기면 이 논문을 제일 첫 번째로 권해주고 싶을 정도로, 딥러닝을 많이 해봐야 얻을 수 있는 직관들과 꿀팁들을 알려주는 논문이라 매우 좋았습니다. 논문 구현 코드는 제 깃허브&lt;a href=&quot;https://github.com/seominseok0429/pytorch-warmup-cosine-lr&quot;&gt;https://github.com/seominseok0429/pytorch-warmup-cosine-lr&lt;/a&gt;에 올려 두었습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">Bag of Tricks for Image Classification with Convolutional Neural Networks 리뷰</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">MobileNetV2</title>
      <link href="https://blog.airlab.re.kr/2019/07/mobilenetv2" rel="alternate" type="text/html" title="MobileNetV2" />
      <published>2019-07-22T19:00:00+09:00</published>
      <updated>2019-07-22T19:00:00+09:00</updated>
      <id>https://blog.airlab.re.kr/2019/07/mobilenetv2</id>
      <content type="html" xml:base="https://blog.airlab.re.kr/2019/07/mobilenetv2">&lt;p&gt;MobileNetV2 : Inverted Residuals and Linear Bottlenecks 리뷰&lt;/p&gt;

&lt;p&gt;안녕하세요. &lt;strong&gt;AiRLab&lt;/strong&gt;(한밭대학교 인공지능 및 로보틱스 연구실) 서민석입니다. 제가 이번에 리뷰할 논문은 제목에도 써 있는것과 같이 MobileNetV2 : Inverted Residuals and Linear Bottlenecks 입니다. 간단하게 한 줄로 이 논문을 소개하자면 모바일이나, 임베디드에서도 실시간을 작동할 수 있게 모델이 경량화 되면서도, 정확도 또한 많이 떨어지지 않게하여, 속도와 정확도 사이의 트레이드 오프 문제를 어느정도 해결한 네트워크 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure4.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 이 논문을 읽기전에 알아두면 좋은 Related Works는 아래 두 논문 입니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Xception: Deep Learning with Depthwise Separable Convolutions(https://arxiv.org/abs/1610.02357)&lt;/li&gt;
  &lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications(https://arxiv.org/abs/1704.04861)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;개인적으로 이 논문을 읽으면서 기존에 존재했던 “Xception”, “MobileNets” 과의 크게 다른점을 저는 느끼지 못했습니다(그렇기에 이 논문을 제대로 이해 하시려면 앞에서 언급한 두 논문을 읽어보시는걸 추천 드립니다). 이 논문은 저자들이 “Xception”에서 제안했던  Depthwise Separable Convolutions을 그대로 사용합니다. 또한 Depthwise Separable Convolutions이 사용하는 철학, 가설을 그대로 채택합니다. 의식의 흐름대로 읽다보면 Depthwise Separable Convolutions이 뭐지? 하는 질문이 당연히 드실꺼라고 생각합니다.&lt;/p&gt;

&lt;h3 id=&quot;depthwise-separable-convolutions이란&quot;&gt;Depthwise Separable Convolutions이란?&lt;/h3&gt;

&lt;p&gt;Depthwise Separable Convolutions을 아주 간단하게 요약하면 &lt;strong&gt;Depthwise Convolutions + Pointwise Convolutions&lt;/strong&gt; 입니다.&lt;/p&gt;

&lt;p&gt;Depthwise Separable Convolutions을 설명하기 전에 기존의 Convolutions을 생각해 봅시다.
&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure5.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에 보이시는 것 처럼 기존의 Convolutions은 채널과 과 필터가 동시에 고려되서 최종 아웃풋을 만듭니다. 하지만 이 논문의 저자는 cross-channels correlation(입력 채널들 사이의 유사도)과 spatial correlation(필터와 하나의 특정 채널 사이의 관계)이 완전하게 독립적이기 때문에 &lt;strong&gt;채널과 필터를 따로 분리해서 학습&lt;/strong&gt;을 진행해도 문제가 없다고 주장합니다. 실제로 연상량을 계산해보면 Traditional convolutions은 &lt;strong&gt;입력 이미지의 크기x입력 이미지의 채널x 커널사이즈 제곱x아웃풋채널&lt;/strong&gt; 이지만 Depthwise Separable Convolutions의 연산량은 &lt;strong&gt;입력 이미지의 크기x입력 이미지의 채널x (커널사이즈 제곱+아웃풋채널)&lt;/strong&gt; 이기 때문에 &lt;strong&gt;8~9배&lt;/strong&gt; 정도 연산량이 줄어듭니다.(커널사이즈는 3 이라고 가정합니다)&lt;/p&gt;

&lt;p&gt;다음으로 이 논문에서 주장하는 Linear Bottlenecks 입니다.&lt;/p&gt;

&lt;h3 id=&quot;linear-bottlenecks-이란&quot;&gt;Linear Bottlenecks 이란?&lt;/h3&gt;

&lt;p&gt;지금부터 조금 어려운 이야기를 직관적으로 쉽게 풀이하겠습니다(논문에서도 직관이라는 단어를 많이 사용합니다). 우선 manifold라는 말을 알고 있으셔야 합니다.
manifold란 어떤 이미지의 차원들이 존재하는 공간이라고 생각하시면 됩니다. 이 논문에서는 Manifold의 가설을 언급합니다(It has been longassumed  that  manifolds  of  interest  in  neural  networkscould be embedded in low-dimensional subspaces.). manifold 가설은 고차원의 정보는 사실 저차원으로 표현 가능하다는 것입니다. 예를 들어서 설명하면, 실제 세상에 존재하는 모든 사물들은 3차원 이라고 이야기를 하지만 사람들은 실제로 사물을 구분할 때는 2차원 정보를 받아들여 사물을 구분할 수 있다는 것 입니다. 즉 고차원 정보는 사실 저차원 정보로도 충분히 구분 할 수 있다는 것 입니다.&lt;/p&gt;

&lt;p&gt;지금까지 Manifold에 대하여 설명한 이유는 이 논문에서 Linear Bottlenecks을 만들때 1x1의 pointwise Convolutions을 하여 차원수를 줄이기 때문입니다. &lt;strong&gt;Manifold의 가설 그대로 고차원의 채널은 사실 저차원의 채널로 표현할 수 있다&lt;/strong&gt; 라는 논리 전개 입니다.(채널을 과도하게 줄이면 안됩니다. 예를들어서 사람은 3차원의 정보를 2차원으로 구분하지만 1차원으로는 구분 못하는 것과 같습니다.)&lt;/p&gt;

&lt;p&gt;Linear Bottlenecks에서 주장하는 또 다른 하나는 ReLU는 필연적으로 정보 손실을 야기하기 때문에 어떤 특별한 작용을 해줘서 그 정보손실을 방어해야 한다는 것 입니다. 이제 그 특별한 작용에 대하여 말씀드리겠습니다.&lt;/p&gt;

&lt;p&gt;시작하기 전에 가장 간단히 한줄로 요약하면 &lt;strong&gt;“채널수가 충분히 많으면 ReLU를 사용해도 중요 정보는 보존된다!”&lt;/strong&gt; 입니다. 이 문장을 계속 상기시키면서 글을 읽으시면 이해하시는데 도움이 되실 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure8.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 보이시는 그림처럼 채널이 1인 데이터가 ReLU를 지나면 중요 정보가 삭제 될 수 도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure9.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 위에 보이시는 그림처럼 채널이 2 인 데이터가 ReLU를 지나면 중요 정보가 삭제 되더라도 다른 채널에서는 아직까지 존재할 가능성이 채널이 많으면 많을수록 높기 때문에 &lt;strong&gt;채널이 많을때 ReLU를 사용하면 괜찮다는 것&lt;/strong&gt; 입니다.(어차피 나중에 전부 합쳐져서 예측하기 때문에)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure6.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에 보이시는 그림은 이 주장을 실험적으로 증명한 것 입니다. 차원을 2, 3, 5, 15, 30 을 각각쓰고 ReLU를 쓰고 원래대로 복원하였습니다. 그림에서 보이는 것 과 같이 차원이 적을때는 ReLU를 쓰면 정보가 손실되어 원본 영상을 복원할 수 없지만 차원을 충분히 늘리고 ReLU를 쓰면 15, 30 과 같이 잘 복원 할 수 있다는 것 입니다.&lt;/p&gt;

&lt;p&gt;마지막으로 Linear Bottlenecks은 ReLU를 적용하지 않습니다. 위에서 말씀드린것과 같이 차원이 매우많이 축소된 상태이기 때문에 ReLU를 사용하면 정보손실이 있을 수도 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure3.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험적으로 증명을 했는데, Linear Bottlenecks에 ReLU6을 썻을때와 안썻을때의 정확도의 차이 입니다.(ReLU6를 사용하는 이유는 연산량에 있어서 이득을 볼 수 있다고 알아보았는데 정확하진 않습니다. 혹시 정확한 이유를 아시는분은 댓글 부탁드립니다.) 또 shortcut 위치에 대한 실험도 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;inverted-residuals이란&quot;&gt;Inverted Residuals이란?&lt;/h3&gt;

&lt;p&gt;앞에서 설명드린 Depthwise Separable Convolutions과 Linear Bottlenecks을 결합하면 Inverted Residuals 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure1.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존의 Residuals을 거꾸로 뒤집은 모양이라 Inverted Residuals이라고 부르는것 같습니다. 앞에서 언급한 논리되로 ReLU를 사용해야 하기 때문에 채널을 확장(pointwise Convolutions)하고 Depthwise Convolutions을 진행합니다. 또 Linear Bottlenecks에서 대로 다시 채널수를 줄입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2019-07-22-mobilenetv2/figure2.png&quot; alt=&quot;MobileNetV2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;stride가 1일때는 shortcut이 있지만 strdie가 2 일때는 shortcut 이 없습니다. 이유는 논문에서 설명하지 않고 있지만 이미지의 크기가 줄어들때 정보의 선형성이 보장되 않기 때문이라고 추측하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;memory-efficient-inference&quot;&gt;Memory Efficient Inference&lt;/h3&gt;

&lt;p&gt;논문에서는 gpu에서 내부 메모리와 외부 메모리가 있기 때문에 내부로 올릴때의 크기과 나갈때의 크기만 중요하기때문에 메모리 스왑적인 부분에서 봤을때도 이 논문에서 제안한 Inverted Residuals구조가 효율적이라고 주장하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;후기&quot;&gt;후기&lt;/h3&gt;
&lt;p&gt;논문리뷰 끝입니다. 논문의 Conclusions은 개인적인 견해가 필요하고, 이 부분은 이 글을 읽고 있는 독자 여러분이 편견없이 논문을 읽으면 좋겠다고 생각하여 리뷰하지 않겠습니다.&lt;/p&gt;

&lt;p&gt;코드는 cifar10에 적용한 것이고 &lt;a href=&quot;https://github.com/seominseok0429/cifar10-mobilenetv2-pytorch&quot;&gt;https://github.com/seominseok0429/cifar10-mobilenetv2-pytorch&lt;/a&gt; 에 배포해 두었습니다.&lt;/p&gt;

&lt;p&gt;끝까지 읽어 주셔서 감사합니다!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Minseok Seo</name>
        
        
      </author>

      

      
        <category term="paper-review" />
      

      
        <summary type="html">MobileNetV2 : Inverted Residuals and Linear Bottlenecks 리뷰</summary>
      

      
      
    </entry>
  
</feed>
